{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Download the required Dependencies**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EvLYL2g4zSuq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GMGRy4YF-ky8"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade openai langchain langchain-community langchain_openai  openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY=\"Enter your api key\""
      ],
      "metadata": {
        "id": "ivyc9t_zXQ_H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "kwMRam_oXtcX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name=\"gpt-3.5-turbo\"\n",
        ")"
      ],
      "metadata": {
        "id": "6VFMYku8myR_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Zero Short Prompt**"
      ],
      "metadata": {
        "id": "kjM7wUnzBC76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This involves giving the AI a task without any prior examples. You describe what you want in detail, assuming the AI has no prior knowledge of the task.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4RUie2P-vCdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template=\"what is mean by large language model ?\"\n",
        "\n",
        "# Prompt template (simple instruction)\n",
        "template = PromptTemplate(template=template)\n",
        "\n",
        "# Send prompt and get response\n",
        "response = llm.invoke(template.format())\n",
        "\n",
        "# Print response\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BRV2dV1BJpG",
        "outputId": "25aee61d-0020-494e-e214-449337e8f763"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A large language model is a type of artificial intelligence system that is trained on vast amounts of text data in order to understand and generate human language. These models are capable of processing and generating text in a way that is very similar to how humans communicate. They are typically used for tasks such as language translation, text generation, and natural language understanding. Examples of large language models include GPT-3 (Generative Pre-trained Transformer 3) and BERT (Bidirectional Encoder Representations from Transformers).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **One Short Prompt**"
      ],
      "metadata": {
        "id": "L9JBXUT3Dy67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You provide one example along with your prompt. This helps the AI understand the context or format youâ€™re expecting."
      ],
      "metadata": {
        "id": "2rFNVu67vUc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "# Create a one-shot prompt\n",
        "template = \"\"\"\n",
        "A Foundation Model in AI refers to a model like GPT-3,\n",
        "which is trained on a large dataset and can be adapted to various tasks.\n",
        "Explain what BERT is in this context.\n",
        "\"\"\"\n",
        "\n",
        "# Define the prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "# Generate the response\n",
        "response = llm.invoke(prompt_template.format())\n",
        "\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYgokIFbt5ba",
        "outputId": "60f1ab9a-ac52-46d1-cb88-74f00908fd47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT (Bidirectional Encoder Representations from Transformers) is another example of a Foundation Model in AI. It is a pre-trained model developed by Google that is trained on a large corpus of text data. BERT is designed to understand the context of words in a sentence by considering the words that come before and after each word. This bidirectional approach allows BERT to capture more complex relationships and nuances in language compared to traditional models. Like GPT-3, BERT can be fine-tuned for specific tasks such as text classification, question answering, and named entity recognition.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Few Short prompt**"
      ],
      "metadata": {
        "id": "4WjXYQNTD52-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a technique where you provide a small set of examples or instructions to guide the model towards a specific task or response style"
      ],
      "metadata": {
        "id": "LNySGJG7w46j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "# First, create the list of few shot examples.\n",
        "examples = [\n",
        "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
        "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
        "]\n",
        "\n",
        "# Next, we specify the template to format the examples we have provided.\n",
        "# We use the `PromptTemplate` class for this.\n",
        "example_formatter_template = \"\"\"Word: {word}\n",
        "Antonym: {antonym}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"word\", \"antonym\"],\n",
        "    template=example_formatter_template,\n",
        ")"
      ],
      "metadata": {
        "id": "KFzPvcCWD5hI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we create the `FewShotPromptTemplate` object.\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    # These are the examples we want to insert into the prompt.\n",
        "    examples=examples,\n",
        "    # This is how we want to format the examples when we insert them into the prompt.\n",
        "    example_prompt=example_prompt,\n",
        "    # The prefix is some text that goes before the examples in the prompt.\n",
        "    # Usually, this consists of intructions.\n",
        "    prefix=\"Give the antonym of every input\\n\",\n",
        "    # The suffix is some text that goes after the examples in the prompt.\n",
        "    # Usually, this is where the user input will go\n",
        "    suffix=\"Word: {input}\\nAntonym: \",\n",
        "    # The input variables are the variables that the overall prompt expects.\n",
        "    input_variables=[\"input\"],\n",
        "    # The example_separator is the string we will use to join the prefix, examples, and suffix together with.\n",
        "    example_separator=\"\\n\",\n",
        ")"
      ],
      "metadata": {
        "id": "9odyeHP4xVUE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(few_shot_prompt.format(input='big'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5Tsn7iZxlRi",
        "outputId": "e0a4f808-368f-400e-edc2-7106a2160177"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "Word: happy\n",
            "Antonym: sad\n",
            "\n",
            "Word: tall\n",
            "Antonym: short\n",
            "\n",
            "Word: big\n",
            "Antonym: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain=LLMChain(llm=llm,prompt=few_shot_prompt)\n",
        "chain({'input':\"big\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhbKX5z7xpI4",
        "outputId": "f0b5059c-2605-426d-bcac-8eac7c0d6fb5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'big', 'text': 'small'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chain-of-Thought Prompt**"
      ],
      "metadata": {
        "id": "2QKa6upMEEsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chain-of-Thought (CoT) prompting is a technique that guides LLMs to follow a reasoning process when dealing with hard problems. This is done by showing the model a few examples where the step-by-step reasoning is clearly laid out. The model is then expected to follow that \"chain of thought\" reasoning and get to the correct answer."
      ],
      "metadata": {
        "id": "rUk5b8lCw_WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.chains import SimpleChain\n",
        "# Define the initial prompt as a question\n",
        "prompt = \"What is the sum of 5 and 3?\"\n",
        "\n",
        "# Chain of Thought (CoT) steps explained as text\n",
        "cot_explanation = \"\"\"\n",
        "1. Let's add the first number, 5.\n",
        "2. Then, add the second number, 3, to the previously obtained sum.\n",
        "3. The answer is the final sum.\n",
        "\"\"\"\n",
        "\n",
        "# Combine the prompt and CoT explanation for clarity\n",
        "prompt_with_cot = f\"{prompt}\\n\\n{cot_explanation}\"\n",
        "\n",
        "# Define a prompt template without any input variables\n",
        "prompt_template = PromptTemplate(input_variables=[], template=prompt_with_cot)\n",
        "\n",
        "# Initialize the SimpleChain with the prompt template\n",
        "chain = SimpleChain(prompt_template=prompt_template)\n",
        "\n",
        "# Generate the response using the chain\n",
        "response = chain.run()\n",
        "\n",
        "# Print the content of the response (assuming it's text)\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "qt-nT58GuLD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tree of thoughts**"
      ],
      "metadata": {
        "id": "oBdadjLETJKY"
      }
    },
    {
      "source": [
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "def tree_of_thoughts(prompt, depth=2, breadth=3, llm=None):\n",
        "  \"\"\"\n",
        "  This function generates a tree of thoughts using an LLM.\n",
        "\n",
        "  Args:\n",
        "      prompt: The initial prompt to start the exploration.\n",
        "      depth: Maximum depth of the tree (number of levels).\n",
        "      breadth: Number of branches to explore at each level.\n",
        "      llm: The LLM object to use for generating responses (optional).\n",
        "\n",
        "  Returns:\n",
        "      A list of branches, where each branch is a tuple containing the prompt\n",
        "      and a list of responses generated by the LLM.\n",
        "  \"\"\"\n",
        "\n",
        "  branches = []\n",
        "\n",
        "  def generate_branches(prompt, current_depth):\n",
        "    if current_depth == depth:\n",
        "      return\n",
        "\n",
        "    # Create an instance of PromptTemplate\n",
        "    prompt_template = PromptTemplate(input_variables=[], template=prompt)\n",
        "\n",
        "    # Check if LLM is provided, otherwise raise an error\n",
        "    if not llm:\n",
        "      raise ValueError(\"LLM object is required for generating responses.\")\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "    # Use 'run' method to generate a single response and replicate it for breadth\n",
        "    response = chain.run({})\n",
        "    responses = [response] * breadth\n",
        "\n",
        "    branches.append((prompt, responses))\n",
        "\n",
        "    for response in responses:\n",
        "      new_prompt = prompt + \" \" + response\n",
        "      generate_branches(new_prompt, current_depth + 1)\n",
        "\n",
        "  generate_branches(prompt, 0)\n",
        "  return branches\n",
        "\n",
        "# Define the initial prompt\n",
        "initial_prompt = \"Find the sum of 5 and 3\"\n",
        "\n",
        "# Assuming you have an LLM object defined as 'my_llm'\n",
        "thought_tree = tree_of_thoughts(initial_prompt, llm=llm) # Uncomment and replace my_llm with your actual LLM object\n",
        "thought_tree"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ygw7Hx9WVEbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12bdb409-95dc-4fa3-95ec-9c8d5cf6fb8a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Find the sum of 5 and 3',\n",
              "  ['The sum of 5 and 3 is 8.',\n",
              "   'The sum of 5 and 3 is 8.',\n",
              "   'The sum of 5 and 3 is 8.']),\n",
              " ('Find the sum of 5 and 3 The sum of 5 and 3 is 8.',\n",
              "  ['The sum of 5 and 3 is 8.',\n",
              "   'The sum of 5 and 3 is 8.',\n",
              "   'The sum of 5 and 3 is 8.']),\n",
              " ('Find the sum of 5 and 3 The sum of 5 and 3 is 8.',\n",
              "  ['The sum of 5 and 3 is 8.',\n",
              "   'The sum of 5 and 3 is 8.',\n",
              "   'The sum of 5 and 3 is 8.']),\n",
              " ('Find the sum of 5 and 3 The sum of 5 and 3 is 8.',\n",
              "  ['The sum of 5 and 3 is 8.',\n",
              "   'The sum of 5 and 3 is 8.',\n",
              "   'The sum of 5 and 3 is 8.'])]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **self consisitency prompt**"
      ],
      "metadata": {
        "id": "cqs46VUQsGh-"
      }
    },
    {
      "source": [
        "import openai\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def tree_of_thoughts(prompt, depth=2, breadth=3, llm=None):  # Add llm parameter here\n",
        "    \"\"\"\n",
        "    This function generates a tree of thoughts using an LLM.\n",
        "\n",
        "    Args:\n",
        "        prompt: The initial prompt to start the exploration.\n",
        "        depth: Maximum depth of the tree (number of levels).\n",
        "        breadth: Number of branches to explore at each level.\n",
        "        llm: The LLM object to use for generating responses.\n",
        "\n",
        "    Returns:\n",
        "        A list of branches, where each branch is a tuple containing the prompt\n",
        "        and a list of responses generated by the LLM.\n",
        "    \"\"\"\n",
        "    if llm is None:\n",
        "        raise ValueError(\"An LLM instance must be provided.\")  # Raise error if no LLM is provided\n",
        "\n",
        "    branches = []\n",
        "\n",
        "    def generate_branches(prompt, current_depth):\n",
        "        if current_depth == depth:\n",
        "            return\n",
        "\n",
        "        # Pass the prompt as the 'template' argument\n",
        "        prompt_template = PromptTemplate(input_variables=[], template=prompt)\n",
        "        chain = LLMChain(llm=llm, prompt=prompt_template)  # Use 'prompt' instead of 'prompt_template' here\n",
        "        # Use 'run' to generate responses as it returns a list of strings\n",
        "        responses = [chain.run({}) for _ in range(breadth)]\n",
        "\n",
        "        branches.append((prompt, responses))\n",
        "\n",
        "        for response in responses:\n",
        "            new_prompt = prompt + \" \" + response  # Directly use response as it's already a string\n",
        "            generate_branches(new_prompt, current_depth + 1)\n",
        "\n",
        "    generate_branches(prompt, 0)\n",
        "    return branches\n",
        "\n",
        "\n",
        "def evaluate_branches_with_self_consistency(branches):\n",
        "    \"\"\"\n",
        "    This function evaluates branches using self-consistency (most common response).\n",
        "\n",
        "    Args:\n",
        "        branches: A list of branches generated by the tree_of_thoughts function.\n",
        "\n",
        "    Returns:\n",
        "        The most common response across all branches (or None if no clear winner).\n",
        "    \"\"\"\n",
        "\n",
        "    all_responses = []\n",
        "    for prompt, responses in branches:\n",
        "        # Responses are now strings, so no need for 'get'\n",
        "        all_responses.extend(responses)\n",
        "\n",
        "    response_counter = Counter(all_responses)\n",
        "    most_common_response, count = response_counter.most_common(1)[0]\n",
        "\n",
        "    # Consider responses with a minimum threshold count for better self-consistency\n",
        "    threshold = 2  # Minimum occurrence to consider a response \"common\"\n",
        "    if count >= threshold:\n",
        "        return most_common_response\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Define the initial prompt (replace with your OpenAI API key)\n",
        "initial_prompt = \"How can we improve urban transportation?\"\n",
        "\n",
        "# Initialize your LLM (replace with your actual LLM object)\n",
        "llm = OpenAI(temperature=0) # Replace with your actual LLM object\n",
        "\n",
        "# Generate tree of thoughts\n",
        "thought_tree = tree_of_thoughts(initial_prompt, llm=llm)  # Pass the LLM object\n",
        "\n",
        "# Evaluate and select the best branch with self-consistency\n",
        "best_thought = evaluate_branches_with_self_consistency(thought_tree)\n",
        "print(\"Best Thought Path with Self-Consistency:\", best_thought)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ6PXgMSoRDM",
        "outputId": "0d5ef556-80e4-4485-c38e-afbb5188caab"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Thought Path with Self-Consistency:  that takes into account the needs of all residents, including those with disabilities, and considers the impact on the environment.\n",
            "\n",
            "8. Encourage remote work and flexible schedules: With the rise of remote work, cities can encourage companies to offer flexible work schedules to reduce rush hour traffic and ease the strain on transportation systems.\n",
            "\n",
            "9. Promote electric and alternative fuel vehicles: Governments can provide incentives for people to switch to electric or alternative fuel vehicles, reducing air pollution and dependence on fossil fuels.\n",
            "\n",
            "10. Involve the community: It is important to involve the community in the planning and decision-making process for urban transportation. This can help identify specific needs and concerns and ensure that the solutions implemented are effective and beneficial for all residents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Self Prompting**"
      ],
      "metadata": {
        "id": "eum2YuTKornq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Function to generate a prompt based on a previous response\n",
        "def generate_self_prompt(response):\n",
        "    # You can customize this logic to fit your specific use case\n",
        "    prompt = f\"Based on the following text, generate a continuation or a related question:\\n\\n{response}\\n\\nContinuation or related question:\"\n",
        "    return prompt\n",
        "\n",
        "# Function to get a response from the GPT model\n",
        "def get_gpt_response(prompt):\n",
        "    # Note: Replace 'your_api_key' with your actual OpenAI API key\n",
        "    openai.api_key = 'enter your Api Key'\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"gpt-3.5-turbo\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Initial user-provided prompt\n",
        "initial_prompt = \"Once upon a time, in a land far away, there was a village surrounded by mountains.\"\n",
        "\n",
        "# Get the first response from the GPT model\n",
        "response = get_gpt_response(initial_prompt)\n",
        "print(\"GPT-3 Response:\", response)\n",
        "\n",
        "# Generate self-prompts and responses iteratively\n",
        "for _ in range(5):  # Adjust the range for more or fewer iterations\n",
        "    # Generate a new prompt based on the last response\n",
        "    new_prompt = generate_self_prompt(response)\n",
        "\n",
        "    # Get a new response from the GPT model\n",
        "    response = get_gpt_response(new_prompt)\n",
        "\n",
        "    # Print the response\n",
        "    print(\"GPT-3 Response:\", response)\n"
      ],
      "metadata": {
        "id": "88KWv0Rau2Hr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}