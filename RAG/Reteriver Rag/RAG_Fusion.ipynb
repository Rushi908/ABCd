{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Download the required libraries**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ro8F3Nv3v5fO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCSFfox0vkZR",
        "outputId": "c1e59b2b-e842-4de5-d392-95b8153f478e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.1/163.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.0/869.0 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.3/717.3 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for token (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ed25519 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-community pypdf sentence-transformers openai tiktoken chromadb huggingface-hub token google-generativeai langchain-google-genai unstructured"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "def wrap_text(text, width=90): #preserve_newlines\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n"
      ],
      "metadata": {
        "id": "p6mjPc19ynqT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **load the data**"
      ],
      "metadata": {
        "id": "9phpIAbcwohJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader,PyPDFLoader\n",
        "loader = DirectoryLoader('data', glob='**/*.pdf', loader_cls=PyPDFLoader)\n",
        "documents = loader.load()\n",
        "documents"
      ],
      "metadata": {
        "id": "UGkDfc2NIepK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x7PS3qDynca",
        "outputId": "07cd0c0d-abf3-4645-8f56-48bc11e3bef2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = documents[:50]\n",
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlNlGeW_ynYV",
        "outputId": "44cb0883-92ba-4fa9-a4d6-79ed90ce3db9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert into the chunk**"
      ],
      "metadata": {
        "id": "zUwSpwzIHKN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "V1GMNHTiylXv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define the openi model and embedding model**"
      ],
      "metadata": {
        "id": "9-TI5pW6HOiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]=\"Enter your api Key\""
      ],
      "metadata": {
        "id": "V7A3ECQVzlar"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "nREj_FexylKJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "model = ChatOpenAI(temperature=0.0, model_name=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy5_LQZ7yk_D",
        "outputId": "54eb7268-0377-468c-e99c-231da09297ee"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Store the data in vector Database**"
      ],
      "metadata": {
        "id": "IU5KJUQwHfCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "db = Chroma.from_documents(texts, embeddings,persist_directory=\"./chroma_db\")"
      ],
      "metadata": {
        "id": "zWZ30UB-HSih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is mean by attention?\"\n",
        "\n",
        "db.similarity_search(query, k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhdPH44Syk7e",
        "outputId": "e5215427-9d06-45a4-ed59-7b99c6199e07"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='The ﬁrst attention model was proposed by Bah-\\ndanau et al. (2014), there are several other types of\\nattention proposed, such as the one by Luong et al.', metadata={'page': 4, 'source': 'data/MachineTranslationwithAttention.pdf'}),\n",
              " Document(page_content='2016; Vaswani et al., 2017), language modeling (Raffel et al., 2020), speech-to-text (Chan et al.,\\n2015; Prabhavalkar et al., 2017; Chiu et al., 2018), etc. Many of these architectures make use of\\nattention (Bahdanau et al., 2014), a mechanism that allows the network to focus on a speciﬁc part of\\nthe input most relevant to the current prediction step. Attention has proven to be a critical mechanism;\\nindeed many modern architectures, such as the Transformer, are fully attention-based (Vaswani et al.,\\n2017). However, despite the success of these architectures, an understanding of how said networks\\nsolve such tasks using attention remains largely unknown.\\nAttention mechanisms are attractive because they are interpretable, and often illuminate key com-\\nputations required for a task. For example, consider neural machine translation—trained networks\\nexhibit attention matrices that align words in the encoder sequence with the appropriate correspond-', metadata={'page': 0, 'source': 'data/2110.15253.pdf'}),\n",
              " Document(page_content='work for short sentences and not long sentences.\\nEven though if we used LSTM or GRU, which\\ntends to remember the words that occured very\\nearly in the sequence, it will still not be able to\\nlearn the alignment between the source word and\\nthe target word. They often forget the initial part\\nof the sentence once they are processed in the en-\\ncoder. That is why we use an alignment mecha-\\nnism called attention. They help to memorize this\\ninformation for longer sentences. But we need to\\nlearn this alignment. It can vary from language to\\nlanguage.3.2 Attention\\nIn this section, we will speciﬁcally deﬁne the\\nalignment mechanism, [See Figure 2] that we used\\nin our model. In RNN Encoder-Decoder model\\n(Sutskever et al., 2014), we faced with the bottle-\\nneck problem, where the complete sequence of in-\\nformation of the source sentence, must be captured\\nby one single vector, i.e. the last hidden unit of the\\nencoder RNN is used as a context vector for the', metadata={'page': 4, 'source': 'data/MachineTranslationwithAttention.pdf'}),\n",
              " Document(page_content='encoder RNN is used as a context vector for the\\ndecoder, which becomes difﬁcult for the decoder\\nto summarise large input sequence at once. This\\nalso poses a problem where the encoder is not able\\nto memorize the words coming at the beginning\\nof the sentences, which leads to poor translation\\nof the source sentence. The Attention mechanism\\njust addresses this issue, by retaining and utilising\\nall the hidden state of the input sentence during the\\ndecoding phase.\\nDuring the decoding phase, the model creates\\nan alignment between each time step of the de-\\ncoder output and all of the encoder hidden state.\\nWe need to learn this alignment. Each output of\\nthe decoder can selectively pick out speciﬁc ele-\\nments from the sequence to produce the output.\\nSo, this allows the model to focus and pay more\\n”Attention” to the relevant part of the input se-\\nquence.\\nThe ﬁrst attention model was proposed by Bah-\\ndanau et al. (2014), there are several other types of', metadata={'page': 4, 'source': 'data/MachineTranslationwithAttention.pdf'}),\n",
              " Document(page_content='ing position in the decoder sentence (Ghader & Monz, 2017; Ding et al., 2019). In this case, the\\nattention matrix already contains information about which words in the source sequence are relevant\\nfor translating a particular word in the target sequence; that is, forming the attention matrix itself\\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).arXiv:2110.15253v1  [cs.LG]  28 Oct 2021', metadata={'page': 0, 'source': 'data/2110.15253.pdf'})]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define the retriever**"
      ],
      "metadata": {
        "id": "1opfL93HHoiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reteriver=db.as_retriever()\n",
        "reteriver.get_relevant_documents(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY7JqPQnyk35",
        "outputId": "e3e8b81e-6a79-4bf7-e3ad-75371fa64026"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='The ﬁrst attention model was proposed by Bah-\\ndanau et al. (2014), there are several other types of\\nattention proposed, such as the one by Luong et al.', metadata={'page': 4, 'source': 'data/MachineTranslationwithAttention.pdf'}),\n",
              " Document(page_content='2016; Vaswani et al., 2017), language modeling (Raffel et al., 2020), speech-to-text (Chan et al.,\\n2015; Prabhavalkar et al., 2017; Chiu et al., 2018), etc. Many of these architectures make use of\\nattention (Bahdanau et al., 2014), a mechanism that allows the network to focus on a speciﬁc part of\\nthe input most relevant to the current prediction step. Attention has proven to be a critical mechanism;\\nindeed many modern architectures, such as the Transformer, are fully attention-based (Vaswani et al.,\\n2017). However, despite the success of these architectures, an understanding of how said networks\\nsolve such tasks using attention remains largely unknown.\\nAttention mechanisms are attractive because they are interpretable, and often illuminate key com-\\nputations required for a task. For example, consider neural machine translation—trained networks\\nexhibit attention matrices that align words in the encoder sequence with the appropriate correspond-', metadata={'page': 0, 'source': 'data/2110.15253.pdf'}),\n",
              " Document(page_content='work for short sentences and not long sentences.\\nEven though if we used LSTM or GRU, which\\ntends to remember the words that occured very\\nearly in the sequence, it will still not be able to\\nlearn the alignment between the source word and\\nthe target word. They often forget the initial part\\nof the sentence once they are processed in the en-\\ncoder. That is why we use an alignment mecha-\\nnism called attention. They help to memorize this\\ninformation for longer sentences. But we need to\\nlearn this alignment. It can vary from language to\\nlanguage.3.2 Attention\\nIn this section, we will speciﬁcally deﬁne the\\nalignment mechanism, [See Figure 2] that we used\\nin our model. In RNN Encoder-Decoder model\\n(Sutskever et al., 2014), we faced with the bottle-\\nneck problem, where the complete sequence of in-\\nformation of the source sentence, must be captured\\nby one single vector, i.e. the last hidden unit of the\\nencoder RNN is used as a context vector for the', metadata={'page': 4, 'source': 'data/MachineTranslationwithAttention.pdf'}),\n",
              " Document(page_content='encoder RNN is used as a context vector for the\\ndecoder, which becomes difﬁcult for the decoder\\nto summarise large input sequence at once. This\\nalso poses a problem where the encoder is not able\\nto memorize the words coming at the beginning\\nof the sentences, which leads to poor translation\\nof the source sentence. The Attention mechanism\\njust addresses this issue, by retaining and utilising\\nall the hidden state of the input sentence during the\\ndecoding phase.\\nDuring the decoding phase, the model creates\\nan alignment between each time step of the de-\\ncoder output and all of the encoder hidden state.\\nWe need to learn this alignment. Each output of\\nthe decoder can selectively pick out speciﬁc ele-\\nments from the sequence to produce the output.\\nSo, this allows the model to focus and pay more\\n”Attention” to the relevant part of the input se-\\nquence.\\nThe ﬁrst attention model was proposed by Bah-\\ndanau et al. (2014), there are several other types of', metadata={'page': 4, 'source': 'data/MachineTranslationwithAttention.pdf'})]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chain**"
      ],
      "metadata": {
        "id": "MGj-Ep2aHuDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt=ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain=(\n",
        "    {\"context\":reteriver,\"question\":RunnablePassthrough() }\n",
        "    |prompt\n",
        "    |model\n",
        "    |StrOutputParser()\n",
        ")\n"
      ],
      "metadata": {
        "id": "59MgYljuykz0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_reply = chain.invoke(\"Describe the attention layer ?\")\n",
        "\n",
        "print(wrap_text(text_reply))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBv1Cok54YNo",
        "outputId": "9e7bd84e-173c-499b-9e33-390e61c4deaf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The attention layer is a mechanism that allows the network to focus on a specific part of\n",
            "the input that is most relevant to the current prediction step. It is a critical mechanism\n",
            "in many modern architectures, such as the Transformer, which are fully attention-based.\n",
            "Attention mechanisms are attractive because they are interpretable and often illuminate\n",
            "key computations required for a task. The dynamics of attention can vary across tasks and\n",
            "architectures, and understanding how networks form attention and how they solve tasks\n",
            "using attention remains largely unknown.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **With Rag Fusion**"
      ],
      "metadata": {
        "id": "JHD5cWB85AO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate,ChatPromptTemplate,PromptTemplate"
      ],
      "metadata": {
        "id": "dG8YckMk44_R"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate(input_variables=['original_query'],\n",
        "                            messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(\n",
        "                                input_variables=[],template='You are a helpful assistant that generates multiple search queries based on a single input query.'\n",
        "                                )),\n",
        "                            HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['original_query'], template='Generate multiple search queries related to: {question} \\n OUTPUT (4 queries):'))])\n",
        ""
      ],
      "metadata": {
        "id": "SxDYeYSQ4477"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_query = \"Describe the attention layer ?\""
      ],
      "metadata": {
        "id": "IwdokFmc444s"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_queries = (\n",
        "    prompt | model | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "leOg1ZH56SsY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_queries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn7w-BV56So9",
        "outputId": "f7b1653f-847b-4fd1-f8a1-77379a584ccd"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant that generates multiple search queries based on a single input query.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='Generate multiple search queries related to: {question} \\n OUTPUT (4 queries):'))])\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7876382601f0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x787638261900>, temperature=0.0, openai_api_key='sk-proj-9BUSUpqoa5eIUjtDXQayT3BlbkFJabpGeJYocUgAC9Sy4xnZ', openai_proxy='')\n",
              "| StrOutputParser()\n",
              "| RunnableLambda(lambda x: x.split('\\n'))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reciprocal Rank**"
      ],
      "metadata": {
        "id": "MYHwmGbbINZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    fused_scores = {}\n",
        "    for docs in results:\n",
        "        # Assumes the docs are returned in sorted order of relevance\n",
        "        for rank, doc in enumerate(docs):\n",
        "            doc_str = dumps(doc)\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "    return reranked_results"
      ],
      "metadata": {
        "id": "9Ht373UW6Sl1"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ragfusion_chain = generate_queries | reteriver.map() | reciprocal_rank_fusion"
      ],
      "metadata": {
        "id": "pQHdRX6U6SfU"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.debug = True"
      ],
      "metadata": {
        "id": "Y4Fv_UrK6voe"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ragfusion_chain.input_schema.schema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYqFKUgv6vlC",
        "outputId": "6703416a-1fb4-423b-e7a0-3a75f1154d4b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'PromptInput',\n",
              " 'type': 'object',\n",
              " 'properties': {'question': {'title': 'Question', 'type': 'string'}}}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ragfusion_chain.invoke({\"question\": original_query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNX50oM66viB",
        "outputId": "4c276cbe-cb49-46c2-8769-e05be0f6598e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Describe the attention layer ?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Describe the attention layer ?\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: You are a helpful assistant that generates multiple search queries based on a single input query.\\nHuman: Generate multiple search queries related to: Describe the attention layer ? \\n OUTPUT (4 queries):\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [1.50s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"1. What is the purpose of the attention layer in neural networks?\\n2. How does the attention layer improve the performance of deep learning models?\\n3. Explain the mechanism of the attention layer in natural language processing tasks.\\n4. Compare different types of attention mechanisms used in deep learning models.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"1. What is the purpose of the attention layer in neural networks?\\n2. How does the attention layer improve the performance of deep learning models?\\n3. Explain the mechanism of the attention layer in natural language processing tasks.\\n4. Compare different types of attention mechanisms used in deep learning models.\",\n",
            "            \"response_metadata\": {\n",
            "              \"token_usage\": {\n",
            "                \"completion_tokens\": 58,\n",
            "                \"prompt_tokens\": 46,\n",
            "                \"total_tokens\": 104\n",
            "              },\n",
            "              \"model_name\": \"gpt-3.5-turbo\",\n",
            "              \"system_fingerprint\": null,\n",
            "              \"finish_reason\": \"stop\",\n",
            "              \"logprobs\": null\n",
            "            },\n",
            "            \"type\": \"ai\",\n",
            "            \"id\": \"run-5eaff3ad-ee6a-4140-8d7e-3a6c0da11144-0\",\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 58,\n",
            "      \"prompt_tokens\": 46,\n",
            "      \"total_tokens\": 104\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"1. What is the purpose of the attention layer in neural networks?\\n2. How does the attention layer improve the performance of deep learning models?\\n3. Explain the mechanism of the attention layer in natural language processing tasks.\\n4. Compare different types of attention mechanisms used in deep learning models.\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"1. What is the purpose of the attention layer in neural networks?\\n2. How does the attention layer improve the performance of deep learning models?\\n3. Explain the mechanism of the attention layer in natural language processing tasks.\\n4. Compare different types of attention mechanisms used in deep learning models.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": [\n",
            "    \"1. What is the purpose of the attention layer in neural networks?\",\n",
            "    \"2. How does the attention layer improve the performance of deep learning models?\",\n",
            "    \"3. Explain the mechanism of the attention layer in natural language processing tasks.\",\n",
            "    \"4. Compare different types of attention mechanisms used in deep learning models.\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableEach<VectorStoreRetriever>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": [\n",
            "    \"1. What is the purpose of the attention layer in neural networks?\",\n",
            "    \"2. How does the attention layer improve the performance of deep learning models?\",\n",
            "    \"3. Explain the mechanism of the attention layer in natural language processing tasks.\",\n",
            "    \"4. Compare different types of attention mechanisms used in deep learning models.\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableEach<VectorStoreRetriever>] [534ms] Exiting Chain run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:reciprocal_rank_fusion] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:reciprocal_rank_fusion] [12ms] Exiting Chain run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [2.05s] Exiting Chain run with output:\n",
            "\u001b[0m[outputs]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
            "  warn_beta(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(page_content='2016; Vaswani et al., 2017), language modeling (Raffel et al., 2020), speech-to-text (Chan et al.,\\n2015; Prabhavalkar et al., 2017; Chiu et al., 2018), etc. Many of these architectures make use of\\nattention (Bahdanau et al., 2014), a mechanism that allows the network to focus on a speciﬁc part of\\nthe input most relevant to the current prediction step. Attention has proven to be a critical mechanism;\\nindeed many modern architectures, such as the Transformer, are fully attention-based (Vaswani et al.,\\n2017). However, despite the success of these architectures, an understanding of how said networks\\nsolve such tasks using attention remains largely unknown.\\nAttention mechanisms are attractive because they are interpretable, and often illuminate key com-\\nputations required for a task. For example, consider neural machine translation—trained networks\\nexhibit attention matrices that align words in the encoder sequence with the appropriate correspond-', metadata={'page': 0, 'source': 'data/2110.15253.pdf'}),\n",
              "  0.06666666666666667),\n",
              " (Document(page_content='The ﬁrst attention model was proposed by Bah-\\ndanau et al. (2014), there are several other types of\\nattention proposed, such as the one by Luong et al.', metadata={'page': 4, 'source': 'data/MachineTranslationwithAttention.pdf'}),\n",
              "  0.06530936012691699),\n",
              " (Document(page_content='Sutskever, I., Vinyals, O., and Le, Q. V . Sequence to sequence learning with neural networks. arXiv\\npreprint arXiv:1409.3215 , 2014.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and\\nPolosukhin, I. Attention is all you need. In Advances in neural information processing systems , pp.\\n5998–6008, 2017.\\nWiegreffe, S. and Pinter, Y . Attention is not not explanation, 2019.\\nWu, Y ., Schuster, M., Chen, Z., Le, Q. V ., Norouzi, M., Macherey, W., Krikun, M., Cao, Y ., Gao, Q.,\\nand et al., K. M. Google’s neural machine translation system: Bridging the gap between human\\nand machine translation, 2016.\\nA Additional Details\\nFigure 7: Comparison of the three primary architectures used in this work and the relation between\\nthem. The three architectures are vanilla encoder-decoder (VED), encoder-decoder with attention (AED), and\\nattention only (AO). The encoder RNNs, decoder RNNs, and linear readout layer are showing in orange, purple,', metadata={'page': 11, 'source': 'data/2110.15253.pdf'}),\n",
              "  0.048131080389144903),\n",
              " (Document(page_content='constitutes a signiﬁcant part of solving the overall task. How is it that networks are able to achieve\\nthis? What are the mechanisms underlying how networks form attention, and how do they vary across\\ntasks and architectures?\\nIn this work, we study these questions by analyzing three different encoder-decoder architectures on\\nsequence-to-sequence tasks. We develop a method for decomposing the hidden states of the network\\ninto a sum of components that let us isolate input driven behavior from temporal (or sequence) driven\\nbehavior. We use this to ﬁrst understand how networks solve tasks where all samples use the same\\nattention matrix, a diagonal one. We then build on that to show how additional mechanisms can\\ngenerate sample-dependent attention matrices that are still close to the average matrix.\\nOur Contributions\\n•We propose a decomposition of hidden state dynamics into separate pieces, one of which explains', metadata={'page': 1, 'source': 'data/2110.15253.pdf'}),\n",
              "  0.031746031746031744),\n",
              " (Document(page_content='andthetargetsentencewhichmadetheoptimizationproblem easier.\\n1 Introduction\\nDeep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-\\ncellentperformanceondifﬁcultproblemssuchasspeechrec ognition[13,7]andvisualobjectrecog-\\nnition [19, 6, 21, 20]. DNNs are powerful because they can per formarbitrary parallel computation\\nfor a modest number of steps. A surprising example of the powe r of DNNs is their ability to sort\\nN N-bit numbersusingonly 2 hiddenlayersofquadraticsize [27 ]. So, while neuralnetworksare\\nrelated to conventional statistical models, they learn an i ntricate computation. Furthermore, large\\nDNNscanbetrainedwithsupervisedbackpropagationwhenev erthelabeledtrainingsethasenough\\ninformationto specify the network’sparameters. Thus, if t here exists a parametersetting of a large\\nDNN that achieves good results (for example, because humans can solve the task very rapidly),\\nsupervisedbackpropagationwillﬁndthese parametersands olvetheproblem.', metadata={'page': 0, 'source': 'data/1409.3215.pdf'}),\n",
              "  0.01639344262295082),\n",
              " (Document(page_content='work for short sentences and not long sentences.\\nEven though if we used LSTM or GRU, which\\ntends to remember the words that occured very\\nearly in the sequence, it will still not be able to\\nlearn the alignment between the source word and\\nthe target word. They often forget the initial part\\nof the sentence once they are processed in the en-\\ncoder. That is why we use an alignment mecha-\\nnism called attention. They help to memorize this\\ninformation for longer sentences. But we need to\\nlearn this alignment. It can vary from language to\\nlanguage.3.2 Attention\\nIn this section, we will speciﬁcally deﬁne the\\nalignment mechanism, [See Figure 2] that we used\\nin our model. In RNN Encoder-Decoder model\\n(Sutskever et al., 2014), we faced with the bottle-\\nneck problem, where the complete sequence of in-\\nformation of the source sentence, must be captured\\nby one single vector, i.e. the last hidden unit of the\\nencoder RNN is used as a context vector for the', metadata={'page': 4, 'source': 'data/MachineTranslationwithAttention.pdf'}),\n",
              "  0.016129032258064516),\n",
              " (Document(page_content='Bañón, M., Chen, P., Haddow, B., Heaﬁeld, K., Hoang, H., Esplà-Gomis, M., Forcada, M. L.,\\nKamran, A., Kirefu, F., Koehn, P., Ortiz Rojas, S., Pla Sempere, L., Ramírez-Sánchez, G.,\\nSarrías, E., Strelec, M., Thompson, B., Waites, W., Wiggins, D., and Zaragoza, J. ParaCrawl:\\nWeb-scale acquisition of parallel corpora. In Proceedings of the 58th Annual Meeting of the\\nAssociation for Computational Linguistics , pp. 4555–4567, Online, July 2020. Association for\\nComputational Linguistics. doi: 10.18653/v1/2020.acl-main.417. URL https://www.aclweb.\\norg/anthology/2020.acl-main.417 .\\nBastings, J. and Filippova, K. The elephant in the interpretability room: Why use attention as\\nexplanation when we have saliency methods?, 2020.\\nChan, W., Jaitly, N., Le, Q. V ., and Vinyals, O. Listen, attend and spell, 2015.\\nChefer, H., Gur, S., and Wolf, L. Transformer interpretability beyond attention visualization, 2020.', metadata={'page': 10, 'source': 'data/2110.15253.pdf'}),\n",
              "  0.015873015873015872)]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "full_rag_fusion_chain = (\n",
        "    {\n",
        "        \"context\": ragfusion_chain,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "4ambHoPH6ve6"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_rag_fusion_chain.input_schema.schema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gemtv_b47T80",
        "outputId": "a3cb7ed7-c680-4878-9c64-8f3e8fb3ce65"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'RunnableParallel<context,question>Input',\n",
              " 'type': 'object',\n",
              " 'properties': {'question': {'title': 'Question', 'type': 'string'}}}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_rag_fusion_chain.invoke({\"question\": \"Describe the attention layer ?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e50WG_tl7T5S",
        "outputId": "cadfbc5d-33fa-410e-af78-a0f1b3647967"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Describe the attention layer ?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Describe the attention layer ?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Describe the attention layer ?\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Describe the attention layer ?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Describe the attention layer ?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Describe the attention layer ?\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: You are a helpful assistant that generates multiple search queries based on a single input query.\\nHuman: Generate multiple search queries related to: Describe the attention layer ? \\n OUTPUT (4 queries):\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > llm:ChatOpenAI] [1.52s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"1. What is the purpose of the attention layer in neural networks?\\n2. How does the attention layer improve the performance of deep learning models?\\n3. Explain the mechanism of the attention layer in natural language processing tasks.\\n4. Compare different types of attention mechanisms used in deep learning models.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"1. What is the purpose of the attention layer in neural networks?\\n2. How does the attention layer improve the performance of deep learning models?\\n3. Explain the mechanism of the attention layer in natural language processing tasks.\\n4. Compare different types of attention mechanisms used in deep learning models.\",\n",
            "            \"response_metadata\": {\n",
            "              \"token_usage\": {\n",
            "                \"completion_tokens\": 58,\n",
            "                \"prompt_tokens\": 46,\n",
            "                \"total_tokens\": 104\n",
            "              },\n",
            "              \"model_name\": \"gpt-3.5-turbo\",\n",
            "              \"system_fingerprint\": null,\n",
            "              \"finish_reason\": \"stop\",\n",
            "              \"logprobs\": null\n",
            "            },\n",
            "            \"type\": \"ai\",\n",
            "            \"id\": \"run-a717d154-fa67-4d51-b21a-af6d0f862b8f-0\",\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 58,\n",
            "      \"prompt_tokens\": 46,\n",
            "      \"total_tokens\": 104\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"1. What is the purpose of the attention layer in neural networks?\\n2. How does the attention layer improve the performance of deep learning models?\\n3. Explain the mechanism of the attention layer in natural language processing tasks.\\n4. Compare different types of attention mechanisms used in deep learning models.\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"1. What is the purpose of the attention layer in neural networks?\\n2. How does the attention layer improve the performance of deep learning models?\\n3. Explain the mechanism of the attention layer in natural language processing tasks.\\n4. Compare different types of attention mechanisms used in deep learning models.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": [\n",
            "    \"1. What is the purpose of the attention layer in neural networks?\",\n",
            "    \"2. How does the attention layer improve the performance of deep learning models?\",\n",
            "    \"3. Explain the mechanism of the attention layer in natural language processing tasks.\",\n",
            "    \"4. Compare different types of attention mechanisms used in deep learning models.\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:RunnableEach<VectorStoreRetriever>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": [\n",
            "    \"1. What is the purpose of the attention layer in neural networks?\",\n",
            "    \"2. How does the attention layer improve the performance of deep learning models?\",\n",
            "    \"3. Explain the mechanism of the attention layer in natural language processing tasks.\",\n",
            "    \"4. Compare different types of attention mechanisms used in deep learning models.\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:RunnableEach<VectorStoreRetriever>] [253ms] Exiting Chain run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:reciprocal_rank_fusion] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:reciprocal_rank_fusion] [1ms] Exiting Chain run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence] [1.78s] Exiting Chain run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] [1.80s] Exiting Chain run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: Answer the question based only on the following context:\\n[(Document(page_content='2016; Vaswani et al., 2017), language modeling (Raffel et al., 2020), speech-to-text (Chan et al.,\\\\n2015; Prabhavalkar et al., 2017; Chiu et al., 2018), etc. Many of these architectures make use of\\\\nattention (Bahdanau et al., 2014), a mechanism that allows the network to focus on a speciﬁc part of\\\\nthe input most relevant to the current prediction step. Attention has proven to be a critical mechanism;\\\\nindeed many modern architectures, such as the Transformer, are fully attention-based (Vaswani et al.,\\\\n2017). However, despite the success of these architectures, an understanding of how said networks\\\\nsolve such tasks using attention remains largely unknown.\\\\nAttention mechanisms are attractive because they are interpretable, and often illuminate key com-\\\\nputations required for a task. For example, consider neural machine translation—trained networks\\\\nexhibit attention matrices that align words in the encoder sequence with the appropriate correspond-', metadata={'page': 0, 'source': 'data/2110.15253.pdf'}), 0.06666666666666667), (Document(page_content='The ﬁrst attention model was proposed by Bah-\\\\ndanau et al. (2014), there are several other types of\\\\nattention proposed, such as the one by Luong et al.', metadata={'page': 4, 'source': 'data/MachineTranslationwithAttention.pdf'}), 0.06530936012691699), (Document(page_content='Sutskever, I., Vinyals, O., and Le, Q. V . Sequence to sequence learning with neural networks. arXiv\\\\npreprint arXiv:1409.3215 , 2014.\\\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and\\\\nPolosukhin, I. Attention is all you need. In Advances in neural information processing systems , pp.\\\\n5998–6008, 2017.\\\\nWiegreffe, S. and Pinter, Y . Attention is not not explanation, 2019.\\\\nWu, Y ., Schuster, M., Chen, Z., Le, Q. V ., Norouzi, M., Macherey, W., Krikun, M., Cao, Y ., Gao, Q.,\\\\nand et al., K. M. Google’s neural machine translation system: Bridging the gap between human\\\\nand machine translation, 2016.\\\\nA Additional Details\\\\nFigure 7: Comparison of the three primary architectures used in this work and the relation between\\\\nthem. The three architectures are vanilla encoder-decoder (VED), encoder-decoder with attention (AED), and\\\\nattention only (AO). The encoder RNNs, decoder RNNs, and linear readout layer are showing in orange, purple,', metadata={'page': 11, 'source': 'data/2110.15253.pdf'}), 0.048131080389144903), (Document(page_content='constitutes a signiﬁcant part of solving the overall task. How is it that networks are able to achieve\\\\nthis? What are the mechanisms underlying how networks form attention, and how do they vary across\\\\ntasks and architectures?\\\\nIn this work, we study these questions by analyzing three different encoder-decoder architectures on\\\\nsequence-to-sequence tasks. We develop a method for decomposing the hidden states of the network\\\\ninto a sum of components that let us isolate input driven behavior from temporal (or sequence) driven\\\\nbehavior. We use this to ﬁrst understand how networks solve tasks where all samples use the same\\\\nattention matrix, a diagonal one. We then build on that to show how additional mechanisms can\\\\ngenerate sample-dependent attention matrices that are still close to the average matrix.\\\\nOur Contributions\\\\n•We propose a decomposition of hidden state dynamics into separate pieces, one of which explains', metadata={'page': 1, 'source': 'data/2110.15253.pdf'}), 0.031746031746031744), (Document(page_content='andthetargetsentencewhichmadetheoptimizationproblem easier.\\\\n1 Introduction\\\\nDeep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-\\\\ncellentperformanceondifﬁcultproblemssuchasspeechrec ognition[13,7]andvisualobjectrecog-\\\\nnition [19, 6, 21, 20]. DNNs are powerful because they can per formarbitrary parallel computation\\\\nfor a modest number of steps. A surprising example of the powe r of DNNs is their ability to sort\\\\nN N-bit numbersusingonly 2 hiddenlayersofquadraticsize [27 ]. So, while neuralnetworksare\\\\nrelated to conventional statistical models, they learn an i ntricate computation. Furthermore, large\\\\nDNNscanbetrainedwithsupervisedbackpropagationwhenev erthelabeledtrainingsethasenough\\\\ninformationto specify the network’sparameters. Thus, if t here exists a parametersetting of a large\\\\nDNN that achieves good results (for example, because humans can solve the task very rapidly),\\\\nsupervisedbackpropagationwillﬁndthese parametersands olvetheproblem.', metadata={'page': 0, 'source': 'data/1409.3215.pdf'}), 0.01639344262295082), (Document(page_content='work for short sentences and not long sentences.\\\\nEven though if we used LSTM or GRU, which\\\\ntends to remember the words that occured very\\\\nearly in the sequence, it will still not be able to\\\\nlearn the alignment between the source word and\\\\nthe target word. They often forget the initial part\\\\nof the sentence once they are processed in the en-\\\\ncoder. That is why we use an alignment mecha-\\\\nnism called attention. They help to memorize this\\\\ninformation for longer sentences. But we need to\\\\nlearn this alignment. It can vary from language to\\\\nlanguage.3.2 Attention\\\\nIn this section, we will speciﬁcally deﬁne the\\\\nalignment mechanism, [See Figure 2] that we used\\\\nin our model. In RNN Encoder-Decoder model\\\\n(Sutskever et al., 2014), we faced with the bottle-\\\\nneck problem, where the complete sequence of in-\\\\nformation of the source sentence, must be captured\\\\nby one single vector, i.e. the last hidden unit of the\\\\nencoder RNN is used as a context vector for the', metadata={'page': 4, 'source': 'data/MachineTranslationwithAttention.pdf'}), 0.016129032258064516), (Document(page_content='Bañón, M., Chen, P., Haddow, B., Heaﬁeld, K., Hoang, H., Esplà-Gomis, M., Forcada, M. L.,\\\\nKamran, A., Kirefu, F., Koehn, P., Ortiz Rojas, S., Pla Sempere, L., Ramírez-Sánchez, G.,\\\\nSarrías, E., Strelec, M., Thompson, B., Waites, W., Wiggins, D., and Zaragoza, J. ParaCrawl:\\\\nWeb-scale acquisition of parallel corpora. In Proceedings of the 58th Annual Meeting of the\\\\nAssociation for Computational Linguistics , pp. 4555–4567, Online, July 2020. Association for\\\\nComputational Linguistics. doi: 10.18653/v1/2020.acl-main.417. URL https://www.aclweb.\\\\norg/anthology/2020.acl-main.417 .\\\\nBastings, J. and Filippova, K. The elephant in the interpretability room: Why use attention as\\\\nexplanation when we have saliency methods?, 2020.\\\\nChan, W., Jaitly, N., Le, Q. V ., and Vinyals, O. Listen, attend and spell, 2015.\\\\nChefer, H., Gur, S., and Wolf, L. Transformer interpretability beyond attention visualization, 2020.', metadata={'page': 10, 'source': 'data/2110.15253.pdf'}), 0.015873015873015872)]\\n\\nQuestion: {'question': 'Describe the attention layer ?'}\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [2.33s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"The attention layer is a mechanism that allows the network to focus on a specific part of the input that is most relevant to the current prediction step. It is a critical mechanism used in many modern architectures, such as the Transformer, which are fully attention-based. Attention mechanisms are attractive because they are interpretable and often illuminate key computations required for a task. In neural machine translation, trained networks exhibit attention matrices that align words in the encoder sequence with the appropriate corresponding words in the target sentence, making it easier to optimize the overall task.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"The attention layer is a mechanism that allows the network to focus on a specific part of the input that is most relevant to the current prediction step. It is a critical mechanism used in many modern architectures, such as the Transformer, which are fully attention-based. Attention mechanisms are attractive because they are interpretable and often illuminate key computations required for a task. In neural machine translation, trained networks exhibit attention matrices that align words in the encoder sequence with the appropriate corresponding words in the target sentence, making it easier to optimize the overall task.\",\n",
            "            \"response_metadata\": {\n",
            "              \"token_usage\": {\n",
            "                \"completion_tokens\": 106,\n",
            "                \"prompt_tokens\": 1842,\n",
            "                \"total_tokens\": 1948\n",
            "              },\n",
            "              \"model_name\": \"gpt-3.5-turbo\",\n",
            "              \"system_fingerprint\": null,\n",
            "              \"finish_reason\": \"stop\",\n",
            "              \"logprobs\": null\n",
            "            },\n",
            "            \"type\": \"ai\",\n",
            "            \"id\": \"run-1c0e349a-916e-45e5-8106-89e1e2318d8b-0\",\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 106,\n",
            "      \"prompt_tokens\": 1842,\n",
            "      \"total_tokens\": 1948\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"The attention layer is a mechanism that allows the network to focus on a specific part of the input that is most relevant to the current prediction step. It is a critical mechanism used in many modern architectures, such as the Transformer, which are fully attention-based. Attention mechanisms are attractive because they are interpretable and often illuminate key computations required for a task. In neural machine translation, trained networks exhibit attention matrices that align words in the encoder sequence with the appropriate corresponding words in the target sentence, making it easier to optimize the overall task.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [4.15s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"The attention layer is a mechanism that allows the network to focus on a specific part of the input that is most relevant to the current prediction step. It is a critical mechanism used in many modern architectures, such as the Transformer, which are fully attention-based. Attention mechanisms are attractive because they are interpretable and often illuminate key computations required for a task. In neural machine translation, trained networks exhibit attention matrices that align words in the encoder sequence with the appropriate corresponding words in the target sentence, making it easier to optimize the overall task.\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The attention layer is a mechanism that allows the network to focus on a specific part of the input that is most relevant to the current prediction step. It is a critical mechanism used in many modern architectures, such as the Transformer, which are fully attention-based. Attention mechanisms are attractive because they are interpretable and often illuminate key computations required for a task. In neural machine translation, trained networks exhibit attention matrices that align words in the encoder sequence with the appropriate corresponding words in the target sentence, making it easier to optimize the overall task.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3M0YfYUe7Ty5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}