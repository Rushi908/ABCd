{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain openai langchain langchain-community pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc5BzQyxVK1v",
        "outputId": "bcfb004a-13ab-4ccb-ad44-0c1f6690f562"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/290.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/290.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]=\"Enter your API KEY\""
      ],
      "metadata": {
        "id": "KSg1rezHM1cJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import chat_models\n",
        "llm=chat_models.ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "4lA9YxAMYytl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic prompt Summarization**"
      ],
      "metadata": {
        "id": "jcMUw50vW4iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage,AIMessage,SystemMessage"
      ],
      "metadata": {
        "id": "WUGS1frBV5NA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "speech=\"\"\"\n",
        "People across the country, involved in government, political, and social activities, are dedicating their time to make the ‘Viksit Bharat Sankalp Yatra’ (Developed India Resolution Journey) successful. Therefore, as a Member of Parliament, it was my responsibility to also contribute my time to this program. So, today, I have come here just as a Member of Parliament and your ‘sevak’, ready to participate in this program, much like you.\n",
        "\n",
        "In our country, governments have come and gone, numerous schemes have been formulated, discussions have taken place, and big promises have been made. However, my experience and observations led me to believe that the most critical aspect that requires attention is ensuring that the government’s plans reach the intended beneficiaries without any hassles. If there is a ‘Pradhan Mantri Awas Yojana’ (Prime Minister’s housing scheme), then those who are living in jhuggis and slums should get their houses. And he should not need to make rounds of the government offices for this purpose. The government should reach him. Since you have assigned this responsibility to me, about four crore families have got their ‘pucca’ houses. However, I have encountered cases where someone is left out of the government benefits. Therefore, I have decided to tour the country again, to listen to people’s experiences with government schemes, to understand whether they received the intended benefits, and to ensure that the programs are reaching everyone as planned without paying any bribes. We will get the real picture if we visit them again. Therefore, this ‘Viksit Bharat Sankalp Yatra’ is, in a way, my own examination. I want to hear from you and the people across the country whether what I envisioned and the work I have been doing aligns with reality and whether it has reached those for whom it was meant.\n",
        "\n",
        "It is crucial to check whether the work that was supposed to happen has indeed taken place. I recently met some individuals who utilized the Ayushman card to get treatment for serious illnesses. One person met with a severe accident, and after using the card, he could afford the necessary operation, and now he is recovering well. When I asked him, he said: “How could I afford this treatment? Now that there is the Ayushman card, I mustered courage and underwent an operation. Now I am perfectly fine.”  Such stories are blessings to me.\n",
        "\n",
        "The bureaucrats, who prepare good schemes, expedite the paperwork and even allocate funds, also feel satisfied that 50 or 100 people who were supposed to get the funds have got it. The funds meant for a thousand villages have been released. But their job satisfaction peaks when they hear that their work has directly impacted someone’s life positively. When they see the tangible results of their efforts, their enthusiasm multiplies. They feel satisfied. Therefore, ‘Viksit Bharat Sankalp Yatra’ has had a positive impact on government officers. It has made them more enthusiastic about their work, especially when they witness the tangible benefits reaching the people. Officers now feel satisfied with their work, saying, “I made a good plan, I created a file, and the intended beneficiaries received the benefits.” When they find that the money has reached a poor widow under the Jeevan Jyoti scheme and it was a great help to her during her crisis, they realise that they have done a good job. When a government officer listens to such stories, he feels very satisfied.\n",
        "\n",
        "There are very few who understand the power and impact of the ‘Viksit Bharat Sankalp Yatra’. When I hear people connected to bureaucratic circles talking about it, expressing their satisfaction, it resonates with me. I’ve heard stories where someone suddenly received 2 lakh rupees after the death of her husband, and a sister mentioned how the arrival of gas in her home transformed her lives. The most significant aspect is when someone says that the line between rich and poor has vanished. While the slogan ‘Garibi Hatao’ (Remove Poverty) is one thing, but the real change happens when a person says, “As soon as the gas stove came to my house, the distinction between poverty and affluence disappeared.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "16UdOx9hV5J4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_messages=[\n",
        "    SystemMessage(content='You are an expert assistant with expertize in summarizing speeches'),\n",
        "    HumanMessage(content=f'Please provide a short and concise summary of the following speech:\\n TEXT: {speech}')\n",
        "]\n",
        "llm=llm"
      ],
      "metadata": {
        "id": "o5_rC5JiV5Gx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##total tokens\n",
        "print(llm.get_num_tokens(speech))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLe65q2DYKSC",
        "outputId": "158d79b4-ff50-40fc-9b8f-752d5e76439e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm(chat_messages[1].content)  # Pass the content of the HumanMessage\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCHc6fMeYPw1",
        "outputId": "e8ca1f1b-9237-459e-af8e-30da51f2ac56"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The speaker, a Member of Parliament, is participating in the 'Viksit Bharat Sankalp Yatra' to ensure government schemes reach intended beneficiaries without any hassles. They have encountered cases where people are left out of benefits and are touring the country to listen to people's experiences with government programs. The Yatra has had a positive impact on government officers, making them more enthusiastic about their work when they see tangible benefits reaching people. The speaker emphasizes the importance of ensuring that government plans are actually implemented and positively impact people's lives.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##get_summary\n",
        "print(llm(chat_messages).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzbCBFcKY-tl",
        "outputId": "6680118d-0825-4960-dadc-0dcfb69a4774"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The speech emphasizes the importance of ensuring government schemes reach the intended beneficiaries without hassles. The speaker, a Member of Parliament, shares experiences of providing housing through schemes like Pradhan Mantri Awas Yojana and Ayushman card for healthcare. The 'Viksit Bharat Sankalp Yatra' aims to evaluate the impact of government programs and ensure tangible benefits reach the people. Positive stories of individuals benefiting from government schemes highlight the satisfaction of both officials and recipients. The Yatra has inspired enthusiasm among government officers and bridged the gap between rich and poor, showcasing the tangible impact of the initiatives.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompt template for text Summarization**"
      ],
      "metadata": {
        "id": "nBlvmEh5atPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "generic_prompt=\"\"\"\n",
        "write the summary of the following speech:\n",
        "speech: {speech}\n",
        "Translate the precise summary to {language}.\n",
        "\"\"\"\n",
        "prompt=PromptTemplate(template=generic_prompt,input_variables=[\"speech\",\"language\"])\n"
      ],
      "metadata": {
        "id": "TPONrWtFasTu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.format(speech=speech,language=\"Marathi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "54_mDOnubgeV",
        "outputId": "2b271183-35b5-431c-d5f9-13a7a92bcf46"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwrite the summary of the following speech:\\nspeech: \\nPeople across the country, involved in government, political, and social activities, are dedicating their time to make the ‘Viksit Bharat Sankalp Yatra’ (Developed India Resolution Journey) successful. Therefore, as a Member of Parliament, it was my responsibility to also contribute my time to this program. So, today, I have come here just as a Member of Parliament and your ‘sevak’, ready to participate in this program, much like you.\\n\\nIn our country, governments have come and gone, numerous schemes have been formulated, discussions have taken place, and big promises have been made. However, my experience and observations led me to believe that the most critical aspect that requires attention is ensuring that the government’s plans reach the intended beneficiaries without any hassles. If there is a ‘Pradhan Mantri Awas Yojana’ (Prime Minister’s housing scheme), then those who are living in jhuggis and slums should get their houses. And he should not need to make rounds of the government offices for this purpose. The government should reach him. Since you have assigned this responsibility to me, about four crore families have got their ‘pucca’ houses. However, I have encountered cases where someone is left out of the government benefits. Therefore, I have decided to tour the country again, to listen to people’s experiences with government schemes, to understand whether they received the intended benefits, and to ensure that the programs are reaching everyone as planned without paying any bribes. We will get the real picture if we visit them again. Therefore, this ‘Viksit Bharat Sankalp Yatra’ is, in a way, my own examination. I want to hear from you and the people across the country whether what I envisioned and the work I have been doing aligns with reality and whether it has reached those for whom it was meant.\\n\\nIt is crucial to check whether the work that was supposed to happen has indeed taken place. I recently met some individuals who utilized the Ayushman card to get treatment for serious illnesses. One person met with a severe accident, and after using the card, he could afford the necessary operation, and now he is recovering well. When I asked him, he said: “How could I afford this treatment? Now that there is the Ayushman card, I mustered courage and underwent an operation. Now I am perfectly fine.”  Such stories are blessings to me.\\n\\nThe bureaucrats, who prepare good schemes, expedite the paperwork and even allocate funds, also feel satisfied that 50 or 100 people who were supposed to get the funds have got it. The funds meant for a thousand villages have been released. But their job satisfaction peaks when they hear that their work has directly impacted someone’s life positively. When they see the tangible results of their efforts, their enthusiasm multiplies. They feel satisfied. Therefore, ‘Viksit Bharat Sankalp Yatra’ has had a positive impact on government officers. It has made them more enthusiastic about their work, especially when they witness the tangible benefits reaching the people. Officers now feel satisfied with their work, saying, “I made a good plan, I created a file, and the intended beneficiaries received the benefits.” When they find that the money has reached a poor widow under the Jeevan Jyoti scheme and it was a great help to her during her crisis, they realise that they have done a good job. When a government officer listens to such stories, he feels very satisfied.\\n\\nThere are very few who understand the power and impact of the ‘Viksit Bharat Sankalp Yatra’. When I hear people connected to bureaucratic circles talking about it, expressing their satisfaction, it resonates with me. I’ve heard stories where someone suddenly received 2 lakh rupees after the death of her husband, and a sister mentioned how the arrival of gas in her home transformed her lives. The most significant aspect is when someone says that the line between rich and poor has vanished. While the slogan ‘Garibi Hatao’ (Remove Poverty) is one thing, but the real change happens when a person says, “As soon as the gas stove came to my house, the distinction between poverty and affluence disappeared.\\n\\nTranslate the precise summary to Marathi.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
        "summary=llm_chain.run({'speech':speech,'language':'Marathi'})"
      ],
      "metadata": {
        "id": "fuW3V-qUb7xz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "MegZE-bSb7uh",
        "outputId": "3fb51961-ec30-45d6-9d62-ce2c919ca353"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"भारतात विकसित भारत संकल्प यात्रेच्या सफलतेसाठी सर्वत्र सरकार, राजकीय आणि सामाजिक क्रियाकलापांमध्ये सहभागी असणारे लोक आपले वेळ समर्पित करत आहेत. त्यामुळे, सदस्य आमदार म्हणून, या कार्यक्रमात सहभागी होऊ आमच्याला कर्तव्य मानलं. म्हणजे, आज मी इथे हेच सदस्य आमदार आणि तुमचा 'सेवक' म्हणून आलो आहे, या कार्यक्रमात सहभागी होऊ तुमच्यापेक्षा कितीतरी.\\n\\nआमच्या देशात सरकारे येत गेल्याआणि जात गेल्या, अनेक योजना स्थापित केल्या गेल्या, चर्चा झाली आणि मोठ्या वचनांची साथ झाली. पण, माझ्या अनुभवानुसार, सरकारच्या योजनांच्या लक्षात लागणार्या योग्यउंटांचं विना-विचार करण्याची एक महत्त्वाची अवस्था आहे. जर 'प्रधानमंत्री आवास योजना' असेल तर, त्यांना झोपड्यांमध्ये राहत असलेल्या व्यक्तींना त्यांच्या घरांत पोहोचावे लागतं. आणि त्याच्या उद्देशानुसार योजना पोहोचवायला त्यांना सरकार काहीही म्हणिलं नसेल. सरकार त्यांना पोहोचू लागवी. ज्यामुळे तुम्हाला या जबाबदारीस नियुक्त केली आहे, चार कोटी कुटुंबांना त्यांच्या 'पुक्का' घरांचं मिळालं आहे. पण, मला एका कितीतरी व्यक्तींच्या योजनांमध्ये खाली राहिलेलं आहे. म्हणून, मी पुन्हा देशात भ्रमण करण्याची निर्णय घेतलं आहे, लोकांच्या सरकारच्या योजनांबद्दलचा अनुभव सुनवण्याचा, समजण्याचा आणि सुनवण्याचा उद्देश्य साधण्याचा. आम्हाला त्यांच्या विचारांची खरी चित्रित करायला हवं. पण, हे 'विकसित भारत संकल्प यात्रा' एक प्रकारचं माझं स्वत:चं म्हणजे परीक्षा आहे. मी तुमच्याच आणि देशातील लोकांना ऐकून पाहू इच्छितो, माझ्या लक्ष्यांचं आणि मी काम करत आहे त्यातलं वास्तविकपणाचं समान आहे का आणि तो त्यांच्यासाठी असलं आहे का हे कितीतरी.\\n\\nप्रशासकीय वर्गांशी संबंधित लोकांनी ‘विकसित भारत संकल्प यात्रा’ ची शक्ती आणि परिणाम समझलेली असते. मी त्यांच्याबरोबर वाचलेल्या कथा पाहून, त्यांनी आत्मसंतोष कसं वाटतंय ते मला समजतंय. मी काही व्यक्तींच्या दुखी आजारांसाठी उपयुक्त उपचार सापडल्यासाठी आयुष्मान कार्ड वापरल्याचं अनुभवलं. एक व्यक्तीने गंभीर दुर्घटनेनंतर आयुष्मान कार्ड वापरल्यावर, तेणे आवश्यक ऑपरेशनचं खर्च करू शकलं आणि आता तो चांगला वाटतो. मी त्यांना विचारल्यावर, तो म्हणाला: “मी हे उपचार कसे देऊ शकतो? आता जर आयुष्मान कार्ड आहे, तर मी साहस जमा केला आणि ऑपरेशन केलं. आता मी पूर्णपणे बरा आहे.” ज्या कथांमुळे हे त्याला आशीर्वाद वाटतात.\\n\\nयोजना तयार करणारे अधिकाऱ्यांनी आपल्या कर्याची संतोषजनकता वाढविली, कागदपत्रकारिता वाढविली आणि कर्ज नियुक्त झालेले. पण त्यांच्या कामाची संतोषजनकता वाढते जेणेकरून त्यांना ऐकाच विकटं लाभेत गेलं. त्यांच्या प्रयत्नांचं वाटायला त्यांची उत्साहावाढ होते. त्यांनी आत्ता आपल्या कामावर संतोष वाटतंय, म्हणून वाटतंय, “मी एक उत्तम योजना बनवली, मी एक फाईल तयार केली, आणि इच्छित लाभार्थ्यांना त्यांना मिळालं.” जर त्यांनी पूर्ण जीवन ज्योती योजनेतील गरीब विधवेच्या हातात रुपये पोहोचविले असलं आणि त्यांच्या संकटांच्या काळात त्यांना मदत करणारं असलं, तर त्यांना वाटतंय की त्यांनी एक उत्तम काम केलं. जर एक सरकारी अधिकाऱ्यांनी त्याच्या प्रसंगांनी ऐकलं तर, त्याला अत्यंत संतोष वाटतो.\\n\\n'विकसित भारत संकल्प यात्रेच्या' शक्ती आणि परिणामांची किंमत समजणारं लोक कमी आहेत. मी आश्चर्य करतो की ब्यूरोक्रेटिक वर्गांशी संबंधित लोक त्याची आलंबालंबी करणारं आणि संतोष वाटणारं आहेत. मी कथा ऐकल्यावर कदाचित एका व्यक्तीला त्याच्या पतीच्या मृत्यूनंतर अचानक २ लाख रुपये मिळाले होते, आणि एक बहीण ने म्हटलं की त्यांच्या घरात गॅस येऊन त्यांच जीवनात कसा बदलला. सर्वात महत्वाचं असलं तेंव्हा होतं, किव्हा एक व्यक्ती म्हणतो की, “जेव्हा गॅस स्टोव्ह माझ्या घरात येतं, तेव्हा गरिबी आणि धनवानपणाचा वेगळा वेगळा अंत झाला.”\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StuffDocumentChain Text Summarization\n",
        "\n",
        "This chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window of the LLM you are using."
      ],
      "metadata": {
        "id": "sVS2n_8aOXno"
      }
    },
    {
      "source": [
        "from pypdf import PdfReader\n",
        "\n",
        "pdf_loader = PdfReader(\"/content/1 bit llm.pdf\")  # Create PdfReader instance\n",
        "\n",
        "# read text from pdf\n",
        "text = ''\n",
        "for i, page in enumerate(pdf_loader.pages):  # Iterate over pages of the pdf_loader instance\n",
        "    content = page.extract_text()\n",
        "    if content:\n",
        "        text += content"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rDeB6iake9N9"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "1qPDG3wmpcTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "docs=[Document(page_content=text)]"
      ],
      "metadata": {
        "id": "UpDFYvvapXRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import load_summarize_chain\n",
        "from langchain.docstore.document import Document\n",
        "template=\"\"\"\n",
        "write the conise and short summary of the following speech:\n",
        "speech: {text}\n",
        "\n",
        "\"\"\"\n",
        "prompt=PromptTemplate(template=template,input_variables=[\"text\"])"
      ],
      "metadata": {
        "id": "g1uZSFl6c0CN"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain=load_summarize_chain(llm,chain_type=\"stuff\",prompt=prompt)\n",
        "output_summary = chain.run(docs)"
      ],
      "metadata": {
        "id": "HjVU26Gacz-V"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9NRj5Jgcz6l",
        "outputId": "18010f71-960e-4a8a-a545-323add0171a3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The speech discusses the introduction of a new era of 1-bit Large Language Models (LLMs) with the BitNet b1.58 variant. This model uses ternary values {-1, 0, 1} for every parameter, offering high performance and cost-effectiveness compared to full-precision models. BitNet b1.58 shows improved efficiency in terms of latency, memory, throughput, and energy consumption, making it a Pareto improvement over existing LLMs. The speech also highlights the potential for new hardware optimized for 1-bit LLMs and the model's application in edge and mobile devices. Additionally, the model's performance is compared to full-precision baselines across various tasks and model sizes, demonstrating its effectiveness and scalability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summarizing the Large documents using the Map Reduce**\n",
        "\n",
        "this chain first passes each document through an LLM, then reduces them using the ReduceDocumentsChain. It is useful in the same situations as ReduceDocumentsChain, but does an initial LLM call before trying to reduce the documents."
      ],
      "metadata": {
        "id": "_TedBpjKiU5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import load_summarize_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "0pk7RwbXiI7c"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "pdf_reader = PdfReader(\"/content/Quantization.pdf\")\n",
        "text = ''\n",
        "for i, page in enumerate(pdf_reader.pages):\n",
        "    content = page.extract_text()\n",
        "    if content:\n",
        "        text += content"
      ],
      "metadata": {
        "id": "ifUOPMoAczyd"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.get_num_tokens(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMTe-adVj-eN",
        "outputId": "f40a0d96-b277-4ea4-dc37-b070fbcf5648"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30506"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Splittting the text\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=20)\n",
        "chunks = text_splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "iOqDUzuCczu-"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6x_YcWGczm0",
        "outputId": "b3c1d7f3-6e23-40e9-9a7d-990eb4c970ad"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain=load_summarize_chain(llm,chain_type=\"map_reduce\",verbose=True)\n",
        "output_summary = chain.run(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hTEmTZ8cziw",
        "outputId": "a32e123e-9d34-4f71-94c8-6b3354bfbf69"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"A White Paper on Neural Network Quantization\n",
            "Markus Nagel∗\n",
            "Qualcomm AI Research†\n",
            "markusn@qti.qualcomm.comMarios Fournarakis∗\n",
            "Qualcomm AI Research†\n",
            "mfournar@qti.qualcomm.com\n",
            "Rana Ali Amjad\n",
            "Qualcomm AI Research†\n",
            "ramjad@qti.qualcomm.comYelysei Bondarenko\n",
            "Qualcomm AI Research†\n",
            "ybodaren@qti.qualcomm.com\n",
            "Mart van Baalen\n",
            "Qualcomm AI Research†\n",
            "mart@qti.qualcomm.comTijmen Blankevoort\n",
            "Qualcomm AI Research†\n",
            "tijmen@qti.qualcomm.com\n",
            "Abstract\n",
            "While neural networks have advanced the frontiers in many applications, they\n",
            "often come at a high computational cost. Reducing the power and latency of neural\n",
            "network inference is key if we want to integrate modern networks into edge devices\n",
            "with strict power and compute requirements. Neural network quantization is one\n",
            "of the most effective ways of achieving these savings but the additional noise it\n",
            "induces can lead to accuracy degradation.\n",
            "In this white paper, we introduce state-of-the-art algorithms for mitigating the\n",
            "impact of quantization noise on the network’s performance while maintaining\n",
            "low-bit weights and activations. We start with a hardware motivated introduction\n",
            "to quantization and then consider two main classes of algorithms: Post-Training\n",
            "Quantization (PTQ) and Quantization-Aware-Training (QAT). PTQ requires no\n",
            "re-training or labelled data and is thus a lightweight push-button approach to\n",
            "quantization. In most cases, PTQ is sufﬁcient for achieving 8-bit quantization with\n",
            "close to ﬂoating-point accuracy. QAT requires ﬁne-tuning and access to labeled\n",
            "training data but enables lower bit quantization with competitive results. For both\n",
            "solutions, we provide tested pipelines based on existing literature and extensive\n",
            "experimentation that lead to state-of-the-art performance for common deep learning\n",
            "models and tasks.\n",
            "1 Introduction\n",
            "With the rise in popularity of deep learning as a general-purpose tool to inject intelligence into\n",
            "electronic devices, the necessity for small, low-latency and energy efﬁcient neural networks solutions\n",
            "has increased. Today neural networks can be found in many electronic devices and services, from\n",
            "smartphones, smart glasses and home appliances, to drones, robots and self-driving cars. These\n",
            "devices are typically subject to strict time restrictions on the execution of neural networks or stringent\n",
            "power requirements for long-duration performance.\n",
            "One of the most impactful ways to decrease the computational time and energy consumption of\n",
            "neural networks is quantization. In neural network quantization, the weights and activation tensors\n",
            "are stored in lower bit precision than the 16 or 32-bit precision they are usually trained in. When\n",
            "∗Equal contribution.†Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.arXiv:2106.08295v1  [cs.LG]  15 Jun 2021moving from 32 to 8 bits, the memory overhead of storing tensors decreases by a factor of 4 while the\n",
            "computational cost for matrix multiplication reduces quadratically by a factor of 16. Neural networks\n",
            "have been shown to be robust to quantization, meaning they can be quantized to lower bit-widths\n",
            "with a relatively small impact on the network’s accuracy. Besides, neural network quantization can\n",
            "often be applied along with other common methods for neural network optimization, such as neural\n",
            "architecture search, compression and pruning. It is an essential step in the model efﬁciency pipeline\n",
            "for any practical use-case of deep learning. However, neural network quantization is not free. Low\n",
            "bit-width quantization introduces noise to the network that can lead to a drop in accuracy. While\n",
            "some networks are robust to this noise, other networks require extra work to exploit the beneﬁts of\n",
            "quantization.\n",
            "In this white paper, we introduce the state-of-the-art in neural network quantization. We start with an\n",
            "introduction to quantization and discuss hardware and practical considerations. We then consider two\n",
            "different regimes of quantizing neural networks: Post-Training Quantization (PTQ) and Quantization-\n",
            "Aware Training (QAT). PTQ methods, discussed in section 3, take a trained network and quantize\n",
            "it with little or no data, requires minimal hyperparameter tuning and no end-to-end training. This\n",
            "makes them a push-button approach to quantizing neural networks with low engineering effort and\n",
            "computational cost. In contrast, QAT, discussed in section 4, relies on retraining the neural networks\n",
            "with simulated quantization in the training pipeline. While this requires more effort in training and\n",
            "potentially hyperparameter tuning, it generally further closes the gap to the full-precision accuracy\n",
            "compared to PTQ for low-bit quantization. For both regimes, we introduce standard pipelines based\n",
            "on existing literature and extensive experimentation that lead to state-of-the-art performance for\n",
            "common computer vision and natural language processing models. We also propose a debugging\n",
            "workﬂow to identify and address common issues when quantizing a new model.\n",
            "2 Quantization fundamentals\n",
            "In this section, we introduce the basic principles of neural network quantization and of ﬁxed-point\n",
            "accelerators on which quantized networks run on. We start with a hardware motivation and then\n",
            "introduce standard quantization schemes and their properties. Later we discuss practical considerations\n",
            "related to layers commonly found in modern neural networks and their implications for ﬁxed-point\n",
            "accelerators.\n",
            "2.1 Hardware background\n",
            "Before diving into the technical details, we ﬁrst explore the hardware background of quantization\n",
            "and how it enables efﬁcient inference on device. Figure 1 provides a schematic overview of how\n",
            "a matrix-vector multiplication, y=Wx+b, is calculated in a neural network (NN) accelerator.\n",
            "This is the building block of larger matrix-matrix multiplications and convolutions found in neural\n",
            "networks. Such hardware blocks aim at improving the efﬁciency of NN inference by performing as\n",
            "many calculations as possible in parallel. The two fundamental components of this NN accelerator\n",
            "are the processing elements Cn,mand the accumulators An. Our toy example in ﬁgure 1 has 16\n",
            "processing elements arranged in a square grid and 4accumulators. The calculation starts by loading\n",
            "the accumulators with the bias value bn. We then load the weight values Wn,mand the input values\n",
            "xminto the array and compute their product in the respective processing elements Cn,m=Wn,mxm\n",
            "in a single cycle. Their results are then added in the accumulators:\n",
            "An=bn+∑\n",
            "mCn,m (1)\n",
            "The above operation is also referred to as Multiply-Accumulate (MAC). This step is repeated many\n",
            "times for larger matrix-vector multiplications. Once all cycles are completed, the values in the\n",
            "accumulators are then moved back to memory to be used in the next neural network layer. Neural\n",
            "networks are commonly trained using FP32 weights and activations. If we were to perform inference\n",
            "in FP32, the processing elements and the accumulator would have to support ﬂoating-point logic,\n",
            "and we would need to transfer the 32-bit data from memory to the processing units. MAC operations\n",
            "and data transfer consume the bulk of the energy spent during neural network inference. Hence,\n",
            "signiﬁcant beneﬁts can be achieved by using a lower bit ﬁxed-point or quantized representation for\n",
            "these quantities. Low-bit ﬁxed-point representations, such as INT8, not only reduce the amount data\n",
            "transfer but also the size and energy consumption of the MAC operation (Horowitz, 2014). This is\n",
            "2because the cost of digital arithmetic typically scales linearly to quadratically with the number of bits\n",
            "used and because ﬁxed-point addition is more efﬁcient than its ﬂoating-point counterpart (Horowitz,\n",
            "2014).\n",
            "3Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample textW!,!W!,#W!,$W!,%x!x#x$x%#!,!#!,##!,$#!,%##,!##,###,$##,%#$,!#$,##$,$#$,%#%,!#%,##%,$#%,%$!$#$$$%Input valuesWeight valuesAccumulatorsW#,!W#,#W#,$W#,%W$,!W$,#W$,$W$,%W%,!W%,#W%,$W%,%\n",
            "Figure 1: A schematic overview of matrix-multiply logic in neural network accelerator hardware.\n",
            "To move from ﬂoating-point to the efﬁcient ﬁxed-point operations, we need a scheme for converting\n",
            "ﬂoating-point vectors to integers. A ﬂoating-point vector xcan be expressed approximately as a\n",
            "scalar multiplied by a vector of integer values:\n",
            "ˆx=sx·xint≈x (2)\n",
            "wheresxis a ﬂoating-point scale factor andxintis an integer vector, e.g., INT8. We denote this\n",
            "quantized version of the vector as ˆx. By quantizing the weights and activations we can write the\n",
            "quantized version of the accumulation equation:\n",
            "ˆAn=ˆbn+∑\n",
            "mˆWn,mˆxm\n",
            "=ˆbn+∑\n",
            "m(\n",
            "swWint\n",
            "n,m)(\n",
            "sxxint\n",
            "m)\n",
            "=ˆbn+swsx∑\n",
            "mWint\n",
            "n,mxint\n",
            "m (3)\n",
            "Note that we used a separate scale factor for weights, sw, and activations, sx. This provides ﬂexibility\n",
            "and reduces the quantization error (more in section 2.2). Since each scale factor is applied to the\n",
            "whole tensor, this scheme allows us to factor the scale factors out of the summation in equation (3)\n",
            "and perform MAC operations in ﬁxed-point format. We intentionally ignore bias quantization for\n",
            "now, because the bias is normally stored in higher bit-width (32-bits) and its scale factor depends on\n",
            "that of the weights and activations (Jacob et al., 2018).\n",
            "Figure 2 shows how the neural network accelerator changes when we introduce quantization. In\n",
            "our example, we use INT8 arithmetic, but this could be any quantization format for the sake of\n",
            "this discussion. It is important to maintain a higher bit-width for the accumulators, typical 32-bits\n",
            "wide. Otherwise, we risk incurring loss due to overﬂow as more products are accumulated during the\n",
            "computation.\n",
            "The activations stored in the 32-bit accumulators need to be written to memory before they can be\n",
            "used by the next layer. To reduce data transfer and the complexity of the next layer’s operations, these\n",
            "activations are quantized back to INT8. This requires a requantization step which is shown in ﬁgure\n",
            "2.\n",
            "2.2 Uniform afﬁne quantization\n",
            "In this section we deﬁne the quantization scheme that we will use in this paper. This scheme is\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"called uniform quantization and it is the most commonly used quantization scheme because it permits\n",
            "efﬁcient implementation of ﬁxed-point arithmetic.\n",
            "35Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample textx!x#x$x%#!,!#!,##!,$#!,%##,!##,###,$##,%#$,!#$,##$,$#$,%#%,!#%,##%,$#%,%$!$#$$$%Input valuesWeight values!!⋅!\"⋅\n",
            "INT8INT8\n",
            "INT32RequantizationMemoryINT8W!,!W!,#W!,$W!,%#!,!#!,##!,$#!,%##,!##,###,$##,%#$,!#$,##$,$#$,%#%,!#%,##%,$#%,%$!$#$$$%W#,!W#,#W#,$W#,%W$,!W$,#W$,$W$,%W%,!W%,#W%,$W%,%Figure 2: A schematic of matrix-multiply logic in an neural network accelerator for quantized\n",
            "inference.\n",
            "Uniform afﬁne quantization , also known as asymmetric quantization , is deﬁned by three quantization\n",
            "parameters: the scale factors, the zero-pointzand the bit-widthb. The scale factor and the zero-point\n",
            "are used to to map a ﬂoating point value to the integer grid, whose size depends on the bit-width. The\n",
            "scale factor is commonly represented as a ﬂoating-point number and speciﬁes the step-size of the\n",
            "quantizer. The zero-point is an integer that ensures that real zero is quantized without error. This is\n",
            "important to ensure that common operations like zero padding or ReLU do not induce quantization\n",
            "error.\n",
            "Once the three quantization parameters are deﬁned we can proceed with the quantization operation.\n",
            "Starting from a real-valued vector xwe ﬁrst map it to the unsigned integer grid{0,..., 2b−1}:\n",
            "xint= clamp(⌊x\n",
            "s⌉\n",
            "+z; 0,2b−1)\n",
            ", (4)\n",
            "where⌊·⌉is the round-to-nearest operator and clamping is deﬁned as:\n",
            "clamp (x;a,c) =\n",
            "\n",
            "a, x<a,\n",
            "x, a≤x≤c,\n",
            "c, x>c.(5)\n",
            "To approximate the real-valued input xwe perfrom a de-quantization step:\n",
            "x≈ˆx=s(xint−z) (6)\n",
            "Combining the two steps above we can provide a general deﬁnition for the quantization function ,\n",
            "q(·), as:\n",
            "ˆx=q(x;s,z,b ) =s[\n",
            "clamp(⌊x\n",
            "s⌉\n",
            "+z; 0,2b−1)\n",
            "−z]\n",
            ", (7)\n",
            "Through the de-quantization step, we can also deﬁne the quantization grid limits (qmin,qmax)where\n",
            "qmin=−szandqmax=s(2b−1−z). Any values of xthat lie outside this range will be clipped\n",
            "to its limits, incurring a clipping error . If we want to reduce the clipping error we can expand\n",
            "the quantization range by increasing the scale factor. However, increasing the scale factor leads to\n",
            "increased rounding error as the rounding error lies in the range[\n",
            "−1\n",
            "2s,1\n",
            "2s]\n",
            ". In section 3.1, we explore\n",
            "in more detail how to choose the quantization parameters to achieve the right trade-off between\n",
            "clipping and rounding errors.\n",
            "2.2.1 Symmetric uniform quantization\n",
            "Symmetric quantization is a simpliﬁed version of the general asymmetric case. The symmetric\n",
            "quantizer restricts the zero-point to 0. This reduces the computational overhead of dealing with\n",
            "zero-point offset during the accumulation operation in equation (3). But the lack of offset restricts the\n",
            "mapping between integer and ﬂoating-point domain. As a result, the choice of signed or unsigned\n",
            "4integer grid matters:\n",
            "ˆx=sxint (8a)\n",
            "xint= clamp(⌊x\n",
            "s⌉\n",
            "; 0,2b−1)\n",
            "for unsigned integers (8b)\n",
            "xint= clamp(⌊x\n",
            "s⌉\n",
            ";−2b−1,2b−1−1)\n",
            "for signed integers (8c)\n",
            "Unsigned symmetric quantization is well suited for one-tailed distributions, such as ReLU activations\n",
            "(see ﬁgure 3). On the other hand, signed symmetric quantization can be chosen for distributions that\n",
            "are roughly symmetric about zero.\n",
            "13Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.000<latexit sha1_base64=\"KWF1WsL3Cx0f3yYpGDxUvz20H+g=\">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4Kkkp6rHgxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzU1INS2a24C5B14uWkDDkag9JXfxizNEJpmKBa9zw3MX5GleFM4KzYTzUmlE3oCHuWShqh9rPFoTNyaZUhCWNlSxqyUH9PZDTSehoFtjOiZqxXvbn4n9dLTXjrZ1wmqUHJlovCVBATk/nXZMgVMiOmllCmuL2VsDFVlBmbTdGG4K2+vE7a1Yp3Xak2a+V6LY+jAOdwAVfgwQ3U4R4a0AIGCM/wCm/Oo/PivDsfy9YNJ585gz9wPn8A3LmM8A==</latexit>s\n",
            "<latexit sha1_base64=\"+Fy6cPh/qqHpQFs8d3cpWayzoQ8=\">AAAB+XicbVBNS8NAEN3Ur1q/oh69LBbBU0mKqMeCF48V7Qe0oWw2k3bp7ibsbgol9J948aCIV/+JN/+N2zYHbX0w8Hhvhpl5YcqZNp737ZQ2Nre2d8q7lb39g8Mj9/ikrZNMUWjRhCeqGxINnEloGWY4dFMFRIQcOuH4bu53JqA0S+STmaYQCDKULGaUGCsNXPdxKgQYxSjWbCghGrhVr+YtgNeJX5AqKtAcuF/9KKGZAGkoJ1r3fC81QU6UYZTDrNLPNKSEjskQepZKIkAH+eLyGb6wSoTjRNmSBi/U3xM5EVpPRWg7BTEjverNxf+8Xmbi2yBnMs0MSLpcFGccmwTPY8ARU0ANn1pCqGL2VkxHRBFqbFgVG4K/+vI6addr/nWt/lCvNq6KOMroDJ2jS+SjG9RA96iJWoiiCXpGr+jNyZ0X5935WLaWnGLmFP2B8/kDnHaTmQ==</latexit>Symmetric signed<latexit sha1_base64=\"LAN1Fz9JJpBsYhcuRNmqr7w7Hzc=\">AAACCHicbVDLSsNAFJ34rPVVdenCwSK4Kkkp2mXBjcsK9gFNCJPJpB06eTBzIy0hSzf+ihsXirj1E9z5N07bLLT1wMDhnHuZe46XCK7ANL+NtfWNza3t0k55d2//4LBydNxVcSop69BYxLLvEcUEj1gHOAjWTyQjoSdYzxvfzPzeA5OKx9E9TBPmhGQY8YBTAlpyK2fKpn4M2A4JjLwgm+RuZgObQMYjaOa5W6maNXMOvEqsglRRgbZb+bL9mKYhi4AKotTAMhNwMiKBU8Hysp0qlhA6JkM20DQiIVNONg+S4wut+DiIpX4R4Ln6eyMjoVLT0NOTs3vVsjcT//MGKQRNR0dKUmARXXwUpAJDjGetYJ9LRkFMNSFUcn0rpiMiCQXdXVmXYC1HXiXdes26qtXvGtVWo6ijhE7RObpEFrpGLXSL2qiDKHpEz+gVvRlPxovxbnwsRteMYucE/YHx+QMcwpqk</latexit>s·xint8<latexit sha1_base64=\"wQVDMLDEUIqvtUS9drDQVQOpZco=\">AAACCXicbVDLSsNAFJ3UV62vqks3g0VwVZJStMuCG5cV7AOaEiaTSTt0MgkzN9ISsnXjr7hxoYhb/8Cdf+P0sdDWAxcO59zLvff4ieAabPvbKmxsbm3vFHdLe/sHh0fl45OOjlNFWZvGIlY9n2gmuGRt4CBYL1GMRL5gXX98M/O7D0xpHst7mCZsEJGh5CGnBIzklbF2aRADdiMCIz/MJrmXucAmkKVcQiPPvXLFrtpz4HXiLEkFLdHyyl9uENM0YhKoIFr3HTuBQUYUcCpYXnJTzRJCx2TI+oZKEjE9yOaf5PjCKAEOY2VKAp6rvycyEmk9jXzTOTtYr3oz8T+vn0LYGGRcJikwSReLwlRgiPEsFhxwxSiIqSGEKm5uxXREFKFgwiuZEJzVl9dJp1Z1rqq1u3qlWV/GUURn6BxdIgddoya6RS3URhQ9omf0it6sJ+vFerc+Fq0Fazlziv7A+vwBAG+bIw==</latexit>s·xuint80255\n",
            "<latexit sha1_base64=\"KWF1WsL3Cx0f3yYpGDxUvz20H+g=\">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4Kkkp6rHgxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzU1INS2a24C5B14uWkDDkag9JXfxizNEJpmKBa9zw3MX5GleFM4KzYTzUmlE3oCHuWShqh9rPFoTNyaZUhCWNlSxqyUH9PZDTSehoFtjOiZqxXvbn4n9dLTXjrZ1wmqUHJlovCVBATk/nXZMgVMiOmllCmuL2VsDFVlBmbTdGG4K2+vE7a1Yp3Xak2a+V6LY+jAOdwAVfgwQ3U4R4a0AIGCM/wCm/Oo/PivDsfy9YNJ585gz9wPn8A3LmM8A==</latexit>s\n",
            "<latexit sha1_base64=\"BXQfQ41gDqSWSLOzY4///IszX2g=\">AAAB+3icbVBNS8NAEN34WetXrEcvi0XwVJIi6rHgxWNF+wFtKJvNtF26uwm7GzGE/hUvHhTx6h/x5r9x2+agrQ8GHu/NMDMvTDjTxvO+nbX1jc2t7dJOeXdv/+DQPaq0dZwqCi0a81h1Q6KBMwktwwyHbqKAiJBDJ5zczPzOIyjNYvlgsgQCQUaSDRklxkoDt3KfCQFGMYpTqdlIQjRwq17NmwOvEr8gVVSgOXC/+lFMUwHSUE607vleYoKcKMMoh2m5n2pICJ2QEfQslUSADvL57VN8ZpUID2NlSxo8V39P5ERonYnQdgpixnrZm4n/eb3UDK+DnMkkNSDpYtEw5djEeBYEjpgCanhmCaGK2VsxHRNFqLFxlW0I/vLLq6Rdr/mXtfpdvdq4KOIooRN0is6Rj65QA92iJmohip7QM3pFb87UeXHenY9F65pTzByjP3A+fwBMZZSQ</latexit>Symmetric unsigned\n",
            "<latexit sha1_base64=\"UwZKxCfWWgLJS364fOkrkkiOzms=\">AAAB8XicbVDLSgMxFM3UV62vqks3wSK4KjNF1GXFjcsK9oHtUDLpnTY0kxmSO0IZ+hduXCji1r9x59+YtrPQ1gOBwzn3knNPkEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVHJo8lrHuBMyAFAqaKFBCJ9HAokBCOxjfzvz2E2gjYvWAkwT8iA2VCAVnaKXHGzOJIkAteL9ccavuHHSVeDmpkByNfvmrN4h5GoFCLpkxXc9N0M+YRsElTEu91EDC+JgNoWupYhEYP5snntIzqwxoGGv7FNK5+nsjY5Gx0QI7GTEcmWVvJv7ndVMMr/1MqCRFUHzxUZhKijGdnU8HQgNHObGEcS1sVspHTDOOtqSSLcFbPnmVtGpV77Jau69V6hd5HUVyQk7JOfHIFamTO9IgTcKJIs/klbw5xnlx3p2PxWjByXeOyR84nz/KFpDz</latexit>Asymmetric0255\n",
            "<latexit sha1_base64=\"KWF1WsL3Cx0f3yYpGDxUvz20H+g=\">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4Kkkp6rHgxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzU1INS2a24C5B14uWkDDkag9JXfxizNEJpmKBa9zw3MX5GleFM4KzYTzUmlE3oCHuWShqh9rPFoTNyaZUhCWNlSxqyUH9PZDTSehoFtjOiZqxXvbn4n9dLTXjrZ1wmqUHJlovCVBATk/nXZMgVMiOmllCmuL2VsDFVlBmbTdGG4K2+vE7a1Yp3Xak2a+V6LY+jAOdwAVfgwQ3U4R4a0AIGCM/wCm/Oo/PivDsfy9YNJ585gz9wPn8A3LmM8A==</latexit>s\n",
            "<latexit sha1_base64=\"88ad+IysUXrTTs3EV6DCXUwRgCQ=\">AAAB9XicbVBNSwMxEM36WetX1aOXYBG8WHZLUS9CwYvHCvYD2rVk02wbmmSXZFatS/+HFw+KePW/ePPfmLZ70NYHA4/3ZpiZF8SCG3Ddb2dpeWV1bT23kd/c2t7ZLeztN0yUaMrqNBKRbgXEMMEVqwMHwVqxZkQGgjWD4dXEb94zbXikbmEUM1+SvuIhpwSsdNcB9gip5Gp8eWqeuoWiW3KnwIvEy0gRZah1C1+dXkQTyRRQQYxpe24Mfko0cCrYON9JDIsJHZI+a1uqiGTGT6dXj/GxVXo4jLQtBXiq/p5IiTRmJAPbKQkMzLw3Ef/z2gmEF37KVZwAU3S2KEwEhghPIsA9rhkFMbKEUM3trZgOiCYUbFB5G4I3//IiaZRL3lmpfFMpVitZHDl0iI7QCfLQOaqia1RDdUSRRs/oFb05D86L8+58zFqXnGzmAP2B8/kD2iGSsw==</latexit>min =\u0000sz<latexit sha1_base64=\"OXdvs7X7RdZOZARTQeZFogawHdU=\">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvgqewWUY8FLx4r2g9ol5JNs21okl2SrFiW/gQvHhTx6i/y5r8xbfegrQ8GHu/NMDMvTAQ31vO+UWFtfWNzq7hd2tnd2z8oHx61TJxqypo0FrHuhMQwwRVrWm4F6ySaERkK1g7HNzO//ci04bF6sJOEBZIMFY84JdZJ95I89csVr+rNgVeJn5MK5Gj0y1+9QUxTyZSlghjT9b3EBhnRllPBpqVealhC6JgMWddRRSQzQTY/dYrPnDLAUaxdKYvn6u+JjEhjJjJ0nZLYkVn2ZuJ/Xje10XWQcZWklim6WBSlAtsYz/7GA64ZtWLiCKGau1sxHRFNqHXplFwI/vLLq6RVq/qX1dpdrVK/yOMowgmcwjn4cAV1uIUGNIHCEJ7hFd6QQC/oHX0sWgsonzmGP0CfP2cIjdU=</latexit>max<latexit sha1_base64=\"OXdvs7X7RdZOZARTQeZFogawHdU=\">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvgqewWUY8FLx4r2g9ol5JNs21okl2SrFiW/gQvHhTx6i/y5r8xbfegrQ8GHu/NMDMvTAQ31vO+UWFtfWNzq7hd2tnd2z8oHx61TJxqypo0FrHuhMQwwRVrWm4F6ySaERkK1g7HNzO//ci04bF6sJOEBZIMFY84JdZJ95I89csVr+rNgVeJn5MK5Gj0y1+9QUxTyZSlghjT9b3EBhnRllPBpqVealhC6JgMWddRRSQzQTY/dYrPnDLAUaxdKYvn6u+JjEhjJjJ0nZLYkVn2ZuJ/Xje10XWQcZWklim6WBSlAtsYz/7GA64ZtWLiCKGau1sxHRFNqHXplFwI/vLLq6RVq/qX1dpdrVK/yOMowgmcwjn4cAV1uIUGNIHCEJ7hFd6QQC/oHX0sWgsonzmGP0CfP2cIjdU=</latexit>max\n",
            "<latexit sha1_base64=\"OXdvs7X7RdZOZARTQeZFogawHdU=\">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvgqewWUY8FLx4r2g9ol5JNs21okl2SrFiW/gQvHhTx6i/y5r8xbfegrQ8GHu/NMDMvTAQ31vO+UWFtfWNzq7hd2tnd2z8oHx61TJxqypo0FrHuhMQwwRVrWm4F6ySaERkK1g7HNzO//ci04bF6sJOEBZIMFY84JdZJ95I89csVr+rNgVeJn5MK5Gj0y1+9QUxTyZSlghjT9b3EBhnRllPBpqVealhC6JgMWddRRSQzQTY/dYrPnDLAUaxdKYvn6u+JjEhjJjJ0nZLYkVn2ZuJ/Xje10XWQcZWklim6WBSlAtsYz/7GA64ZtWLiCKGau1sxHRFNqHXplFwI/vLLq6RVq/qX1dpdrVK/yOMowgmcwjn4cAV1uIUGNIHCEJ7hFd6QQC/oHX0sWgsonzmGP0CfP2cIjdU=</latexit>maxDUPLICATE 2 (less high)\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"0127-128<latexit sha1_base64=\"usQ8n/DwTDCgaXu4jV5+gU9AQfU=\">AAACCXicbVC7SgNBFJ2Nrxhfq5Y2g0GIhWE3BE0ZsLGMYB6QhDA7mU2GzM4uM3clcdnWxl+xsVDE1j+w82+cPApNPHDhcM693HuPFwmuwXG+rcza+sbmVnY7t7O7t39gHx41dBgryuo0FKFqeUQzwSWrAwfBWpFiJPAEa3qj66nfvGdK81DewSRi3YAMJPc5JWCkno01LnQCAkPPT8ZpL+kAG0MScwmVNMUXD+c9O+8UnRnwKnEXJI8WqPXsr04/pHHAJFBBtG67TgTdhCjgVLA014k1iwgdkQFrGypJwHQ3mX2S4jOj9LEfKlMS8Ez9PZGQQOtJ4JnO6dF62ZuK/3ntGPxKN+EyioFJOl/kxwJDiKex4D5XjIKYGEKo4uZWTIdEEQomvJwJwV1+eZU0SkX3sli6Leer5UUcWXSCTlEBuegKVdENqqE6ougRPaNX9GY9WS/Wu/Uxb81Yi5lj9AfW5w+QIZo1</latexit>s(xuint8\u0000z)\n",
            "Figure 3: A visual explanation of the different uniform quantization grids for a bit-width of 8. sis the\n",
            "scaling factor, zthe zero-point. The ﬂoating-point grid is in black, the integer quantized grid in blue.\n",
            "2.2.2 Power-of-two quantizer\n",
            "Power-of-two quantization is a special case of symmetric quantization, in which the scale factor is\n",
            "restricted to a power-of-two, s= 2−k. This choice can bring hardware efﬁciencies because scaling\n",
            "withscorresponds to simple bit-shifting. However, the restricted expressiveness of the scale factor\n",
            "can complicate the trade-off between rounding and clipping error.\n",
            "2.2.3 Quantization granularity\n",
            "So far, we have deﬁned a single set of quantization parameters (quantizer) per tensor, one for the\n",
            "weights and one for activations, as seen in equation (3). This is called per-tensor quantization .\n",
            "We can also deﬁne a separate quantizer for individual segments of a tensor (e.g., output channels\n",
            "of a weight tensor), thus increasing the quantization granularity . In neural network quantization,\n",
            "per-tensor quantization is the the most common choice of granularity due to its simpler hardware\n",
            "implementation: all accumulators in equation (3)use the same scale factor, swsx. However, we\n",
            "could use ﬁner granularity to further improve performance. For example, for weight tensors, we can\n",
            "specify a different quantizer per output channel. This is known as per-channel quantization and its\n",
            "implications are discussed in more detailed in section 2.4.2.\n",
            "Other works go beyond per-channel quantization parameters and apply separate quantizers per group\n",
            "of weights or activations (Rouhani et al., 2020; Stock et al., 2019; Nascimento et al., 2019). Increasing\n",
            "the granularity of the groups generally improves accuracy at the cost of some extra overhead. The\n",
            "overhead is associated with accumulators handling sums of values with varying scale factors. Most\n",
            "existing ﬁxed-point accelerators do not currently support such logic and for this reason, we will not\n",
            "consider them in this work. However, as research in this area grows, more hardware support for these\n",
            "methods can be expected in the future.\n",
            "2.3 Quantization simulation\n",
            "To test how well a neural network would run on a quantized device, we often simulate the quantized\n",
            "behavior on the same general purpose hardware we use for training neural networks. This is called\n",
            "57Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample textQuantized inference on device\n",
            "ConvWeightsInputBiasesOutput\n",
            "int32\n",
            "int8Accumulatorint32RequantizationActivationint32int8\n",
            "int8ConvWeightsInputBiases+ActivationQuantizerQuantizerOutput(a) Diagram for quantized on-device inference with\n",
            "ﬁxed-point operations.\n",
            "5Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample textQuantized inference on device\n",
            "ConvWeightsInputBiasesOutput\n",
            "int32\n",
            "int8Accumulatorint32RequantizationActivationint32int8\n",
            "int8ConvWeightsInputBiases+ActivationQuantizerQuantizerOutput(b) Simulated quantization using ﬂoating-point\n",
            "operations.\n",
            "Figure 4: Schematic overview of quantized forward pass for convolutional layer: a) Compute graph\n",
            "of actual on-device quantized inference. b) Simulation of quantized inference for general-purpose\n",
            "ﬂoating-point hardware.\n",
            "quantization simulation . We aim to approximate ﬁxed-point operations using ﬂoating-point hard-\n",
            "ware. Such simulations are signiﬁcantly easier to implement compared to running experiments on\n",
            "actual quantized hardware or using quantized kernels. They allow the user to efﬁciently test various\n",
            "quantization options and it enables GPU acceleration for quantization-aware training as described\n",
            "in section 4. In this section, we ﬁrst explain the fundamentals of this simulation process and then\n",
            "discuss techniques that help to reduce the difference between the simulated and the actual on-device\n",
            "performance.\n",
            "Previously, we saw how matrix-vector multiplication is calculated in dedicated ﬁxed-point hardware.\n",
            "In ﬁgure 4a, we generalize this process for a convolutional layer, but we also include an activation\n",
            "function to make it more realistic. During on-device inference, all the inputs (biases, weight and input\n",
            "activations) to the hardware are in a ﬁxed-point format. However, when we simulate quantization\n",
            "using common deep learning frameworks and general-purpose hardware these quantities are in\n",
            "ﬂoating-point. This is why we introduce quantizer blocks in the compute graph to induce quantization\n",
            "effects.\n",
            "Figure 4b shows how the same convolutional layer is modelled in a deep-learning framework. Quan-\n",
            "tizer blocks are added in between the weights and the convolution to simulate weight quantization,\n",
            "and after the activation function to simulate activation quantization. The bias is often not quantized\n",
            "because it is stored in higher-precision. In section 2.3.2, we discuss in more detail when it is appropri-\n",
            "ate to position the quantizer after the non-linearity. The quantizer block implements the quantization\n",
            "function of equation (7)and each quantizer is deﬁned by a set of quantization parameters (scale factor,\n",
            "zero-point, bit-width). Both the input and output of the quantizer are in ﬂoating-point format but the\n",
            "output lies on the quantization grid.\n",
            "2.3.1 Batch normalization folding\n",
            "Batch normalization (Ioffe & Szegedy, 2015) is a standard component of modern convolutional\n",
            "networks. Batch normalization normalizes the output of a linear layer before scaling and adding an\n",
            "offset (see equation 9). For on-device inference, these operations are folded into the previous or next\n",
            "linear layers in a step called batch normalization folding (Krishnamoorthi, 2018; Jacob et al., 2018).\n",
            "This removes the batch normalization operations entirely from the network, as the calculations are\n",
            "absorbed into an adjacent linear layer. Besides reducing the computational overhead of the additional\n",
            "scaling and offset, this prevents extra data movement and the quantization of the layer’s output. More\n",
            "6formally, during inference, batch normalization is deﬁned as an afﬁne map of the output x:\n",
            "BatchNorm( x) =γ(x−µ√\n",
            "σ2+ϵ)\n",
            "+β (9)\n",
            "whereµandσare the mean and variance computed during training as exponential moving average\n",
            "over batch-statistics, and γandβare learned afﬁne hyper-parameters per-channel. If batch normal-\n",
            "ization is applied right after a linear layer y= BatchNorm( Wx), we can rewrite the terms such\n",
            "that the batch normalization operation is fused with the linear layer itself. Assuming a weight matrix\n",
            "W∈Rn×mwe apply batch normalization to each output ykfork={1,...,n}:\n",
            "yk= BatchNorm( Wk,:x)\n",
            "=γk(\n",
            "Wk,:x−µk√\n",
            "σ2\n",
            "k+ϵ)\n",
            "+βk\n",
            "=γkWk,:√\n",
            "σ2\n",
            "k+ϵx+(\n",
            "βk−γkµk√\n",
            "σ2\n",
            "k+ϵ)\n",
            "=˜Wk,:x+˜bk (10)\n",
            "where:\n",
            "˜Wk,:=γkWk,:√\n",
            "σ2\n",
            "k+ϵ, (11)\n",
            "˜bk=βk−γkµk√\n",
            "σ2\n",
            "k+ϵ. (12)\n",
            "2.3.2 Activation function fusing\n",
            "In our naive quantized accelerator introduced in section 2.1, we saw that the requantization of\n",
            "activations happens after the matrix multiplication or convolutional output values are calculated.\n",
            "However, in practice, we often have a non-linearity directly following the linear operation. It would\n",
            "be wasteful to write the linear layer’s activations to memory, and then load them back into a compute\n",
            "core to apply a non-linearity. For this reason, many hardware solutions come with a hardware unit that\n",
            "applies the non-linearity before the requantization step. If this is the case, we only have to simulate\n",
            "requantization that happens after the non-linearity. For example, ReLU non-linearities are readily\n",
            "modelled by the requantization block, as you can just set the minimum representable value of that\n",
            "activation quantization to 0.\n",
            "Other more complex activation functions, such as sigmoid or Swish (Ramachandran et al., 2017),\n",
            "require more dedicated support. If this support is not available, we need to add a quantization step\n",
            "before and after the non-linearity in the graph. This can have a big impact on the accuracy of\n",
            "quantized model. Although newer activations like Swish functions provide accuracy improvement in\n",
            "ﬂoating-point, these may vanish after quantization or may be less efﬁcient to deploy on ﬁxed-point\n",
            "hardware.\n",
            "2.3.3 Other layers and quantization\n",
            "There are many other types of layers being used in neural networks. How these are modeled depends\n",
            "greatly on the speciﬁc hardware implementation. Sometimes the mismatch between simulated\n",
            "quantization and on-target performance is down to layers not being properly quantized. Here, we\n",
            "provide some guidance on how to simulate quantization for a few commonly used layers:\n",
            "Max pooling Activation quantization is not required because the input and output values are on the\n",
            "same quantization grid.\n",
            "Average pooling The average of integers is not necessarily an integer. For this reason, a quantization\n",
            "step is required after average-pooling. However, we use the same quantizer for the inputs\n",
            "and outputs as the quantization range does not signiﬁcantly change.\n",
            "Element-wise addition Despite its simple nature, this operation is difﬁcult to simulate accurately.\n",
            "During addition, the quantization ranges of both inputs have to match exactly. If these ranges\n",
            "7do not match, extra care is needed to make addition work as intended. There is no single\n",
            "accepted solution for this but adding a requantization step can simulate the added noise\n",
            "coarsely. Another approach is to optimize the network by tying the quantization grids of the\n",
            "inputs. This would prevent the requantization step but may require ﬁne-tuning.\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"Concatenation The two branches that are being concatenated generally do not share the same\n",
            "quantization parameters. This means that their quantization grids may not overlap making a\n",
            "requantization step necessary. As with element-wise addition, it is possible to optimize your\n",
            "network to have shared quantization parameters for the branches being concatenated.\n",
            "2.4 Practical considerations\n",
            "When quantizing neural networks with multiple layers, we are confronted with a large space of\n",
            "quantization choices including the quantization scheme, granularity, and bit-width. In this section, we\n",
            "explore some of the practical considerations that help reduce the search space.\n",
            "Note that in this white paper we only consider homogeneous bit-width. This means that the bit-width\n",
            "chosen for either weights or activations remains constant across all layers. Homogeneous bit-width is\n",
            "more universally supported by hardware but some recent works also explore the implementation of\n",
            "heterogeneous bit-width or mixed-precision (van Baalen et al., 2020; Dong et al., 2019; Uhlich et al.,\n",
            "2020).\n",
            "2.4.1 Symmetric vs. asymmetric quantization\n",
            "For each weight and activation quantization, we have to choose a quantization scheme. On one hand,\n",
            "asymmetric quantization is more expressive because there is an extra offset parameter, but on the\n",
            "other hand there is a possible computational overhead. To see why this is the case, consider what\n",
            "happens when asymmetric weights, ˆW=sw(Wint−zw), are multiplied with asymmetric activations\n",
            "ˆx=sx(xint−zx):\n",
            "ˆWˆx=sw(Wint−zw)sx(xint−zx)\n",
            "=swsxWintxint−swzwsxxint−swsxzxWint+swzwsxzx. (13)\n",
            "The ﬁrst term is what we would have if both operations were in symmetric format. The third and\n",
            "fourth terms depend only on the scale, offset and weight values, which are known in advance. Thus\n",
            "these two terms can be pre-computed and added to the bias term of a layer at virtually no cost. The\n",
            "second term, however, depends on the input data x. This means that for each batch of data we need to\n",
            "compute an additional term during inference. This can lead to signiﬁcant overhead in both latency\n",
            "and power, as it is equivalent to adding an extra channel.\n",
            "For this reason, it is a common approach to use asymmetric activation quantization andsymmetric\n",
            "weight quantization that avoids the additional data-dependent term.\n",
            "2.4.2 Per-tensor and per-channel quantization\n",
            "In section 2.2.3, we discussed different levels of quantization granularity. Per-tensor quantization\n",
            "of weights and activations has been standard for a while because it is supported by all ﬁxed-point\n",
            "accelerators. However, per-channel quantization of the weights can improve accuracy, especially\n",
            "when the distribution of weights varies signiﬁcantly from channel to channel. Looking back at the\n",
            "quantized MAC operation in equation (3), we can see that per-channel weight quantization can\n",
            "be implemented in the accelerator by applying a separate per-channel weight scale factor without\n",
            "requiring rescaling. Per-channel quantization of activations is much harder to implement because\n",
            "we cannot factor the scale factor out of the summation and would, therefore, require rescaling the\n",
            "accumulator for each input channel. Whereas per-channel quantization of weights is increasingly\n",
            "becoming common practice, not all commercial hardware supports it. Therefore, it is important to\n",
            "check if it is possible in your intended target device.\n",
            "3 Post-training quantization\n",
            "Post-training quantization (PTQ) algorithms take a pre-trained FP32 network and convert it directly\n",
            "into a ﬁxed-point network without the need for the original training pipeline. These methods can\n",
            "8Model (FP32 accuracy) ResNet18 (69.68) MobileNetV2 (71.72)\n",
            "Bit-width W8 W6 W4 W8 W6 W4\n",
            "Min-Max 69.57 63.90 0.12 71.16 64.48 0.59\n",
            "MSE 69.45 64.64 18.82 71.15 65.43 13.77\n",
            "Min-Max (Per-channel) 69.60 69.08 44.49 71.21 68.52 18.40\n",
            "MSE (Per-channel) 69.66 69.24 54.67 71.46 68.89 27.17\n",
            "Table 1: Ablation study for different methods of range setting of (symmetric uniform) weight\n",
            "quantizers while keeping the activations in FP32. Average ImageNet validation accuracy (%) over 5\n",
            "runs.\n",
            "be data-free or may require a small calibration set, which is often readily available. Additionally,\n",
            "having almost no hyperparameter tuning makes them usable via a single API call as a black-box\n",
            "method to quantize a pretrained neural network in a computationally efﬁcient manner. This frees the\n",
            "neural network designer from having to be an expert in quantization and thus allows for a much wider\n",
            "application of neural network quantization.\n",
            "A fundamental step in the PTQ process is ﬁnding good quantization ranges for each quantizer. We\n",
            "brieﬂy discussed in section 2.2 how the choice of quantization range affects the quantization error. In\n",
            "this section, we start by discussing various common methods used in practice to ﬁnd good quantization\n",
            "parameters. We then explore common issues observed during PTQ and introduce the most successful\n",
            "techniques to overcome them. Using these techniques we present a standard post-training quantization\n",
            "pipeline, which we ﬁnd to work best in most common scenarios and, ﬁnally, we introduce a set of\n",
            "debugging steps to improve the performance of the quantized model.\n",
            "3.1 Quantization range setting\n",
            "Quantization range setting refers to the method of determining clipping thresholds of the quantization\n",
            "grid,qminandqmax(see equation 7). The key trade-off in range setting is between clipping and\n",
            "rounding error, described in section 2.2, and their impact on the ﬁnal task loss for each quantizer\n",
            "being conﬁgured. Each of the methods described here provides a different trade-off between the two\n",
            "quantities. These methods typically optimize local cost functions instead of the task loss. This is\n",
            "because in PTQ we aim for computationally fast methods without the need for end-to-end training.\n",
            "Weights can usually be quantized without any need for calibration data. However, determining\n",
            "parameters for activation quantization often requires a few batches of calibration data.\n",
            "Min-max To cover the whole dynamic range of the tensor, we can deﬁne the quantization parame-\n",
            "ters as follows\n",
            "qmin= min V, (14)\n",
            "qmax= max V, (15)\n",
            "where Vdenotes the tensor to be quantized. This leads to no clipping error. However, this approach\n",
            "is sensitive to outliers as strong outliers may cause excessive rounding errors.\n",
            "Mean squared error (MSE) One way to alleviate the issue of large outliers is to use MSE-based\n",
            "range setting. In this range setting method we ﬁnd qminandqmaxthat minimize the MSE between the\n",
            "original and the quantized tensor:\n",
            "arg min\n",
            "qmin,qmaxV−ˆV(qmin,qmax)2\n",
            "F, (16)\n",
            "where ˆV(qmin,qmax)denotes the quantized version of Vand∥·∥Fis the Frobenius norm. The\n",
            "optimization problem is commonly solved using grid search, golden section method or analytical\n",
            "approximations with closed-form solution (Banner et al., 2019). Several variants of this range setting\n",
            "method exist in literature but they are all very similar in terms of objective function and optimization.\n",
            "9Model (FP32 accuracy) ResNet18 (69.68) MobileNetV2 (71.72)\n",
            "Bit-width A8 A6 A4 A8 A6 A4\n",
            "Min-Max 69.60 68.19 18.82 70.96 64.58 0.53\n",
            "MSE 69.59 67.84 31.40 71.35 67.55 13.57\n",
            "MSE + Xent 69.60 68.91 59.07 71.36 68.85 30.94\n",
            "BN (α= 6) 69.54 68.73 23.83 71.32 65.20 0.66\n",
            "Table 2: Ablation study for different methods of range setting of (asymmetric uniform) activation\n",
            "quantizers while keeping the weights in FP32. Average ImageNet validation accuracy (%) over 5\n",
            "runs.\n",
            "Cross entropy For certain layers, all values in the tensor being quantized may not be equally\n",
            "important. One such scenario is the quantization of logits in the last layer of classiﬁcation networks,\n",
            "in which it is important to preserve the order of the largest value after quantization. MSE may not\n",
            "be a suitable metric for this, as it weighs all the values in a tensor equally regardless of their order.\n",
            "For a larger number of classes, we usually have a large number of small or negative logits that are\n",
            "unimportant for prediction accuracy and few larger values that matter. In this case, MSE would incur a\n",
            "large quantization error to the few larger important logits while trying to reduce the quantization error\n",
            "of the more populous smaller logits. In this speciﬁc case, it is beneﬁcial to minimize the following\n",
            "cross-entropy loss function\n",
            "arg min\n",
            "qmin,qmaxH(ψ(v),ψ(ˆv(qmin,qmax))) (17)\n",
            "whereH(·,·)denotes the cross-entropy function, ψis the softmax function, and vis the logits vector.\n",
            "BN based range setting Range setting for activation quantizers often requires some calibration\n",
            "data. If a layer has batch-normalized activations, the per-channel mean and standard deviation of\n",
            "the activations are equal to the learned batch normalization shift and scale parameters, respectively.\n",
            "These can then be used to ﬁnd suitable parameters for activation quantizer as follows (Nagel et al.,\n",
            "2019):\n",
            "qmin= min ( β−αγ) (18)\n",
            "qmax= max ( β+αγ) (19)\n",
            "where βandγare vectors of per-channel learned shift and scale parameters, and α>0. Nagel et al.\n",
            "(2019) uses α= 6so that only large outliers are clipped.\n",
            "Comparison In table 1 we compare range setting methods for weight quantization. For high bit-\n",
            "widths, the MSE and min-max approaches are mostly on par. However, at lower bit-widths the MSE\n",
            "approach clearly outperforms the min-max. In table 2, we present a similar comparison for activation\n",
            "quantization. We note that MSE combined with cross-entropy for the last layer, denoted as MSE +\n",
            "Xent, outperforms other methods, especially at lower bit-widths. The table also clearly demonstrates\n",
            "the beneﬁt of using cross-entropy for the last layer instead of the MSE objective.\n",
            "3.2 Cross-Layer Equalization\n",
            "A common issue for quantization error is that elements in the same tensor can have signiﬁcantly\n",
            "different magnitudes. As discussed in the previous section, range setting for the quantization grid\n",
            "tries to ﬁnd a good trade-off between clipping and rounding error. Unfortunately, in some cases, the\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"difference in magnitude between them is so large that even for moderate quantization (e.g., INT8),\n",
            "we cannot ﬁnd a suitable trade-off. Nagel et al. (2019) showed that this is especially prevalent in\n",
            "depth-wise separable layers since only a few weights are responsible for each output feature and\n",
            "this might result in higher variability of the weights. Further, they noted that batch normalization\n",
            "folding adds to this effect and can result in a strong imbalance between weights connected to various\n",
            "output channels (see ﬁgure 5). While the latter is less of an issue for a more ﬁne-grained quantization\n",
            "granularity (e.g., per-channel quantization), this remains a big issue for the more widely used per-\n",
            "tensor quantization. Several papers (Krishnamoorthi, 2018; Nagel et al., 2019; Sheng et al., 2018b)\n",
            "noted that efﬁcient models with depth-wise separable convolutions, such as MobileNetV1 (Howard\n",
            "101234567891011121314151617181920212223242526272829303132\n",
            "Output channel index50\n",
            "25\n",
            "0255075100RangeWeight range per output channelFigure 5: Per (output) channel weight ranges of the ﬁrst depthwise-separable layer in MobileNetV2\n",
            "after BN folding. The boxplots show the min and max value, the 2nd and 3rd quartile and the median\n",
            "are plotted for each channel.\n",
            "12Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample text=<latexit sha1_base64=\"meYEYs0luY9YGmCX9DxDNv5g0Cs=\">AAAB+XicbVDLSsNAFL3xWesr6tLNYBHqpiRF1GXRjcsK9gFtLJPppB06mYSZSaGE/IkbF4q49U/c+TdO2iy09cDA4Zx7uWeOH3OmtON8W2vrG5tb26Wd8u7e/sGhfXTcVlEiCW2RiEey62NFORO0pZnmtBtLikOf044/ucv9zpRKxSLxqGcx9UI8EixgBGsjDWy7H2I99oO0kz2lVfciG9gVp+bMgVaJW5AKFGgO7K/+MCJJSIUmHCvVc51YeymWmhFOs3I/UTTGZIJHtGeowCFVXjpPnqFzowxREEnzhEZz9fdGikOlZqFvJvOcatnLxf+8XqKDGy9lIk40FWRxKEg40hHKa0BDJinRfGYIJpKZrIiMscREm7LKpgR3+curpF2vuVe1+sNlpXFb1FGCUziDKrhwDQ24hya0gMAUnuEV3qzUerHerY/F6JpV7JzAH1ifPwwyk0c=</latexit>W(1)<latexit sha1_base64=\"nKzQnz9bX9AwUJlw9oNMON3dZiY=\">AAAB+XicbVDLSsNAFL3xWesr6tLNYBHqpiRF1GXRjcsK9gFtLJPppB06mYSZSaGE/IkbF4q49U/c+TdO2iy09cDA4Zx7uWeOH3OmtON8W2vrG5tb26Wd8u7e/sGhfXTcVlEiCW2RiEey62NFORO0pZnmtBtLikOf044/ucv9zpRKxSLxqGcx9UI8EixgBGsjDWy7H2I99oO0kz2l1fpFNrArTs2ZA60StyAVKNAc2F/9YUSSkApNOFaq5zqx9lIsNSOcZuV+omiMyQSPaM9QgUOqvHSePEPnRhmiIJLmCY3m6u+NFIdKzULfTOY51bKXi/95vUQHN17KRJxoKsjiUJBwpCOU14CGTFKi+cwQTCQzWREZY4mJNmWVTQnu8pdXSbtec69q9YfLSuO2qKMEp3AGVXDhGhpwD01oAYEpPMMrvFmp9WK9Wx+L0TWr2DmBP7A+fwANuJNI</latexit>W(2)\n",
            "<latexit sha1_base64=\"21yzaa2R3mRHu9WKmzG0lV/IwqY=\">AAAB8nicbVBNS8NAEN3Ur1q/qh69LBbBU02KqMeiF48V7AekoWw2m3bpZjfsToQS+jO8eFDEq7/Gm//GbZuDtj4YeLw3w8y8MBXcgOt+O6W19Y3NrfJ2ZWd3b/+genjUMSrTlLWpEkr3QmKY4JK1gYNgvVQzkoSCdcPx3czvPjFtuJKPMElZkJCh5DGnBKzk92mkAHsXZsAH1Zpbd+fAq8QrSA0VaA2qX/1I0SxhEqggxviem0KQEw2cCjat9DPDUkLHZMh8SyVJmAny+clTfGaVCMdK25KA5+rviZwkxkyS0HYmBEZm2ZuJ/3l+BvFNkHOZZsAkXSyKM4FB4dn/OOKaURATSwjV3N6K6YhoQsGmVLEheMsvr5JOo+5d1RsPl7XmbRFHGZ2gU3SOPHSNmugetVAbUaTQM3pFbw44L86787FoLTnFzDH6A+fzB2f8kLA=</latexit>·1/si<latexit sha1_base64=\"iC1CO8RBStlRYCXn0LpsTNBqVaI=\">AAAB8XicbVBNS8NAEN3Ur1q/qh69LBbBU0mKqMeiF48V7Ae2oWw2m3bpZjfsToQS+i+8eFDEq//Gm//GTZuDtj4YeLw3w8y8IBHcgOt+O6W19Y3NrfJ2ZWd3b/+genjUMSrVlLWpEkr3AmKY4JK1gYNgvUQzEgeCdYPJbe53n5g2XMkHmCbMj8lI8ohTAlZ6HNBQATZDXhlWa27dnQOvEq8gNVSgNax+DUJF05hJoIIY0/fcBPyMaOBUsFllkBqWEDohI9a3VJKYGT+bXzzDZ1YJcaS0LQl4rv6eyEhszDQObGdMYGyWvVz8z+unEF37GZdJCkzSxaIoFRgUzt/HIdeMgphaQqjm9lZMx0QTCjakPARv+eVV0mnUvct64/6i1rwp4iijE3SKzpGHrlAT3aEWaiOKJHpGr+jNMc6L8+58LFpLTjFzjP7A+fwBvmOQUA==</latexit>·si\n",
            "<latexit sha1_base64=\"ATgUpwfGWoZqdP72T2bjWzFePXU=\">AAAB8XicbVDLSgMxFL3js9ZX1aWbYBFclZki6rLoxmUF+8C2lEx6pw3NZIYkI5ahf+HGhSJu/Rt3/o2ZdhbaeiBwOOdecu7xY8G1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZQohg0WiUi1fapRcIkNw43AdqyQhr7Alj++yfzWIyrNI3lvJjH2QjqUPOCMGis9dENqRn6QPk37pbJbcWcgy8TLSRly1Pulr+4gYkmI0jBBte54bmx6KVWGM4HTYjfRGFM2pkPsWCppiLqXzhJPyalVBiSIlH3SkJn6eyOlodaT0LeTWUK96GXif14nMcFVL+UyTgxKNv8oSAQxEcnOJwOukBkxsYQyxW1WwkZUUWZsSUVbgrd48jJpViveRaV6d16uXed1FOAYTuAMPLiEGtxCHRrAQMIzvMKbo50X5935mI+uOPnOEfyB8/kD/4ORIw==</latexit>x<latexit sha1_base64=\"4BSd+cda8Ysxy44cgNaLWSX+h7I=\">AAAB+XicbVDLSsNAFL3xWesr6tLNYBHqpiRF1GXRjcsK9gFtLJPppB06mYSZSSGE/okbF4q49U/c+TdO2iy09cDA4Zx7uWeOH3OmtON8W2vrG5tb26Wd8u7e/sGhfXTcVlEiCW2RiEey62NFORO0pZnmtBtLikOf044/ucv9zpRKxSLxqNOYeiEeCRYwgrWRBrbdD7Ee+0GWzp6yqnsxG9gVp+bMgVaJW5AKFGgO7K/+MCJJSIUmHCvVc51YexmWmhFOZ+V+omiMyQSPaM9QgUOqvGyefIbOjTJEQSTNExrN1d8bGQ6VSkPfTOY51bKXi/95vUQHN17GRJxoKsjiUJBwpCOU14CGTFKieWoIJpKZrIiMscREm7LKpgR3+curpF2vuVe1+sNlpXFb1FGCUziDKrhwDQ24hya0gMAUnuEV3qzMerHerY/F6JpV7JzAH1ifP0Cok2k=</latexit>y(1)<latexit sha1_base64=\"7PJEp3dNA0GEj7zWjIJF3sjCXEg=\">AAAB+XicbVDLSsNAFL3xWesr6tLNYBHqpiRF1GXRjcsK9gFtLJPppB06mYSZSSGE/okbF4q49U/c+TdO2iy09cDA4Zx7uWeOH3OmtON8W2vrG5tb26Wd8u7e/sGhfXTcVlEiCW2RiEey62NFORO0pZnmtBtLikOf044/ucv9zpRKxSLxqNOYeiEeCRYwgrWRBrbdD7Ee+0GWzp6yav1iNrArTs2ZA60StyAVKNAc2F/9YUSSkApNOFaq5zqx9jIsNSOczsr9RNEYkwke0Z6hAodUedk8+QydG2WIgkiaJzSaq783MhwqlYa+mcxzqmUvF//zeokObryMiTjRVJDFoSDhSEcorwENmaRE89QQTCQzWREZY4mJNmWVTQnu8pdXSbtec69q9YfLSuO2qKMEp3AGVXDhGhpwD01oAYEpPMMrvFmZ9WK9Wx+L0TWr2DmBP7A+fwBCLpNq</latexit>y(2)\n",
            "<latexit sha1_base64=\"tfEVQCQsfmYqAUfLb1yILPHpKNc=\">AAAB73icbVBNSwMxEJ2tX7V+VT16CRahXspuEfVY9OKxgv2AdinZbLYNzSZrkhXK0j/hxYMiXv073vw3pu0etPXBwOO9GWbmBQln2rjut1NYW9/Y3Cpul3Z29/YPyodHbS1TRWiLSC5VN8CaciZoyzDDaTdRFMcBp51gfDvzO09UaSbFg5kk1I/xULCIEWys1I2qfRJKcz4oV9yaOwdaJV5OKpCjOSh/9UNJ0pgKQzjWuue5ifEzrAwjnE5L/VTTBJMxHtKepQLHVPvZ/N4pOrNKiCKpbAmD5urviQzHWk/iwHbG2Iz0sjcT//N6qYmu/YyJJDVUkMWiKOXISDR7HoVMUWL4xBJMFLO3IjLCChNjIyrZELzll1dJu17zLmv1+4tK4yaPowgncApV8OAKGnAHTWgBAQ7P8ApvzqPz4rw7H4vWgpPPHMMfOJ8/ZViPjg==</latexit>f(·)<latexit sha1_base64=\"tfEVQCQsfmYqAUfLb1yILPHpKNc=\">AAAB73icbVBNSwMxEJ2tX7V+VT16CRahXspuEfVY9OKxgv2AdinZbLYNzSZrkhXK0j/hxYMiXv073vw3pu0etPXBwOO9GWbmBQln2rjut1NYW9/Y3Cpul3Z29/YPyodHbS1TRWiLSC5VN8CaciZoyzDDaTdRFMcBp51gfDvzO09UaSbFg5kk1I/xULCIEWys1I2qfRJKcz4oV9yaOwdaJV5OKpCjOSh/9UNJ0pgKQzjWuue5ifEzrAwjnE5L/VTTBJMxHtKepQLHVPvZ/N4pOrNKiCKpbAmD5urviQzHWk/iwHbG2Iz0sjcT//N6qYmu/YyJJDVUkMWiKOXISDR7HoVMUWL4xBJMFLO3IjLCChNjIyrZELzll1dJu17zLmv1+4tK4yaPowgncApV8OAKGnAHTWgBAQ7P8ApvzqPz4rw7H4vWgpPPHMMfOJ8/ZViPjg==</latexit>f(·)\n",
            "Figure 6: Illustration of the rescaling for a single channel. Scaling a channel in the ﬁrst layer by a\n",
            "factorsileads a reparametrization of the equivalent channel in the second layer by 1/si.\n",
            "et al., 2017) and MobileNetV2 (Sandler et al., 2018), show a signiﬁcant drop for PTQ or even result\n",
            "in random performance.\n",
            "A solution to overcome such imbalances without the need to use per-channel quantization is introduced\n",
            "by Nagel et al. (2019). A similar approach was introduced in concurrent work by Meller et al. (2019).\n",
            "In both papers, the authors observe that for many common activation functions (e.g., ReLU, PreLU),\n",
            "a positive scaling equivariance holds:\n",
            "f(sx) =sf(x) (20)\n",
            "for any non-negative real number s. This equivariance holds for any homogeneous function of degree\n",
            "one and can be extended to also hold for any piece-wise linear function by scaling its parameterization\n",
            "(e.g. ReLU6). We can exploit this positive scaling equivariance in consecutive layers in neural\n",
            "networks. Given two layers, h=f(W(1)x+b(1))andy=f(W(2)h+b(2)), through scaling\n",
            "equivariance we have that:\n",
            "y=f(W(2)f(W(1)x+b(1)) +b(2))\n",
            "=f(W(2)Sˆf(S−1W(1)x+S−1b(1)) +b(2))\n",
            "=f(˜W(2)ˆf(˜W(1)x+˜b(1)) +b(2)) (21)\n",
            "where S= diag( s)is a diagonal matrix with value Siidenoting the scaling factor sifor neuron\n",
            "i. This allows us to reparameterize our model with ˜W(2)=W(2)S,˜W(1)=S−1W(1)and\n",
            "˜b(1)=S−1b(1). In case of CNNs the scaling will be per-channel and broadcast accordingly over the\n",
            "spatial dimensions. We illustrate this rescaling procedure in ﬁgure 6.\n",
            "To make the model more robust to quantization, we can ﬁnd a scaling factor sisuch that the\n",
            "quantization noise in the rescaled layers is minimal. The cross-layer equalization (CLE) procedure\n",
            "11Model FP32 INT8\n",
            "Original model 71.72 0.12\n",
            "+ CLE 71.70 69.91\n",
            "+ absorbing bias 71.57 70.92\n",
            "Per-channel quantization 71.72 70.65\n",
            "Table 3: Impact of cross-layer equalization (CLE) for MobileNetV2 . ImageNet validation accuracy\n",
            "(%), evaluated at full precision and 8-bit quantization.\n",
            "(Nagel et al., 2019) achieves this by equalizing dynamic ranges across consecutive layers. They prove\n",
            "that an optimal weight equalization is achieved by setting Ssuch that:\n",
            "si=1\n",
            "r(2)\n",
            "i√\n",
            "r(1)\n",
            "ir(2)\n",
            "i (22)\n",
            "where r(j)\n",
            "iis the dynamic range of channel iof weight tensor j. The algorithm of Meller et al. (2019)\n",
            "introduces a similar scaling factor that also takes the intermediate activation tensor into account.\n",
            "However, they do not have a proof of optimality for this approach.\n",
            "Absorbing high biases Nagel et al. (2019) further notice that in some cases, especially after CLE,\n",
            "high biases can lead to differences in the dynamic ranges of the activations. Therefore, they propose a\n",
            "procedure to, if possible, absorb high biases into the next layer. To absorb cfrom layer one (followed\n",
            "by a ReLU activation function f) into layer two, we can do the following reparameterization:\n",
            "y=W(2)h+b(2)\n",
            "=W(2)(f(W(1)x+b(1)) +c−c) +b(2)\n",
            "=W(2)(f(W(1)x+˜b(1)) +c) +b(2)\n",
            "=W(2)˜h+˜b(2)(23)\n",
            "where˜b(2)=W(2)c+b(2),˜h=h−c, and˜b(1)=b(1)−c. In step two, we use the fact that\n",
            "for a layer with ReLU function f, there is a non-negative vector csuch thatr(Wx+b−c) =\n",
            "r(Wx+b)−c. The trivial solution c=0holds for all x. However, depending on the distribution\n",
            "ofxand the values of Wandb, there can be some values ci>0for which this equality holds for\n",
            "(almost) all xin the empirical distribution. This value is equal to\n",
            "ci= max(\n",
            "0,min\n",
            "x(\n",
            "W(1)\n",
            "ix+b(1)\n",
            "i))\n",
            ". (24)\n",
            "where minxis evaluated on a small calibration dataset. To remove dependence on data, the authors\n",
            "propose to estimate the right hand side of (24) by the shift and scale parameters of the batch\n",
            "normalization layer which results3inci= max(0,βi−3γi).\n",
            "Experiments In table 3, we demonstrate the effect of CLE and bias absorption for quantiz-\n",
            "ingMobileNetV2 to 8-bit. As skip connections break the equivariance between layers, we apply\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"cross-layer equalization only to the layers within each residual block. Similar to Krishnamoorthi\n",
            "(2018), we observe that model performance is close to random when quantizing MobileNetV2 to\n",
            "INT8. Applying CLE brings us back within 2% of FP32 performance, close to the performance of\n",
            "per-channel quantization. We note that absorbing high biases results in a small drop in FP32 perfor-\n",
            "mance, as it is an approximation, but it boosts quantized performance by 1% due to more precise\n",
            "activation quantization. Together, CLE and bias absorption followed by per-tensor quantization yield\n",
            "better results than per-channel quantization.\n",
            "3.3 Bias correction\n",
            "Another common issue is that quantization error is often biased. This means that the expected output\n",
            "of the original and quantized layer or network is shifted ( E[Wx]̸=E[\n",
            "ˆWx]\n",
            "). This kind of error is\n",
            "3Assuming xis normally distributed, the equality will hold for approximately 99.865% of the inputs.\n",
            "12more pronounced in depth-wise separable layers with only a few elements per output channel (usually\n",
            "9 for a 3×3kernel). The main contributor to this error is often the clipping error, as a few strongly\n",
            "clipped outliers will likely lead to a shift in the expected distribution.\n",
            "Several papers (Nagel et al., 2019; Meller et al., 2019; Finkelstein et al., 2019) noted this issue and\n",
            "introduce methods to correct for the expected shift in distribution. For a quantized layer ˆWwith\n",
            "quantization error ∆W=ˆW−W, the expected output distribution is\n",
            "E[ˆy] =E[\n",
            "ˆWx]\n",
            "=E[(W+ ∆W)x]\n",
            "=E[Wx] +E[∆Wx]. (25)\n",
            "Thus the biased error is given by E[∆Wx]. Since ∆Wis constant, we have that E[∆Wx] =\n",
            "∆WE[x]. In case ∆WE[x]is nonzero, the output distribution is shifted. To counteract this shift\n",
            "we can substract it from the output:\n",
            "E[ycorr] =E[\n",
            "ˆWx]\n",
            "−∆WE[x] =E[y]. (26)\n",
            "Note, this correction term is a vector with the same shape as the bias and can thus be absorbed into\n",
            "the bias without any additional overhead at inference time. There are several ways of calculating the\n",
            "bias correction term, the two most common of which are empirical bias correction andanalytic bias\n",
            "correction .\n",
            "Empirical bias correction If we have access to a calibration dataset the bias correction term can\n",
            "simply be calculated by comparing the activations of the quantized and full precision model. In\n",
            "practice, this can be done layer-wise by computing\n",
            "∆WE[x] =E[\n",
            "ˆWx]\n",
            "−E[Wx]. (27)\n",
            "Analytic bias correction Nagel et al. (2019) introduce a method to analytically calculate the biased\n",
            "error, without the need for data. For common networks with batch normalization and ReLU functions,\n",
            "they use the BN statistics of the preceding layer in order to compute the expected input distribution\n",
            "E[x]. The BN parameters γandβcorrespond to the mean and standard deviation of the BN layers\n",
            "output. Assuming input values are normally distributied, the effect of ReLU on the distribution can\n",
            "be modeled using the clipped normal distribution. They show that\n",
            "E[x] =E[ReLU (xpre)]\n",
            "=γN(−β\n",
            "γ)\n",
            "+β[\n",
            "1−Φ(−β\n",
            "γ)]\n",
            "(28)\n",
            "where xpreis the pre-activation output, which is assumed to be normally distributed with the per-\n",
            "channel means βand per-channel standard deviations γ,Φ(·)is the standard normal CDF, and the\n",
            "notationN(x)is used to denote the standard normal PDF. Note, all vector operations are element-\n",
            "wise (per-channel) operations. After calculating the input distribution E[x], the correction term can\n",
            "be simply derived by multiplying it with the weight quantization error ∆W.\n",
            "Experiments In table 4, we demonstrate the effect of bias correction for quantizing MobileNetV2\n",
            "to 8-bit. Applying analytical bias correction improves quantized model performance from random to\n",
            "over 50%, indicating that the biased error introduced by quantization signiﬁcantly harms model per-\n",
            "formance. When combining bias correction with CLE, we see that both techniques are complementary.\n",
            "Together, they achieve near FP32 performance without using any data.\n",
            "3.4 AdaRound\n",
            "Neural network weights are usually quantized by projecting each FP32 value to the nearest quantiza-\n",
            "tion grid point, as indicated by ⌊·⌉in equation (4)for a uniform quantization grid. We refer to this\n",
            "quantization strategy as rounding-to-nearest. The rounding-to-nearest strategy is motivated by the fact\n",
            "that, for a ﬁxed quantization grid, it yields the lowest MSE between the ﬂoating-point and quantized\n",
            "weights. However, Nagel et al. (2020) showed that rounding-to-nearest is not optimal in terms of the\n",
            "13Model FP32 INT8\n",
            "Original Model 71.72 0.12\n",
            "+ bias correction 71.72 52.02\n",
            "CLE + bias absorption 71.57 70.92\n",
            "+ bias correction 71.57 71.19\n",
            "Table 4: Impact of bias correction for MobileNetV2 . ImageNet validation accuracy (%) evaluated at\n",
            "full precision and 8-bit quantization.\n",
            "0.5 1.0 1.5 2.0 2.5 3.0\n",
            "1\n",
            "2wTH(w)w\n",
            "35404550556065Accuracy (%)\n",
            "Stochastic rounding\n",
            "Nearest rounding\n",
            "Figure 7: Correlation between the cost in equation (30) vs. ImageNet validation accuracy (%) of 100\n",
            "stochastic rounding vectors ˆwfor4-bit quantization of only the ﬁrst layer of ResNet18.\n",
            "task loss when quantizing weights in the post-training regime. To illustrate this the authors quantized\n",
            "the weights of the ﬁrst layer of ResNet18 to 4 bits using 100 different stochastic rounding samples\n",
            "(Gupta et al., 2015) and evaluated the performance of the network for each rounding choice. The\n",
            "best rounding choice among these outperformed rounding-to-nearest by more than 10%. Figure 7\n",
            "illustrates this by plotting the performance of these rounding choices on the y-axis. In this section,\n",
            "we describe AdaRound (Nagel et al., 2020), a systematic approach to ﬁnding good weight rounding\n",
            "choices for PTQ. AdaRound is a theoretically well-founded and computationally efﬁcient method\n",
            "that shows signiﬁcant performance improvement in practice.\n",
            "As the main goal is to minimize the impact of quantization on the ﬁnal task loss, we start by\n",
            "formulating the optimization problem in terms of this loss\n",
            "arg min\n",
            "∆wE[L(x,y,w+ ∆w)−L(x,y,w)] (29)\n",
            "where ∆wdenotes the perturbation due to quantization and can take two possible values for each\n",
            "weight, one by rounding the weight up and the other by rounding the weight down. We want to\n",
            "solve this binary optimization problem efﬁciently. As a ﬁrst step, we approximate the cost function\n",
            "using a second-order Taylor series expansion. This alleviates the need for performance evaluation for\n",
            "each new rounding choice during the optimization. We further assume that the model has converged,\n",
            "implying that the contribution of the gradient term in the approximation can be ignored, and that\n",
            "the Hessian is block-diagonal, which ignores cross-layer correlations. This leads to the following\n",
            "Hessian based quadratic unconstrained binary optimization (QUBO) problem\n",
            "arg min\n",
            "∆w(ℓ)E[\n",
            "∆w(ℓ)TH(w(ℓ))∆w(ℓ)]\n",
            "(30)\n",
            "The clear correlation in ﬁgure 7 between the validation accuracy and objective of equation (30)\n",
            "indicates that the latter serves as a good proxy for the task loss (equation 29), even for 4-bit weight\n",
            "quantization. Despite the performance gains (see table 5), equation (30) cannot be widely applied for\n",
            "weight rounding for main two reasons:\n",
            "•The memory and computational complexity of calculating the Hessian is impractical for\n",
            "general use-cases.\n",
            "14Rounding First layer All layers\n",
            "Nearest 52.29 23.99\n",
            "H(w)task loss (equation 30) 68.62 N/A\n",
            "Cont. relaxation MSE (equation 32) 69.58 66.56\n",
            "AdaRound (equation 35) 69.58 68.60\n",
            "Table 5: Impact of various approximations and assumptions made in section 3.4 on the ImageNet\n",
            "validation accuracy (%) for ResNet18 averaged over 5 runs. N/A implies that the corresponding\n",
            "experiment was computationally infeasible.\n",
            "• The QUBO problem of equation (30) is NP-Hard.\n",
            "To tackle the ﬁrst problem, the authors introduced additional suitable assumptions that allow simpli-\n",
            "fying the objective of equation (30) to the following local optimization problem that minimizes the\n",
            "MSE of the output activations for a layer.\n",
            "arg min\n",
            "∆W(ℓ)\n",
            "k,:E[(\n",
            "∆W(ℓ)\n",
            "k,:x(ℓ−1))2]\n",
            "(31)\n",
            "Equation (31) requires neither the computation of the Hessian nor any other backward or forward\n",
            "propagation information from the subsequent layers. Note that the approximations and the analysis\n",
            "that have been used to link the QUBO problem of equation (30) with the local optimization problem\n",
            "of equation (31) is independent of the rounding problem. Hence this analysis also beneﬁts the design\n",
            "of algorithms for other problems, including model compression and NAS (Moons et al., 2020).\n",
            "The optimization of (31) is still an NP-hard optimization problem. To ﬁnd a good approximate\n",
            "solution with reasonable computational complexity, the authors relax the optimization problem to the\n",
            "following continuous optimization problem\n",
            "arg min\n",
            "VWx−˜Wx2\n",
            "F+λfreg(V), (32)\n",
            "where∥·∥2\n",
            "Fdenotes the Frobenius norm and ˜Ware the soft-quantized weights deﬁned as\n",
            "˜W=s·clamp(⌊W\n",
            "s⌋\n",
            "+h(V) ;n,p)\n",
            ". (33)\n",
            "We usenandpto denote integer grid limits, n=qmin/sandp=qmax/s.Vi,jis the continuous\n",
            "variable that we optimize over and hcan be any monotonic function with values between 0and1,\n",
            "i.e.,h(Vi,j)∈[0,1]. In Nagel et al. (2020), the authors use a rectiﬁed sigmoid as h. The objective of\n",
            "(32) also introduces a regularizer term that encourages the continuous optimization variables h(Vi,j)\n",
            "to converge to either 0or1, so that they are valid solutions to the discrete optimization in (31). The\n",
            "regularizer used in Nagel et al. (2020) is\n",
            "freg(V) =∑\n",
            "i,j1−|2h(Vi,j)−1|β, (34)\n",
            "whereβis annealed during the course of optimization to initially allow free movement of h(Vi,j)\n",
            "and later to force them to converge to 0or1. To avoid error accumulation across layers of the neural\n",
            "network and to account for the non-linearity, the authors propose the following ﬁnal optimization\n",
            "problem\n",
            "arg min\n",
            "Vfa(Wx)−fa(\n",
            "˜Wˆ x)2\n",
            "F+λfreg(V), (35)\n",
            "where ˆ xis the layer’s input with all preceding layers quantized and fais the activation function. The\n",
            "objective of (35) can be effectively and efﬁciently optimized using stochastic gradient descent. This\n",
            "approach of optimizing weight rounding is known as AdaRound.\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"To summarize, the way we round weights during the quantization operation has a signiﬁcant impact\n",
            "on the performance of the network. AdaRound provides a theoretically sound, computationally fast\n",
            "weight rounding method. It requires only a small amount of unlabeled data samples, no hyperparame-\n",
            "ter tuning or end-to-end ﬁnetuning, and can be applied to fully connected and convolutional layers of\n",
            "any neural network.\n",
            "1513Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample textPTQ Pipeline\n",
            "Pre-trained FP Model\n",
            "Add Quantizers\n",
            "CLE\n",
            "Weight Range SettingYesNo\n",
            "Use Data\n",
            "Symmetric Weights\n",
            "Asymmetric Activations\n",
            "MSE Range Setting\n",
            "MSE Range Setting\n",
            "Bias Correction\n",
            "AdaRound\n",
            "Activation Range SettingFigure 8: Standard PTQ pipeline. Blue boxes represent required steps and the turquoise boxes\n",
            "recommended choices.\n",
            "3.5 Standard PTQ pipeline\n",
            "In this section, we present a best-practice pipeline for PTQ based on relevant literature and extensive\n",
            "experimentation. We illustrate the recommended pipeline in ﬁgure 8. This pipeline achieves competi-\n",
            "tive PTQ results for many computer vision as well as natural language processing models and tasks.\n",
            "Depending on the model, some steps might not be required, or other choices could lead to equal or\n",
            "better performance.\n",
            "Cross-layer equalization First we apply cross-layer equalization (CLE), which is a pre-processing\n",
            "step for the full precision model to make it more quantization friendly. CLE is particularly\n",
            "important for models with depth-wise separable layers and for per-tensor quantization, but it\n",
            "often also shows improvements for other layers and quantization choices.\n",
            "Add quantizers Next we choose our quantizers and add quantization operations in our network as\n",
            "described in section 2.3. The choice of quantizer might depend on the speciﬁc target HW;\n",
            "for common AI accelerators we recommend using symmetric quantizers for the weights\n",
            "and asymmetric quantizers for the activations. If supported by the HW/SW stack then it is\n",
            "favorable to use per-channel quantization for weights.\n",
            "Weight range setting To set the quantization parameters of all weight tensors we recommend using\n",
            "the layer-wise MSE based criteria. In the speciﬁc case of per-channel quantization, using\n",
            "the min-max method can be favorable in some cases.\n",
            "AdaRound In case we have a small calibration dataset4available we next apply AdaRound in\n",
            "order to optimize the rounding of the weights. This step is crucial to enable low-bit weight\n",
            "quantization (e.g. 4 bits) in the PTQ.\n",
            "Bias correction In case we do not have such a calibration dataset and the network uses batch\n",
            "normalization, we can use analytical bias correction instead.\n",
            "Activation range setting As the ﬁnal step, we determine the quantization ranges of all data-\n",
            "dependent tensors in the network (i.e., activations). We use the MSE based criteria for\n",
            "most of the layers, which requires a small calibration set to ﬁnd the minimum MSE loss.\n",
            "Alternatively, we can use the BN based range setting to have a fully data-free pipeline.\n",
            "3.6 Experiments\n",
            "We now evaluate the performance of the aforementioned PTQ pipeline on common computer vision\n",
            "and natural language understanding applications. Our results are summarized in table 6. For the\n",
            "task of semantic segmentation, we evaluate DeepLabV3 (with a MobileNetV2 backbone) (Chen\n",
            "et al., 2017) on Pascal VOC and for object detection, EfﬁcientDet (Tan et al., 2020) on COCO 2017.\n",
            "The rest of the computer vision models are evaluated on the ImageNet classiﬁcation benchmark.\n",
            "For natural language understanding, we evaluate BERT-base on the GLUE benchmark (Wang et al.,\n",
            "2018).\n",
            "In all cases, we observe that 8-bit quantization of weights and activation (W8A8) leads to only\n",
            "marginal loss of accuracy compared to ﬂoating-point (within 0.7%) for all models. For W8A8\n",
            "4Usually, between 500 and 1000 unlabeled images are sufﬁcient as a calibration set.\n",
            "16Per-tensor Per-channel\n",
            "Models FP32 W8A8 W4A8 W8A8 W4A8\n",
            "ResNet18 69.68 69.60 68.62 69.56 68.91\n",
            "ResNet50 76.07 75.87 75.15 75.88 75.43\n",
            "MobileNetV2 71.72 70.99 69.21 71.16 69.79\n",
            "InceptionV3 77.40 77.68 76.48 77.71 76.82\n",
            "EfﬁcientNet lite 75.42 75.25 71.24 75.39 74.01\n",
            "DeeplabV3 72.94 72.44 70.80 72.27 71.67\n",
            "EfﬁcientDet-D1 40.08 38.29 0.31 38.67 35.08\n",
            "BERT-base†83.06 82.43 81.76 82.77 82.02\n",
            "Table 6: Performance (average over 5 runs) of our standard PTQ pipeline for various models and tasks.\n",
            "DeeplabV3 (MobileNetV2 backbone) is evaluated on Pascal VOC (mean intersection over union),\n",
            "EfﬁcientDet-D1 on COCO 2017 (mean average precision), BERT-base on the GLUE benchmark\n",
            "and other models on ImageNet (accuracy). We evaluate all models on the respective validation sets.\n",
            "Higher is better in all cases.†A few quantized activations are kept in higher precision (16 bits).\n",
            "quantization we also see no signiﬁcant gains from using per-channel quantization. However, the\n",
            "picture changes when weights are quantized to 4 bits (W4A8). For ResNet18/50 and InceptionV3 the\n",
            "accuracy drop is still within 1% of ﬂoating-point for both per-tensor and per-channel quantization.\n",
            "However, for more efﬁcient networks, such as MobileNetV2 and EfﬁcientNet lite, the drop increases\n",
            "to 2.5% and 4.2% respectively for per-tensor quantization. This is likely due to the quantization of\n",
            "the depth-wise separable convolutions. Here, per-channel quantization can show a signiﬁcant beneﬁt,\n",
            "for example, in EfﬁcientNet lite per-channel quantization increases the accuracy by 2.8% compared\n",
            "to per-tensor quantization, bringing it within 1.4% of full-precision accuracy. We see similar effects\n",
            "for EfﬁcientDet-D1 and DeeplabV3 which both uses depth-wise separable convolutions in their\n",
            "backbone.\n",
            "For BERT-base, we observe that a few activation tensors have extreme differences in their dynamic\n",
            "ranges. To make PTQ still work, we identiﬁed these layers using our debugging procedure outlined\n",
            "in section 3.7 and kept them in 16 bit. Otherwise BERT-base follows similar trends as most other\n",
            "models and our PTQ pipeline allows 4 bit weight quantization within 1.5% drop in GLUE score.\n",
            "3.7 Debugging\n",
            "We showed that the standard PTQ pipeline can achieve competitive results for a wide range of models\n",
            "and networks. However, if after following the steps of our pipeline, the model’s performance is still\n",
            "not satisfactory, we recommend a set of diagnostics steps to identify the bottlenecks and improve\n",
            "the performance. While this is not strictly an algorithm, these debugging steps can provide insights\n",
            "on why a quantized model underperforms and help to tackle the underlying issues. These steps are\n",
            "shown as a ﬂow chart in ﬁgure 9 and are described in more detail below:\n",
            "FP32 sanity check An important initial debugging step is to ensure that the ﬂoating-point and quan-\n",
            "tized model behave similarly in the forward pass, especially when using custom quantization\n",
            "pipelines. Set the quantized model bit-width to 32 bits for both weights and activation, or\n",
            "by-pass the quantization operation, if possible, and check that the accuracy matches that of\n",
            "the FP32 model.\n",
            "Weights or activations quantization The next debugging step is to identify how activation or\n",
            "weight quantization impact the performance independently. Does performance recover\n",
            "if all weights are quantized to a higher bit-width while activations are kept in a lower bit-\n",
            "width, or conversely if all activations use a high bit-width and activations a low bit-width?\n",
            "This step can show the relative contribution of activations and weight quantization to the\n",
            "overall performance drop and point us towards the appropriate solution.\n",
            "Fixing weight quantization If the previous step shows that weight quantization does cause signiﬁ-\n",
            "cant accuracy drop, then there are a few solutions to try:\n",
            "•Apply CLE if not already implemented, especially for models with depth-wise separable\n",
            "convolutions.\n",
            "179Quantized ModelSet !!=!\"=32Fix ModelYESNOerror ≈0for &∈{Activations,Weights}Quantize all tensors in &error <+YESFix problemRange SettingAdjust CLEAdaRoundPer channelCLEBias correctionActivationsWeightserror <+YESNOfor layer l=0,…,0Set !!=!\"=32error <+Visualize tensor 1($)Fix problem in 1($)error <+NO\n",
            "NOYES\n",
            "YESQATNOQuantize tensor 1($)\n",
            "Final Quantized ModelENDEND\n",
            "Quantize Model\n",
            "error <+YESNOFigure 9: PTQ debugging ﬂow chart. Error is the difference between ﬂoating-point and quantized\n",
            "model accuracy.\n",
            "18•Try per-channel quantization. This will address the issue of uneven per-channel weight\n",
            "distribution.\n",
            "• Apply bias correction or AdaRound if calibration data is available.\n",
            "Fixing activation quantization To reduce the quantization error from activation quantization, we\n",
            "can also try using different range setting methods or adjust CLE to take activation quantiza-\n",
            "tion ranges into account, as vanilla CLE can lead to uneven activation distribution.\n",
            "Per-layer analysis If the global solutions have not restored accuracy to acceptable levels, we consider\n",
            "each quantizer individually. We set each quantizer sequentially, to the target bit-width while\n",
            "keeping the rest of the network to 32 bits (see inner for loop in ﬁgure 9).\n",
            "Visualizing layers If the quantization of a individual tensor leads to signiﬁcant accuracy drop, we\n",
            "recommended visualizing the tensor distribution at different granularities, e.g. per-channel\n",
            "as in ﬁgure 5, and dimensions, e.g., per-token or per-embedding for activations in BERT.\n",
            "Fixing individual quantizers The visualization step can reveal the source of the tensor’s sensitivity\n",
            "to quantization. Some common solutions involve custom range setting for this quantizer or\n",
            "allowing a higher bit-width for problematic quantizer, e.g., BERT-base from table 6. If the\n",
            "problem is ﬁxed and the accuracy recovers, we continue to the next quantizer. If not, we\n",
            "may have to resort to other methods, such as quantization-aware training (QAT), which is\n",
            "discussed in section 4.\n",
            "After completing the above steps, the last step is to quantize the complete model to the desired\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"bit-width. If the accuracy is acceptable, we have our ﬁnal quantized model ready to use. Otherwise,\n",
            "we can consider higher bit-widths and smaller granularities or revert to more powerful quantization\n",
            "methods, such as quantization-aware training.\n",
            "4 Quantization-aware training\n",
            "The post-training quantization techniques described in the previous section are the ﬁrst go-to tool in\n",
            "our quantization toolkit. They are very effective and fast to implement because they do not require\n",
            "retraining of the network with labeled data. However, they have limitations, especially when aiming\n",
            "for low-bit quantization of activations, such as 4-bit and below. Post-training techniques may not\n",
            "be enough to mitigate the large quantization error incurred by low-bit quantization. In these cases,\n",
            "we resort to quantization-aware training (QAT). QAT models the quantization noise source (see\n",
            "section 2.3) during training. This allows the model to ﬁnd more optimal solutions than post-training\n",
            "quantization. However, the higher accuracy comes with the usual costs of neural network training,\n",
            "i.e., longer training times, need for labeled data and hyper-parameter search.\n",
            "In this section, we explore how back-propagation works in networks with simulated quantization\n",
            "and provide a standard pipeline for training models with QAT effectively. We will also discuss the\n",
            "implications of batch normalization folding and per-channel quantization in QAT and provide results\n",
            "for a wide range of tasks and models.\n",
            "4.1 Simulating quantization for backward path\n",
            "In section 2.3, we saw how quantization can be simulated using ﬂoating-point in deep learning\n",
            "frameworks. However, if we look at the computational graph of ﬁgure 4, to train such a network\n",
            "we need to back-propagate through the simulated quantizer block. This poses an issue because the\n",
            "gradient of the round-to-nearest operation in equation (4)is either zero or undeﬁned everywhere,\n",
            "which makes gradient-based training impossible. A way around this would be to approximate the\n",
            "gradient using the straight-through estimator (STE, Bengio et al. 2013), which approximates the\n",
            "gradient of the rounding operator as 1:\n",
            "∂⌊y⌉\n",
            "∂y= 1 (36)\n",
            "Using this approximation we can now calculate the gradient of the quantization operation from\n",
            "equation (7). For clarity we assume symmetric quantization, namely z= 0, but the same result\n",
            "applies to asymmetric quantization since the zero-point is a constant. We use nandpto deﬁne the\n",
            "integer grid limits, such that n=qmin/sandp=qmax/s. The gradient of equation (7) w.r.t its input,\n",
            "19xi, is given by:\n",
            "∂ˆxi\n",
            "∂xi=∂q(xi)\n",
            "∂xi\n",
            "=s·∂\n",
            "∂xiclamp(⌊xi\n",
            "s⌉\n",
            ";n,p)\n",
            "+ 0\n",
            "=\n",
            "\n",
            "s·∂⌊xi/s⌉\n",
            "∂(xi/s)∂(xi/s)\n",
            "∂xiifqmin≤xi≤qmax,\n",
            "s·∂n\n",
            "∂xiifxi<q min,\n",
            "s·∂p\n",
            "∂xiifxi>q max,\n",
            "={1ifqmin≤xi≤qmax,\n",
            "0otherwise.(37)\n",
            "Using this gradient deﬁnition we can now back-propagate through the quantization blocks. Figure 10\n",
            "shows a simple computational graph for the forward and backward pass used in quantization-aware\n",
            "training. The forward pass is identical to that of ﬁgure 4, but in the backward pass we effectively\n",
            "skip the quantizer block due to the STE assumption. In earlier QAT work the quantization ranges\n",
            "8Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample textQAT Graph\n",
            "ConvWeightsInputBiases+ActivationQuantizerQuantizer\n",
            "#$$%&%BackwardForward\n",
            "Figure 10: Forward and backward computation graph for quantization aware training with STE\n",
            "assumption.\n",
            "for weights and activations were updated at each iteration most commonly using the min-max range\n",
            "(Krishnamoorthi, 2018). In later work (Esser et al., 2020; Jain et al., 2019; Bhalgat et al., 2020), the\n",
            "STE is used to calculate the gradient w.r.t. the quantization parameters, zands. Using the chain rule\n",
            "and the STE, we ﬁrst calculate the gradient w.r.t. the scale-factor:\n",
            "∂ˆxi\n",
            "∂s=∂\n",
            "∂s[\n",
            "s·clamp(⌊xi\n",
            "s⌉\n",
            ";n,p)]\n",
            "=\n",
            "\n",
            "−xi/s+⌊xi/s⌉ifqmin≤xi≤qmax,\n",
            "n ifxi<q min,\n",
            "p ifxi>q max.(38)\n",
            "Originally, we restricted the zero-point to be an integer. To make zero-point learnable we convert into\n",
            "a real number and apply the rounding operator. The modiﬁed quantization function is deﬁned as:\n",
            "ˆx=q(x;s,z) =s·[\n",
            "clamp(⌊x\n",
            "s⌉\n",
            "+⌊z⌉;n,p)\n",
            "−⌊z⌉]\n",
            "(39)\n",
            "The gradient w.r.t. tozis calculated by applying the STE once again to the rounding operator:\n",
            "∂ˆxi\n",
            "∂z={0qmin≤xi≤qmax,\n",
            "−sotherwise.(40)\n",
            "20Model (FP32 accuracy) ResNet18 (69.68) MobileNetV2 (71.72)\n",
            "Bit-width W4A8 W4A4 W4A8 W4A4\n",
            "Static folding BN 69.76 68.32 70.17 66.43\n",
            "Double forward (Krishnamoorthi, 2018) 69.42 68.20 66.87 63.54\n",
            "Static folding (per-channel) 69.58 68.15 70.52 66.32\n",
            "Keep original BN (per-channel) 70.01 68.83 70.48 66.89\n",
            "Table 7: Ablation study with various ways to include BN into QAT. The learning rate is individually\n",
            "optimized for each conﬁguration. Average ImageNet validation accuracy (%) over 3 runs.\n",
            "4.2 Batch normalization folding and QAT\n",
            "In section 2.3.1, we introduced batch normalization folding that absorbs the scaling and addition\n",
            "into a linear layer to allow for more efﬁcient inference. During quantization-aware training, we want\n",
            "to simulate inference behavior closely, which is why we have to account for BN-folding during\n",
            "training. Note that in some QAT literature, the BN-folding effect is ignored. While this is ﬁne when\n",
            "we employ per-channel quantization (more below in this section), keeping BN unfolded for per-tensor\n",
            "quantization will result in one of the two following cases:\n",
            "1.The BN layer applies per-channel rescaling during inference. In this case we might as well\n",
            "use per-channel quantization in the ﬁrst place.\n",
            "2.We fold BN during deployment into the weight tensor and incur potentially signiﬁcant\n",
            "accuracy drop as we trained the network to adapt to a different quantization noise.\n",
            "A simple but effective approach to modeling BN-folding in QAT is to statically fold the BN scale and\n",
            "offset into the linear layer’s weights and bias, as we saw in equations (11) and(12). This corresponds\n",
            "to re-parametrization of the weights and effectively removes the batch normalization operation\n",
            "from the network entirely. When starting from a converged pre-trained model, static folding is very\n",
            "effective, as we can see from the result of table 7.\n",
            "An alternative approach by Jacob et al. (2018) both updates the running statistics during QAT and\n",
            "applies BN-folding using a correction. This approach is more cumbersome and computationally\n",
            "costly because it involves a double forward pass : one for the batch-statistics and one for the quantized\n",
            "linear operation. However, based on our experiments (see table 7), static-folding performs on par or\n",
            "better despite its simplicity.\n",
            "Per-channel quantization In section 2.4.2, we mentioned that per-channel quantization of the\n",
            "weights can improve accuracy when it is supported by hardware. The static folding re-parametrization\n",
            "is also valid for per-channel quantization. However, per-channel quantization provides additional\n",
            "ﬂexibility as it allows us to absorb the batch normalization scaling operation into the per-channel\n",
            "scale-factor. Let us see how this is possible by revisiting the BN folding equation from section 2.3.1,\n",
            "but this time introduce per-channel quantization of the weights, such that ˆWk,:=q(Wk,:;sw,k) =\n",
            "sw,kWint\n",
            "k,:. By applying batch normalization to the output of a linear layer similar to equation (10),\n",
            "we get:\n",
            "ˆyk= BatchNorm(\n",
            "ˆWk,:x)\n",
            "=γkˆWk,:√\n",
            "σ2\n",
            "k+ϵx+(\n",
            "βk−γkµk√\n",
            "σ2\n",
            "k+ϵ)\n",
            "=γksw,k√\n",
            "σ2\n",
            "k+ϵWint\n",
            "k,:x+˜bk\n",
            "=˜sw,k(\n",
            "Wint\n",
            "k,:x)\n",
            "+˜bk (41)\n",
            "We can see that it is now possible to absorb the batch normalization scaling parameters into the\n",
            "per-channel scale-factor. For QAT, this means that we can keep the BN layer intact during training\n",
            "and merge the BN scaling factor into the per-channel quantization parameters afterward. In practice,\n",
            "21Model (FP32 accuracy) ResNet18 (69.68) MobileNetV2 (71.72)\n",
            "PTQ QAT PTQ QAT\n",
            "W4A8 w/ min-max weight init 0.12 69.61 0.56 69.96\n",
            "W4A8 w/ MSE weight init 18.58 69.76 12.99 70.13\n",
            "W4A4 w/ min-max act init 7.51 68.23 0.22 66.55\n",
            "W4A4 w/ MSE act init 9.62 68.41 0.71 66.29\n",
            "Table 8: Ablation study for various ways to initialize the quantization grid. The learning rate is\n",
            "individually optimized for each conﬁguration. ImageNet validation accuracy (%) averaged over 3\n",
            "runs.\n",
            "this modeling approach is on par or better for per-channel quantization compared to static folding as\n",
            "we can see from the last two rows of table 7.\n",
            "4.3 Initialization for QAT\n",
            "In this section, we will explore the effect of initialization for QAT. It is common practice in literature\n",
            "to start from a pre-trained FP32 model (Esser et al., 2020; Krishnamoorthi, 2018; Jacob et al., 2018;\n",
            "Jain et al., 2019). While it is clear that starting from an FP32 model is beneﬁcial, the effect of the\n",
            "quantization initialization on the ﬁnal QAT result is less studied. Here we explore the effect of using\n",
            "several of our PTQ techniques as an initial step before doing QAT.\n",
            "Effect of range estimation To assess the effect of the initial range setting (see section 3.1) for\n",
            "weights and activations, we perform two sets of experiments, which are summarized in table 8. In the\n",
            "ﬁrst experiment, we quantize the weights to 4-bits and keep the activations in 8-bits. We compare the\n",
            "min-max initialization with the MSE based initialization for the weights quantization range. While\n",
            "the MSE initialized model has a signiﬁcantly higher starting accuracy, the gap closes after training\n",
            "for 20 epochs.\n",
            "To explore the same effect for activation quantization, we perform a similar experiment, where we now\n",
            "quantize the activation to 4-bits and compare min-max initialization with MSE based initialization.\n",
            "The observations from weight range initialization hold here as well. In ﬁgure 11 we show the full\n",
            "training curve of this experiment. In the ﬁrst few epochs, there is a signiﬁcant advantage for using\n",
            "MSE initialization, which almost vanishes in the later stage of training. In conclusion, a better\n",
            "initialization can lead to better QAT results, but the gain is usually small and vanishes the longer the\n",
            "training lasts.\n",
            "0 2 4 6 8 10 12 14 16 18 20\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"Epochs6263646566676869Accuracy (%)\n",
            "W4A4 w/ min-max acts\n",
            "W4A4 w/ MSE acts\n",
            "Figure 11: Inﬂuence of the initial activation range setting on the QAT training behavior of ResNet18 .\n",
            "Average ImageNet validation accuracy (%) after each training epoch over 3 runs (and standard\n",
            "deviation shaded).\n",
            "22Model (FP32 accuracy) ResNet18 (69.68) MobileNetV2 (71.72)\n",
            "PTQ QAT PTQ QAT\n",
            "W4A8 baseline 18.58 69.74 0.10 0.10\n",
            "W4A8 w/ CLE 16.29 69.76 12.99 70.13\n",
            "W4A8 w/ CLE + BC 38.58 69.72 46.90 70.07\n",
            "Table 9: Ablation study with various PTQ initialization. The learning rate is individually optimized\n",
            "for each conﬁguration. ImageNet validation accuracy (%) averaged over 3 runs.\n",
            "Effect of CLE In table 9 we compare the effect of other PTQ improvements such as CLE and\n",
            "bias correction. While for ResNet18 we do not see a signiﬁcant difference in the ﬁnal QAT perfor-\n",
            "mance, for MobileNetV2 we observe that it cannot be trained without CLE. This is likely due to the\n",
            "catastrophic performance drop caused by per-tensor quantization, which we discussed in section 3.2.\n",
            "In conclusion, for models that have severe issues with plain PTQ we may need advanced PTQ\n",
            "techniques such as CLE to initialize QAT. In most other cases, an improved PTQ initialization leads\n",
            "only to a minor improvement in the ﬁnal QAT performance.\n",
            "4.4 Standard QAT pipeline\n",
            "In this section, we present a best-practice pipeline for QAT based on relevant literature and extensive\n",
            "experimentation. We illustrate the recommended pipeline in ﬁgure 12. This pipeline yields good QAT\n",
            "results over a variety of computer vision and natural language processing models and tasks, and can\n",
            "be seen as the go-to tool for achieving low-bit quantization performance. As discussed in previous\n",
            "sections, we always start from a pre-trained model and follow some PTQ steps in order to have faster\n",
            "convergence and higher accuracy.\n",
            "Cross-layer equalization Similar to PTQ, we ﬁrst apply CLE to the full precision model. As\n",
            "we saw in table 9, this step is necessary for models that suffer from imbalanced weight\n",
            "distributions, such as MobileNet architectures. For other networks or in the case of per-\n",
            "channel quantization this step can be optional.\n",
            "Add quantizers Next, we choose our quantizers and add quantization operations in our network as\n",
            "described in section 2.3. The choice for quantizer might depend on the speciﬁc target HW,\n",
            "for common AI accelerators we recommend using symmetric quantizers for the weights\n",
            "and asymmetric quantizers for the activations. If supported by the HW/SW stack, then it is\n",
            "favorable to use per-channel quantization for weights. At this stage we will also take care\n",
            "that our simulation of batch normalization is correct, as discussed in section 4.2.\n",
            "Range estimation Before training we have to initialize all quantization parameters. A better ini-\n",
            "tialization will help faster training and might improve the ﬁnal accuracy, though often\n",
            "the improvement is small (see table 8). In general, we recommend to set all quantization\n",
            "parameters using the layer-wise MSE based criteria. In the speciﬁc case of per-channel\n",
            "quantization, using the min-max setting can sometimes be favorable.\n",
            "Learnable Quantization Parameters We recommend making the quantizer paramaters learnable,\n",
            "as discussed in section 4.1. Learning the quantization parameters directly, rather than\n",
            "updating them at every epoch, leads to higher performance especially when dealing with\n",
            "low-bit quantization. However, using learnable quantizers requires special care when setting\n",
            "up the optimizer for the task. When using SGD-type optimizers, the learning rate for the\n",
            "quantization parameters needs to be reduced compared to the rest of the network parameters.\n",
            "The learning rate adjustment can be avoided if we use optimizers with adaptive learning\n",
            "rates such as Adam or RMSProp.\n",
            "4.5 Experiments\n",
            "Using our QAT pipeline, we quantize and evaluate the same models we used for PTQ in section\n",
            "3.6. Our results are presented in table 10 for different bit-widths and quantization granularities.\n",
            "DeepLabV3 is trained for 80 epochs on Pascal VOC; EfﬁcientDet for 20 epochs on COCO 2017;\n",
            "all other vision models are trained for 20 epochs on ImageNet. BERT-base is trained on each of the\n",
            "2314Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample textQAT Pipeline\n",
            "Pre-trained FP model\n",
            "Add Quantizers\n",
            "Range Estimation\n",
            "CLE\n",
            "Learnable Quantization Params\n",
            "Train\n",
            "Symmetric Weights\n",
            "Asymmetric Activations\n",
            "MSE Range Setting\n",
            "Figure 12: Standard quantization-aware training pipeline. The blue boxes represent the steps and the\n",
            "turquoise boxes recommended choices.\n",
            "Per-tensor Per-channel\n",
            "Models FP32 W8A8 W4A8 W4A4 W8A8 W4A8 W4A4\n",
            "ResNet18 69.68 70.38 69.76 68.32 70.43 70.01 68.83\n",
            "ResNet50 76.07 76.21 75.89 75.10 76.58 76.52 75.53\n",
            "InceptionV3 77.40 78.33 77.84 77.49 78.45 78.12 77.74\n",
            "MobileNetV2 71.72 71.76 70.17 66.43 71.82 70.48 66.89\n",
            "EfﬁcientNet lite 75.42 75.17 71.55 70.22 74.75 73.92 71.55\n",
            "DeeplabV3 72.94 73.99 70.90 66.78 72.87 73.01 68.90\n",
            "EfﬁcientDet-D1 40.08 38.94 35.34 24.70 38.97 36.75 28.68\n",
            "BERT-base 83.06 83.26 82.64 78.83 82.44 82.39 77.63\n",
            "Table 10: Performance (average over 3 runs) of our standard QAT pipeline for various models\n",
            "and tasks. DeeplabV3 ( MobileNetV2 backbone) is evaluated on Pascal VOC (mean intersection\n",
            "over union), EfﬁcientDet-D1 on COCO 2017 (mean average precision), BERT-base on the GLUE\n",
            "benchmark and all other models on ImageNet (accuracy). We evaluate all models on the respective\n",
            "validation sets. Higher is better in all cases.\n",
            "corresponding GLUE tasks for 3 to 12 epochs depending on the task and the quantization granularity.\n",
            "We use the Adam optimizer for all models. We present the results with the best learning rate per\n",
            "quantization conﬁguration and perform no further hyper-parameter tuning.\n",
            "We observe that for networks without depth-wise separable convolutions (ﬁrst 3 rows of table 10),\n",
            "W8A8 and W4A8 quantization perform on par with and even outperform the ﬂoating-point model in\n",
            "certain cases. This could be due to the regularizing effect of training with quantization noise or due to\n",
            "the additional ﬁne-tuning during QAT. For the more aggressive W4A4 case, we notice a small drop\n",
            "but still within 1% of the ﬂoating-point accuracy.\n",
            "Quantizing networks with depth-wise separable layers ( MobileNetV2 , EfﬁcientNet lite, DeeplabV3,\n",
            "EfﬁcientDet-D1) is more challenging; a trend we also observed from the PTQ results in section 3.6\n",
            "and discussed in the literature (Chin et al., 2020; Sheng et al., 2018a). Whereas 8-bit quantization\n",
            "incurs close to no accuracy drop, quantizing weights to 4 bits leads to a larger drop, e.g. approximately\n",
            "4% drop for EfﬁcientNet lite with per-tensor quantization. Per-channel quantization can improve\n",
            "performance signiﬁcantly bringing DeepLabV3 to ﬂoating-point accuracy and reducing the gap of\n",
            "MobileNetV2 and EfﬁcientNet lite to less than 1.5%. Quantizing both weights and activations to\n",
            "4-bits remains a challenging for such networks, even with per-channel quantization it can lead to a\n",
            "drop of up to 5%. EfﬁcientDet-D1 remains more difﬁcult to quantize than the other networks in this\n",
            "group.\n",
            "For BERT-base we observe that QAT with range learning can efﬁciently deal with the high dynamic\n",
            "ranges allowing to keep all activations in 8 bits (unlike for PTQ). W4A8 stays within 1% of the\n",
            "original GLUE score, indicating that low bit weight quantization is not a problem for transformer\n",
            "models. We only notice a signiﬁcant drop in performance when combining this with low bit activation\n",
            "quantization (W4A4).\n",
            "245 Summary and Conclusions\n",
            "Deep learning has become an integral part of many machine learning applications and can now\n",
            "be found in countless electronic devices and services, from smartphones and home appliances to\n",
            "drones, robots and self-driving cars. As the popularity and reach of deep learning in our everyday life\n",
            "increases, so does the need for fast and power-efﬁcient neural network inference. Neural network\n",
            "quantization is one of the most effective ways of reducing the energy and latency requirements of\n",
            "neural networks during inference.\n",
            "Quantization allows us to move from ﬂoating-point representations to a ﬁxed-point format and, in\n",
            "combination with dedicated hardware utilizing efﬁcient ﬁxed-point operations, has the potential\n",
            "to achieve signiﬁcant power gains and accelerate inference. However, to exploit these savings, we\n",
            "require robust quantization methods that can maintain high accuracy, while reducing the bit-width\n",
            "of weights and activations. To this end, we consider two main classes of quantization algorithms:\n",
            "Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT).\n",
            "Post-training quantization techniques take a pre-trained FP32 networks and convert it into a ﬁxed-\n",
            "point network without the need for the original training pipeline. This makes them a lightweight,\n",
            "push-button approach to quantization with low engineering effort and computational cost. We describe\n",
            "a series of recent advances in PTQ and introduce a PTQ pipeline that leads to near ﬂoating-point\n",
            "accuracy results for a wide range of models and machine learning tasks. In particular, using the\n",
            "proposed pipeline we can achieve 8-bit quantization of weights and activations within only 1% of\n",
            "the ﬂoating-point accuracy for all networks. We further show that many networks can be quantized\n",
            "even to 4-bit weights with only a small additional drop in performance. In addition, we introduce a\n",
            "debugging workﬂow to effectively identify and ﬁx problems that might occur when quantizing new\n",
            "networks.\n",
            "Quantization-aware training models the quantization noise during training through simulated quanti-\n",
            "zation operations. This training procedure allows for better solutions to be found compared to PTQ\n",
            "while enabling more effective and aggressive activation quantization. Similar to PTQ, we introduce a\n",
            "standard training pipeline utilizing the latest algorithms in the ﬁeld. We also pay special attention to\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"batch normalization folding during QAT and show that simple static folding outperforms other more\n",
            "computationally expensive approaches. We demonstrate that with our QAT pipeline we can achieve\n",
            "4-bit quantization of weights, and for some models even 4-bit activations, with only a small drop of\n",
            "accuracy compared to ﬂoating-point.\n",
            "The choice between PTQ and QAT depends on the accuracy and power requirements of the application.\n",
            "Both approaches are an essential part of any model efﬁciency toolkit and we hope that our proposed\n",
            "pipelines will help engineers deploy high-performing quantized models with less time and effort.\n",
            "References\n",
            "Banner, R., Nahshan, Y ., and Soudry, D. Post training 4-bit quantization of convolutional networks\n",
            "for rapid-deployment. Neural Information Processing Systems (NeuRIPS) , 2019. 9\n",
            "Bengio, Y ., Léonard, N., and Courville, A. Estimating or propagating gradients through stochastic\n",
            "neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013. 19\n",
            "Bhalgat, Y ., Lee, J., Nagel, M., Blankevoort, T., and Kwak, N. Lsq+: Improving low-bit quantization\n",
            "through learnable offsets and better initialization. In Proceedings of the IEEE/CVF Conference on\n",
            "Computer Vision and Pattern Recognition (CVPR) Workshops , June 2020. 20\n",
            "Chen, L.-C., Papandreou, G., Schroff, F., and Adam, H. Rethinking atrous convolution for semantic\n",
            "image segmentation, 2017. 16\n",
            "Chin, T.-W., Chuang, P. I.-J., Chandra, V ., and Marculescu, D. One weight bitwidth to rule them\n",
            "all. In Bartoli, A. and Fusiello, A. (eds.), Computer Vision – ECCV 2020 Workshops , pp. 85–103,\n",
            "Cham, 2020. Springer International Publishing. ISBN 978-3-030-68238-5. 24\n",
            "Dong, Z., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. HAWQ: hessian aware quantization\n",
            "of neural networks with mixed-precision. International Conference on Computer Vision (ICCV) ,\n",
            "2019. 8\n",
            "25Esser, S. K., McKinstry, J. L., Bablani, D., Appuswamy, R., and Modha, D. S. Learned step size\n",
            "quantization. International Conference on Learning Representations (ICLR) , 2020. 20, 22\n",
            "Finkelstein, A., Almog, U., and Grobman, M. Fighting quantization bias with bias. arXiv preprint\n",
            "arxiv:1906.03193 , 2019. 13\n",
            "Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. Deep learning with limited numerical\n",
            "precision. International Conference on Machine Learning, ICML , 2015. 14\n",
            "Horowitz, M. 1.1 computing’s energy problem (and what we can do about it). In 2014 IEEE\n",
            "International Solid-State Circuits Conference Digest of Technical Papers (ISSCC) , pp. 10–14, 2014.\n",
            "doi: 10.1109/ISSCC.2014.6757323. 2, 3\n",
            "Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and\n",
            "Adam, H. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications.\n",
            "arXiv preprint arXiv:1704.04861 , 2017. 10\n",
            "Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal\n",
            "covariate shift. In Bach, F. and Blei, D. (eds.), Proceedings of the 32nd International Conference on\n",
            "Machine Learning , volume 37 of Proceedings of Machine Learning Research , pp. 448–456, Lille,\n",
            "France, 07–09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/ioffe15.html .\n",
            "6\n",
            "Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and Kalenichenko,\n",
            "D. Quantization and training of neural networks for efﬁcient integer-arithmetic-only inference.\n",
            "Conference on Computer Vision and Pattern Recognition (CVPR) , 2018. 3, 6, 21, 22\n",
            "Jain, S. R., Gural, A., Wu, M., and Dick, C. Trained uniform quantization for accurate and efﬁcient\n",
            "neural network inference on ﬁxed-point hardware. CoRR , abs/1903.08066, 2019. URL http:\n",
            "//arxiv.org/abs/1903.08066 . 20, 22\n",
            "Krishnamoorthi, R. Quantizing deep convolutional networks for efﬁcient inference: A whitepaper.\n",
            "arXiv preprint arXiv:1806.08342 , 2018. 6, 10, 12, 20, 21, 22\n",
            "Meller, E., Finkelstein, A., Almog, U., and Grobman, M. Same, same but different: Recovering neural\n",
            "network quantization error through weight factorization. In Proceedings of the 36th International\n",
            "Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , pp.\n",
            "4486–4495, 2019. URL http://proceedings.mlr.press/v97/meller19a.html . 11, 12, 13\n",
            "Moons, B., Noorzad, P., Skliar, A., Mariani, G., Mehta, D., Lott, C., and Blankevoort, T. Distilling\n",
            "optimal neural networks: Rapid search in diverse spaces. arXiv preprint arXiv:2012.08859 , 2020.\n",
            "15\n",
            "Nagel, M., van Baalen, M., Blankevoort, T., and Welling, M. Data-free quantization through weight\n",
            "equalization and bias correction. International Conference on Computer Vision (ICCV) , 2019. 10,\n",
            "11, 12, 13\n",
            "Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. Up or down? Adaptive\n",
            "rounding for post-training quantization. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th\n",
            "International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning\n",
            "Research , pp. 7197–7206. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/\n",
            "v119/nagel20a.html . 13, 14, 15\n",
            "Nascimento, M. G. d., Fawcett, R., and Prisacariu, V . A. Dsconv: Efﬁcient convolution operator.\n",
            "InProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , October\n",
            "2019. 5\n",
            "Ramachandran, P., Zoph, B., and Le, Q. V . Searching for activation functions. CoRR , abs/1710.05941,\n",
            "2017. URL http://arxiv.org/abs/1710.05941 . 7\n",
            "Rouhani, B., Lo, D., Zhao, R., Liu, M., Fowers, J., Ovtcharov, K., Vinogradsky, A., Massengill, S.,\n",
            "Yang, L., Bittner, R., Forin, A., Zhu, H., Na, T., Patel, P., Che, S., Koppaka, L. C., Song, X., Som,\n",
            "S., Das, K., Tiwary, S., Reinhardt, S., Lanka, S., Chung, E., and Burger, D. Pushing the limits of\n",
            "narrow precision inferencing at cloud scale with microsoft ﬂoating point. In Neural Information\n",
            "Processing Systems (NeurIPS 2020) . ACM, November 2020. 5\n",
            "26Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. Mobilenetv2: Inverted residuals\n",
            "and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern\n",
            "recognition , pp. 4510–4520, 2018. 11\n",
            "Sheng, T., Feng, C., Zhuo, S., Zhang, X., Shen, L., and Aleksic, M. A quantization-friendly separable\n",
            "convolution for mobilenets. 2018 1st Workshop on Energy Efﬁcient Machine Learning and\n",
            "Cognitive Computing for Embedded Applications (EMC2) , Mar 2018a. doi: 10.1109/emc2.2018.\n",
            "00011. URL http://dx.doi.org/10.1109/EMC2.2018.00011 . 24\n",
            "Sheng, T., Feng, C., Zhuo, S., Zhang, X., Shen, L., and Aleksic, M. A quantization-friendly separable\n",
            "convolution for mobilenets. In 1st Workshop on Energy Efﬁcient Machine Learning and Cognitive\n",
            "Computing for Embedded Applications (EMC2) , 2018b. URL https://ieeexplore.ieee.org/\n",
            "abstract/document/8524017 . 10\n",
            "Stock, P., Joulin, A., Gribonval, R., Graham, B., and Jégou, H. And the bit goes down: Revisiting the\n",
            "quantization of neural networks. CoRR , abs/1907.05686, 2019. URL http://arxiv.org/abs/\n",
            "1907.05686 . 5\n",
            "Tan, M., Pang, R., and Le, Q. V . Efﬁcientdet: Scalable and efﬁcient object detection, 2020. 16\n",
            "Uhlich, S., Mauch, L., Cardinaux, F., Yoshiyama, K., Garcia, J. A., Tiedemann, S., Kemp, T., and\n",
            "Nakamura, A. Mixed precision dnns: All you need is a good parametrization. In International\n",
            "Conference on Learning Representations , 2020. URL https://openreview.net/forum?id=\n",
            "Hyx0slrFvH . 8\n",
            "van Baalen, M., Louizos, C., Nagel, M., Amjad, R. A., Wang, Y ., Blankevoort, T., and Welling, M.\n",
            "Bayesian bits: Unifying quantization and pruning. In Larochelle, H., Ranzato, M., Hadsell, R.,\n",
            "Balcan, M. F., and Lin, H. (eds.), Advances in Neural Information Processing Systems , volume 33,\n",
            "pp. 5741–5752. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/\n",
            "paper/2020/file/3f13cf4ddf6fc50c0d39a1d5aeb57dd8-Paper.pdf . 8\n",
            "Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. GLUE: A multi-task benchmark\n",
            "and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP\n",
            "Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pp. 353–355,\n",
            "Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/\n",
            "W18-5446. URL https://www.aclweb.org/anthology/W18-5446 . 16\n",
            "27\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"The white paper discusses the importance of neural network quantization in reducing computational costs for edge devices. It introduces state-of-the-art algorithms for mitigating quantization noise while maintaining low-bit weights and activations. Two main methods, Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT), are discussed, with PTQ requiring minimal re-training and QAT requiring fine-tuning. The paper also covers hardware considerations, quantization fundamentals, and standard pipelines for achieving state-of-the-art performance in deep learning models.\n",
            "\n",
            "The text discusses uniform quantization, a commonly used scheme in quantization, which allows for efficient implementation of fixed-point arithmetic. It explains the parameters involved in uniform affine quantization, the quantization operation, and the trade-offs between clipping and rounding errors. The text also covers symmetric uniform quantization, which simplifies the quantization process by restricting the zero-point offset. It further explores the difference between symmetric unsigned and symmetric signed quantization.\n",
            "\n",
            "The text discusses the concept of quantization in neural networks, focusing on different types of quantization methods, quantization granularity, and simulation techniques. It explains power-of-two quantization, quantization granularity, per-tensor and per-channel quantization, and batch normalization folding. The text also covers activation function fusing, simulation of other layers like max pooling and average pooling, and challenges in simulating element-wise addition accurately.\n",
            "\n",
            "The text discusses the challenges and practical considerations when quantizing neural networks with multiple layers. It covers topics such as concatenation, symmetric vs. asymmetric quantization, per-tensor and per-channel quantization, and post-training quantization. The post-training quantization process involves determining quantization ranges, with methods such as min-max, mean squared error (MSE), and cross entropy. The text also introduces the concept of cross-layer equalization to address quantization errors due to differences in magnitude within tensors.\n",
            "\n",
            "The text discusses the challenges of quantization in depth-wise separable layers in neural networks, particularly in models like MobileNetV1 and MobileNetV2. It highlights issues of weight imbalance and variability, as well as the impact of batch normalization folding. The authors propose a solution to address imbalances through positive scaling equivariance in consecutive layers and cross-layer equalization. They also suggest absorbing high biases into the next layer to minimize dynamic range differences. Experiments show the effectiveness of these techniques in improving model robustness to quantization in MobileNetV2.\n",
            "\n",
            "The text discusses the importance of bias correction and cross-layer equalization in improving the performance of quantized neural networks. It also introduces AdaRound, a method for optimizing weight rounding choices for post-training quantization. By addressing biased errors and optimizing weight rounding, AdaRound significantly improves model performance without using additional data.\n",
            "\n",
            "The text discusses the impact of weight rounding during the quantization operation on network performance. It introduces AdaRound as a fast weight rounding method that requires minimal data samples and no hyperparameter tuning. The text also outlines a standard PTQ pipeline for quantization, including steps such as cross-layer equalization, adding quantizers, weight range setting, AdaRound application, bias correction, and activation range setting. Experimental results show that 8-bit quantization of weights and activations leads to minimal accuracy loss compared to floating-point. The text also provides a debugging flowchart for identifying and fixing performance issues in quantized models.\n",
            "\n",
            "The text discusses quantization techniques in deep learning models, focusing on quantization-aware training (QAT) to optimize for low-bit quantization. It explains how simulated quantization is used in training models with QAT, including the use of the straight-through estimator (STE) for gradient calculation. The text also covers batch normalization folding in QAT, highlighting the effectiveness of static folding and per-channel quantization. Additionally, it explores the impact of initialization techniques on QAT results, showing that better initialization can lead to improved performance.\n",
            "\n",
            "The text discusses the influence of initial activation range setting on the quantization-aware training behavior of ResNet18, as well as an ablation study on various post-training quantization (PTQ) initialization methods. It also outlines a standard quantization-aware training (QAT) pipeline for achieving low-bit quantization performance, including steps such as cross-layer equalization, adding quantizers, range estimation, and using learnable quantization parameters. Experimental results show the performance of the QAT pipeline on various models and tasks, with insights on the challenges and solutions for quantizing networks with depth-wise separable layers. The text emphasizes the importance of quantization in reducing energy and latency requirements for neural network inference in various devices and applications.\n",
            "\n",
            "The text discusses the benefits of batch normalization folding during quantization-aware training (QAT) and compares simple static folding to other more computationally expensive approaches. The authors demonstrate that their QAT pipeline can achieve 4-bit quantization of weights and, in some cases, 4-bit activations with only a small drop in accuracy compared to floating-point. The choice between post-training quantization (PTQ) and QAT depends on the accuracy and power requirements of the application. The text also references various studies and papers related to neural network quantization and efficiency.\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN-5iCR4czef",
        "outputId": "300f8ed2-fd2a-407e-f76e-370f95ec3a82"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The white paper delves into the importance of neural network quantization in reducing computational costs for edge devices. It discusses state-of-the-art algorithms like Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT), hardware considerations, and standard pipelines for achieving optimal performance in deep learning models. The paper covers various quantization techniques, challenges in multi-layer quantization, bias correction, weight rounding, and the impact of initialization techniques. It also explores the benefits of batch normalization folding and emphasizes the significance of quantization in enhancing the efficiency of neural network inference in different devices and applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Map Reduce for custom prompt**"
      ],
      "metadata": {
        "id": "XL7gBd6pks6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_prompt=\"\"\"\n",
        "Please summarize the below speech:\n",
        "Speech:`{text}'\n",
        "Summary:\n",
        "\"\"\"\n",
        "map_prompt_template=PromptTemplate(input_variables=['text'],\n",
        "                                    template=chunks_prompt)"
      ],
      "metadata": {
        "id": "RBxthNA1k0b7"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_combine_prompt='''\n",
        "Provide a final summary of the entire speech with these important points.\n",
        "Add a Generic Motivational Title,\n",
        "Start the precise summary with an introduction and provide the\n",
        "summary in number points for the speech.\n",
        "Speech: `{text}`\n",
        "'''\n",
        "final_combine_prompt_template=PromptTemplate(input_variables=['text'],\n",
        "                                             template=final_combine_prompt)"
      ],
      "metadata": {
        "id": "dO2lVTSamRFW"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type='map_reduce',\n",
        "    map_prompt=map_prompt_template,\n",
        "    combine_prompt=final_combine_prompt_template,\n",
        "    verbose=False\n",
        ")\n",
        "output = summary_chain.run(chunks)"
      ],
      "metadata": {
        "id": "E9qAo6P7mkMH"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP3wITHempRt",
        "outputId": "0d9c185e-7c77-4b3d-9737-be0590980e6b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: \"Optimizing Efficiency: The Power of Neural Network Quantization\"\n",
            "\n",
            "Introduction:\n",
            "The speech delves into the significance of neural network quantization in reducing computational costs for inference, especially on edge devices with constraints. It introduces cutting-edge algorithms like Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT), along with practical considerations for implementation.\n",
            "\n",
            "Summary:\n",
            "1. Explains the concept of uniform quantization, parameters like scale factors, zero points, and bit width, and the trade-off between clipping and rounding errors.\n",
            "2. Covers aspects of quantization in neural networks such as uniform quantization grids, power-of-two quantization, and techniques like batch normalization folding and activation function fusing.\n",
            "3. Discusses practical considerations for quantizing neural networks with multiple layers, including setting quantization ranges and addressing quantization errors through cross-layer equalization.\n",
            "4. Explores challenges in quantizing depth-wise separable layers in models like MobileNetV1 and MobileNetV2, presenting solutions like positive scaling equivariance and bias absorption.\n",
            "5. Emphasizes the importance of bias correction, cross-layer equalization, and weight quantization in enhancing the performance of quantized neural networks.\n",
            "6. Introduces AdaRound for optimal weight rounding choices in post-training quantization and highlights the impact of different quantization strategies on model accuracy.\n",
            "7. Discusses the benefits of Quantization-Aware Training (QAT) in improving model accuracy through modeling quantization noise during training and simulating quantization for the backward path.\n",
            "8. Compares Post-Training Quantization (PTQ) and QAT techniques for achieving high accuracy with reduced bit-width weights and activations, showcasing the effectiveness of batch normalization folding.\n",
            "9. Emphasizes the importance of robust quantization methods for efficient neural network inference and references various studies related to quantization and neural network efficiency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RefineChain For Summarization**"
      ],
      "metadata": {
        "id": "dkAcQRb8mv42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chain collapses documents by generating an initial answer based on the first document and then looping over the remaining documents to refine its answer. This operates sequentially, so it cannot be parallelized. It is useful in similar situations as MapReduceDocuments Chain, but for cases where you want to build up an answer by refining the previous answer (rather than parallelizing calls)."
      ],
      "metadata": {
        "id": "JfRA8kpfpO3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain=load_summarize_chain(llm,chain_type=\"refine\",verbose=True)\n",
        "output_summary = chain.run(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u7BRqMjmuyC",
        "outputId": "fd54c35b-6629-4d9b-a87f-6eb08f91c377"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"A White Paper on Neural Network Quantization\n",
            "Markus Nagel∗\n",
            "Qualcomm AI Research†\n",
            "markusn@qti.qualcomm.comMarios Fournarakis∗\n",
            "Qualcomm AI Research†\n",
            "mfournar@qti.qualcomm.com\n",
            "Rana Ali Amjad\n",
            "Qualcomm AI Research†\n",
            "ramjad@qti.qualcomm.comYelysei Bondarenko\n",
            "Qualcomm AI Research†\n",
            "ybodaren@qti.qualcomm.com\n",
            "Mart van Baalen\n",
            "Qualcomm AI Research†\n",
            "mart@qti.qualcomm.comTijmen Blankevoort\n",
            "Qualcomm AI Research†\n",
            "tijmen@qti.qualcomm.com\n",
            "Abstract\n",
            "While neural networks have advanced the frontiers in many applications, they\n",
            "often come at a high computational cost. Reducing the power and latency of neural\n",
            "network inference is key if we want to integrate modern networks into edge devices\n",
            "with strict power and compute requirements. Neural network quantization is one\n",
            "of the most effective ways of achieving these savings but the additional noise it\n",
            "induces can lead to accuracy degradation.\n",
            "In this white paper, we introduce state-of-the-art algorithms for mitigating the\n",
            "impact of quantization noise on the network’s performance while maintaining\n",
            "low-bit weights and activations. We start with a hardware motivated introduction\n",
            "to quantization and then consider two main classes of algorithms: Post-Training\n",
            "Quantization (PTQ) and Quantization-Aware-Training (QAT). PTQ requires no\n",
            "re-training or labelled data and is thus a lightweight push-button approach to\n",
            "quantization. In most cases, PTQ is sufﬁcient for achieving 8-bit quantization with\n",
            "close to ﬂoating-point accuracy. QAT requires ﬁne-tuning and access to labeled\n",
            "training data but enables lower bit quantization with competitive results. For both\n",
            "solutions, we provide tested pipelines based on existing literature and extensive\n",
            "experimentation that lead to state-of-the-art performance for common deep learning\n",
            "models and tasks.\n",
            "1 Introduction\n",
            "With the rise in popularity of deep learning as a general-purpose tool to inject intelligence into\n",
            "electronic devices, the necessity for small, low-latency and energy efﬁcient neural networks solutions\n",
            "has increased. Today neural networks can be found in many electronic devices and services, from\n",
            "smartphones, smart glasses and home appliances, to drones, robots and self-driving cars. These\n",
            "devices are typically subject to strict time restrictions on the execution of neural networks or stringent\n",
            "power requirements for long-duration performance.\n",
            "One of the most impactful ways to decrease the computational time and energy consumption of\n",
            "neural networks is quantization. In neural network quantization, the weights and activation tensors\n",
            "are stored in lower bit precision than the 16 or 32-bit precision they are usually trained in. When\n",
            "∗Equal contribution.†Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.arXiv:2106.08295v1  [cs.LG]  15 Jun 2021moving from 32 to 8 bits, the memory overhead of storing tensors decreases by a factor of 4 while the\n",
            "computational cost for matrix multiplication reduces quadratically by a factor of 16. Neural networks\n",
            "have been shown to be robust to quantization, meaning they can be quantized to lower bit-widths\n",
            "with a relatively small impact on the network’s accuracy. Besides, neural network quantization can\n",
            "often be applied along with other common methods for neural network optimization, such as neural\n",
            "architecture search, compression and pruning. It is an essential step in the model efﬁciency pipeline\n",
            "for any practical use-case of deep learning. However, neural network quantization is not free. Low\n",
            "bit-width quantization introduces noise to the network that can lead to a drop in accuracy. While\n",
            "some networks are robust to this noise, other networks require extra work to exploit the beneﬁts of\n",
            "quantization.\n",
            "In this white paper, we introduce the state-of-the-art in neural network quantization. We start with an\n",
            "introduction to quantization and discuss hardware and practical considerations. We then consider two\n",
            "different regimes of quantizing neural networks: Post-Training Quantization (PTQ) and Quantization-\n",
            "Aware Training (QAT). PTQ methods, discussed in section 3, take a trained network and quantize\n",
            "it with little or no data, requires minimal hyperparameter tuning and no end-to-end training. This\n",
            "makes them a push-button approach to quantizing neural networks with low engineering effort and\n",
            "computational cost. In contrast, QAT, discussed in section 4, relies on retraining the neural networks\n",
            "with simulated quantization in the training pipeline. While this requires more effort in training and\n",
            "potentially hyperparameter tuning, it generally further closes the gap to the full-precision accuracy\n",
            "compared to PTQ for low-bit quantization. For both regimes, we introduce standard pipelines based\n",
            "on existing literature and extensive experimentation that lead to state-of-the-art performance for\n",
            "common computer vision and natural language processing models. We also propose a debugging\n",
            "workﬂow to identify and address common issues when quantizing a new model.\n",
            "2 Quantization fundamentals\n",
            "In this section, we introduce the basic principles of neural network quantization and of ﬁxed-point\n",
            "accelerators on which quantized networks run on. We start with a hardware motivation and then\n",
            "introduce standard quantization schemes and their properties. Later we discuss practical considerations\n",
            "related to layers commonly found in modern neural networks and their implications for ﬁxed-point\n",
            "accelerators.\n",
            "2.1 Hardware background\n",
            "Before diving into the technical details, we ﬁrst explore the hardware background of quantization\n",
            "and how it enables efﬁcient inference on device. Figure 1 provides a schematic overview of how\n",
            "a matrix-vector multiplication, y=Wx+b, is calculated in a neural network (NN) accelerator.\n",
            "This is the building block of larger matrix-matrix multiplications and convolutions found in neural\n",
            "networks. Such hardware blocks aim at improving the efﬁciency of NN inference by performing as\n",
            "many calculations as possible in parallel. The two fundamental components of this NN accelerator\n",
            "are the processing elements Cn,mand the accumulators An. Our toy example in ﬁgure 1 has 16\n",
            "processing elements arranged in a square grid and 4accumulators. The calculation starts by loading\n",
            "the accumulators with the bias value bn. We then load the weight values Wn,mand the input values\n",
            "xminto the array and compute their product in the respective processing elements Cn,m=Wn,mxm\n",
            "in a single cycle. Their results are then added in the accumulators:\n",
            "An=bn+∑\n",
            "mCn,m (1)\n",
            "The above operation is also referred to as Multiply-Accumulate (MAC). This step is repeated many\n",
            "times for larger matrix-vector multiplications. Once all cycles are completed, the values in the\n",
            "accumulators are then moved back to memory to be used in the next neural network layer. Neural\n",
            "networks are commonly trained using FP32 weights and activations. If we were to perform inference\n",
            "in FP32, the processing elements and the accumulator would have to support ﬂoating-point logic,\n",
            "and we would need to transfer the 32-bit data from memory to the processing units. MAC operations\n",
            "and data transfer consume the bulk of the energy spent during neural network inference. Hence,\n",
            "signiﬁcant beneﬁts can be achieved by using a lower bit ﬁxed-point or quantized representation for\n",
            "these quantities. Low-bit ﬁxed-point representations, such as INT8, not only reduce the amount data\n",
            "transfer but also the size and energy consumption of the MAC operation (Horowitz, 2014). This is\n",
            "2because the cost of digital arithmetic typically scales linearly to quadratically with the number of bits\n",
            "used and because ﬁxed-point addition is more efﬁcient than its ﬂoating-point counterpart (Horowitz,\n",
            "2014).\n",
            "3Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample textW!,!W!,#W!,$W!,%x!x#x$x%#!,!#!,##!,$#!,%##,!##,###,$##,%#$,!#$,##$,$#$,%#%,!#%,##%,$#%,%$!$#$$$%Input valuesWeight valuesAccumulatorsW#,!W#,#W#,$W#,%W$,!W$,#W$,$W$,%W%,!W%,#W%,$W%,%\n",
            "Figure 1: A schematic overview of matrix-multiply logic in neural network accelerator hardware.\n",
            "To move from ﬂoating-point to the efﬁcient ﬁxed-point operations, we need a scheme for converting\n",
            "ﬂoating-point vectors to integers. A ﬂoating-point vector xcan be expressed approximately as a\n",
            "scalar multiplied by a vector of integer values:\n",
            "ˆx=sx·xint≈x (2)\n",
            "wheresxis a ﬂoating-point scale factor andxintis an integer vector, e.g., INT8. We denote this\n",
            "quantized version of the vector as ˆx. By quantizing the weights and activations we can write the\n",
            "quantized version of the accumulation equation:\n",
            "ˆAn=ˆbn+∑\n",
            "mˆWn,mˆxm\n",
            "=ˆbn+∑\n",
            "m(\n",
            "swWint\n",
            "n,m)(\n",
            "sxxint\n",
            "m)\n",
            "=ˆbn+swsx∑\n",
            "mWint\n",
            "n,mxint\n",
            "m (3)\n",
            "Note that we used a separate scale factor for weights, sw, and activations, sx. This provides ﬂexibility\n",
            "and reduces the quantization error (more in section 2.2). Since each scale factor is applied to the\n",
            "whole tensor, this scheme allows us to factor the scale factors out of the summation in equation (3)\n",
            "and perform MAC operations in ﬁxed-point format. We intentionally ignore bias quantization for\n",
            "now, because the bias is normally stored in higher bit-width (32-bits) and its scale factor depends on\n",
            "that of the weights and activations (Jacob et al., 2018).\n",
            "Figure 2 shows how the neural network accelerator changes when we introduce quantization. In\n",
            "our example, we use INT8 arithmetic, but this could be any quantization format for the sake of\n",
            "this discussion. It is important to maintain a higher bit-width for the accumulators, typical 32-bits\n",
            "wide. Otherwise, we risk incurring loss due to overﬂow as more products are accumulated during the\n",
            "computation.\n",
            "The activations stored in the 32-bit accumulators need to be written to memory before they can be\n",
            "used by the next layer. To reduce data transfer and the complexity of the next layer’s operations, these\n",
            "activations are quantized back to INT8. This requires a requantization step which is shown in ﬁgure\n",
            "2.\n",
            "2.2 Uniform afﬁne quantization\n",
            "In this section we deﬁne the quantization scheme that we will use in this paper. This scheme is\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
            "We have provided an existing summary up to a certain point: The white paper discusses the importance of neural network quantization for reducing computational costs in edge devices with power and compute constraints. It introduces state-of-the-art algorithms for mitigating quantization noise, such as Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). The paper explains the hardware background of quantization, the benefits of fixed-point representations, and standard pipelines for achieving state-of-the-art performance in common deep learning models and tasks. It also discusses practical considerations and debugging workflows for quantizing new models.\n",
            "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
            "------------\n",
            "called uniform quantization and it is the most commonly used quantization scheme because it permits\n",
            "efﬁcient implementation of ﬁxed-point arithmetic.\n",
            "35Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample textx!x#x$x%#!,!#!,##!,$#!,%##,!##,###,$##,%#$,!#$,##$,$#$,%#%,!#%,##%,$#%,%$!$#$$$%Input valuesWeight values!!⋅!\"⋅\n",
            "INT8INT8\n",
            "INT32RequantizationMemoryINT8W!,!W!,#W!,$W!,%#!,!#!,##!,$#!,%##,!##,###,$##,%#$,!#$,##$,$#$,%#%,!#%,##%,$#%,%$!$#$$$%W#,!W#,#W#,$W#,%W$,!W$,#W$,$W$,%W%,!W%,#W%,$W%,%Figure 2: A schematic of matrix-multiply logic in an neural network accelerator for quantized\n",
            "inference.\n",
            "Uniform afﬁne quantization , also known as asymmetric quantization , is deﬁned by three quantization\n",
            "parameters: the scale factors, the zero-pointzand the bit-widthb. The scale factor and the zero-point\n",
            "are used to to map a ﬂoating point value to the integer grid, whose size depends on the bit-width. The\n",
            "scale factor is commonly represented as a ﬂoating-point number and speciﬁes the step-size of the\n",
            "quantizer. The zero-point is an integer that ensures that real zero is quantized without error. This is\n",
            "important to ensure that common operations like zero padding or ReLU do not induce quantization\n",
            "error.\n",
            "Once the three quantization parameters are deﬁned we can proceed with the quantization operation.\n",
            "Starting from a real-valued vector xwe ﬁrst map it to the unsigned integer grid{0,..., 2b−1}:\n",
            "xint= clamp(⌊x\n",
            "s⌉\n",
            "+z; 0,2b−1)\n",
            ", (4)\n",
            "where⌊·⌉is the round-to-nearest operator and clamping is deﬁned as:\n",
            "clamp (x;a,c) =\n",
            "\n",
            "a, x<a,\n",
            "x, a≤x≤c,\n",
            "c, x>c.(5)\n",
            "To approximate the real-valued input xwe perfrom a de-quantization step:\n",
            "x≈ˆx=s(xint−z) (6)\n",
            "Combining the two steps above we can provide a general deﬁnition for the quantization function ,\n",
            "q(·), as:\n",
            "ˆx=q(x;s,z,b ) =s[\n",
            "clamp(⌊x\n",
            "s⌉\n",
            "+z; 0,2b−1)\n",
            "−z]\n",
            ", (7)\n",
            "Through the de-quantization step, we can also deﬁne the quantization grid limits (qmin,qmax)where\n",
            "qmin=−szandqmax=s(2b−1−z). Any values of xthat lie outside this range will be clipped\n",
            "to its limits, incurring a clipping error . If we want to reduce the clipping error we can expand\n",
            "the quantization range by increasing the scale factor. However, increasing the scale factor leads to\n",
            "increased rounding error as the rounding error lies in the range[\n",
            "−1\n",
            "2s,1\n",
            "2s]\n",
            ". In section 3.1, we explore\n",
            "in more detail how to choose the quantization parameters to achieve the right trade-off between\n",
            "clipping and rounding errors.\n",
            "2.2.1 Symmetric uniform quantization\n",
            "Symmetric quantization is a simpliﬁed version of the general asymmetric case. The symmetric\n",
            "quantizer restricts the zero-point to 0. This reduces the computational overhead of dealing with\n",
            "zero-point offset during the accumulation operation in equation (3). But the lack of offset restricts the\n",
            "mapping between integer and ﬂoating-point domain. As a result, the choice of signed or unsigned\n",
            "4integer grid matters:\n",
            "ˆx=sxint (8a)\n",
            "xint= clamp(⌊x\n",
            "s⌉\n",
            "; 0,2b−1)\n",
            "for unsigned integers (8b)\n",
            "xint= clamp(⌊x\n",
            "s⌉\n",
            ";−2b−1,2b−1−1)\n",
            "for signed integers (8c)\n",
            "Unsigned symmetric quantization is well suited for one-tailed distributions, such as ReLU activations\n",
            "(see ﬁgure 3). On the other hand, signed symmetric quantization can be chosen for distributions that\n",
            "are roughly symmetric about zero.\n",
            "13Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.000<latexit sha1_base64=\"KWF1WsL3Cx0f3yYpGDxUvz20H+g=\">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4Kkkp6rHgxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzU1INS2a24C5B14uWkDDkag9JXfxizNEJpmKBa9zw3MX5GleFM4KzYTzUmlE3oCHuWShqh9rPFoTNyaZUhCWNlSxqyUH9PZDTSehoFtjOiZqxXvbn4n9dLTXjrZ1wmqUHJlovCVBATk/nXZMgVMiOmllCmuL2VsDFVlBmbTdGG4K2+vE7a1Yp3Xak2a+V6LY+jAOdwAVfgwQ3U4R4a0AIGCM/wCm/Oo/PivDsfy9YNJ585gz9wPn8A3LmM8A==</latexit>s\n",
            "<latexit sha1_base64=\"+Fy6cPh/qqHpQFs8d3cpWayzoQ8=\">AAAB+XicbVBNS8NAEN3Ur1q/oh69LBbBU0mKqMeCF48V7Qe0oWw2k3bp7ibsbgol9J948aCIV/+JN/+N2zYHbX0w8Hhvhpl5YcqZNp737ZQ2Nre2d8q7lb39g8Mj9/ikrZNMUWjRhCeqGxINnEloGWY4dFMFRIQcOuH4bu53JqA0S+STmaYQCDKULGaUGCsNXPdxKgQYxSjWbCghGrhVr+YtgNeJX5AqKtAcuF/9KKGZAGkoJ1r3fC81QU6UYZTDrNLPNKSEjskQepZKIkAH+eLyGb6wSoTjRNmSBi/U3xM5EVpPRWg7BTEjverNxf+8Xmbi2yBnMs0MSLpcFGccmwTPY8ARU0ANn1pCqGL2VkxHRBFqbFgVG4K/+vI6addr/nWt/lCvNq6KOMroDJ2jS+SjG9RA96iJWoiiCXpGr+jNyZ0X5935WLaWnGLmFP2B8/kDnHaTmQ==</latexit>Symmetric signed<latexit sha1_base64=\"LAN1Fz9JJpBsYhcuRNmqr7w7Hzc=\">AAACCHicbVDLSsNAFJ34rPVVdenCwSK4Kkkp2mXBjcsK9gFNCJPJpB06eTBzIy0hSzf+ihsXirj1E9z5N07bLLT1wMDhnHuZe46XCK7ANL+NtfWNza3t0k55d2//4LBydNxVcSop69BYxLLvEcUEj1gHOAjWTyQjoSdYzxvfzPzeA5OKx9E9TBPmhGQY8YBTAlpyK2fKpn4M2A4JjLwgm+RuZgObQMYjaOa5W6maNXMOvEqsglRRgbZb+bL9mKYhi4AKotTAMhNwMiKBU8Hysp0qlhA6JkM20DQiIVNONg+S4wut+DiIpX4R4Ln6eyMjoVLT0NOTs3vVsjcT//MGKQRNR0dKUmARXXwUpAJDjGetYJ9LRkFMNSFUcn0rpiMiCQXdXVmXYC1HXiXdes26qtXvGtVWo6ijhE7RObpEFrpGLXSL2qiDKHpEz+gVvRlPxovxbnwsRteMYucE/YHx+QMcwpqk</latexit>s·xint8<latexit sha1_base64=\"wQVDMLDEUIqvtUS9drDQVQOpZco=\">AAACCXicbVDLSsNAFJ3UV62vqks3g0VwVZJStMuCG5cV7AOaEiaTSTt0MgkzN9ISsnXjr7hxoYhb/8Cdf+P0sdDWAxcO59zLvff4ieAabPvbKmxsbm3vFHdLe/sHh0fl45OOjlNFWZvGIlY9n2gmuGRt4CBYL1GMRL5gXX98M/O7D0xpHst7mCZsEJGh5CGnBIzklbF2aRADdiMCIz/MJrmXucAmkKVcQiPPvXLFrtpz4HXiLEkFLdHyyl9uENM0YhKoIFr3HTuBQUYUcCpYXnJTzRJCx2TI+oZKEjE9yOaf5PjCKAEOY2VKAp6rvycyEmk9jXzTOTtYr3oz8T+vn0LYGGRcJikwSReLwlRgiPEsFhxwxSiIqSGEKm5uxXREFKFgwiuZEJzVl9dJp1Z1rqq1u3qlWV/GUURn6BxdIgddoya6RS3URhQ9omf0it6sJ+vFerc+Fq0Fazlziv7A+vwBAG+bIw==</latexit>s·xuint80255\n",
            "<latexit sha1_base64=\"KWF1WsL3Cx0f3yYpGDxUvz20H+g=\">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4Kkkp6rHgxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzU1INS2a24C5B14uWkDDkag9JXfxizNEJpmKBa9zw3MX5GleFM4KzYTzUmlE3oCHuWShqh9rPFoTNyaZUhCWNlSxqyUH9PZDTSehoFtjOiZqxXvbn4n9dLTXjrZ1wmqUHJlovCVBATk/nXZMgVMiOmllCmuL2VsDFVlBmbTdGG4K2+vE7a1Yp3Xak2a+V6LY+jAOdwAVfgwQ3U4R4a0AIGCM/wCm/Oo/PivDsfy9YNJ585gz9wPn8A3LmM8A==</latexit>s\n",
            "<latexit sha1_base64=\"BXQfQ41gDqSWSLOzY4///IszX2g=\">AAAB+3icbVBNS8NAEN34WetXrEcvi0XwVJIi6rHgxWNF+wFtKJvNtF26uwm7GzGE/hUvHhTx6h/x5r9x2+agrQ8GHu/NMDMvTDjTxvO+nbX1jc2t7dJOeXdv/+DQPaq0dZwqCi0a81h1Q6KBMwktwwyHbqKAiJBDJ5zczPzOIyjNYvlgsgQCQUaSDRklxkoDt3KfCQFGMYpTqdlIQjRwq17NmwOvEr8gVVSgOXC/+lFMUwHSUE607vleYoKcKMMoh2m5n2pICJ2QEfQslUSADvL57VN8ZpUID2NlSxo8V39P5ERonYnQdgpixnrZm4n/eb3UDK+DnMkkNSDpYtEw5djEeBYEjpgCanhmCaGK2VsxHRNFqLFxlW0I/vLLq6Rdr/mXtfpdvdq4KOIooRN0is6Rj65QA92iJmohip7QM3pFb87UeXHenY9F65pTzByjP3A+fwBMZZSQ</latexit>Symmetric unsigned\n",
            "<latexit sha1_base64=\"UwZKxCfWWgLJS364fOkrkkiOzms=\">AAAB8XicbVDLSgMxFM3UV62vqks3wSK4KjNF1GXFjcsK9oHtUDLpnTY0kxmSO0IZ+hduXCji1r9x59+YtrPQ1gOBwzn3knNPkEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVHJo8lrHuBMyAFAqaKFBCJ9HAokBCOxjfzvz2E2gjYvWAkwT8iA2VCAVnaKXHGzOJIkAteL9ccavuHHSVeDmpkByNfvmrN4h5GoFCLpkxXc9N0M+YRsElTEu91EDC+JgNoWupYhEYP5snntIzqwxoGGv7FNK5+nsjY5Gx0QI7GTEcmWVvJv7ndVMMr/1MqCRFUHzxUZhKijGdnU8HQgNHObGEcS1sVspHTDOOtqSSLcFbPnmVtGpV77Jau69V6hd5HUVyQk7JOfHIFamTO9IgTcKJIs/klbw5xnlx3p2PxWjByXeOyR84nz/KFpDz</latexit>Asymmetric0255\n",
            "<latexit sha1_base64=\"KWF1WsL3Cx0f3yYpGDxUvz20H+g=\">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4Kkkp6rHgxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzU1INS2a24C5B14uWkDDkag9JXfxizNEJpmKBa9zw3MX5GleFM4KzYTzUmlE3oCHuWShqh9rPFoTNyaZUhCWNlSxqyUH9PZDTSehoFtjOiZqxXvbn4n9dLTXjrZ1wmqUHJlovCVBATk/nXZMgVMiOmllCmuL2VsDFVlBmbTdGG4K2+vE7a1Yp3Xak2a+V6LY+jAOdwAVfgwQ3U4R4a0AIGCM/wCm/Oo/PivDsfy9YNJ585gz9wPn8A3LmM8A==</latexit>s\n",
            "<latexit sha1_base64=\"88ad+IysUXrTTs3EV6DCXUwRgCQ=\">AAAB9XicbVBNSwMxEM36WetX1aOXYBG8WHZLUS9CwYvHCvYD2rVk02wbmmSXZFatS/+HFw+KePW/ePPfmLZ70NYHA4/3ZpiZF8SCG3Ddb2dpeWV1bT23kd/c2t7ZLeztN0yUaMrqNBKRbgXEMMEVqwMHwVqxZkQGgjWD4dXEb94zbXikbmEUM1+SvuIhpwSsdNcB9gip5Gp8eWqeuoWiW3KnwIvEy0gRZah1C1+dXkQTyRRQQYxpe24Mfko0cCrYON9JDIsJHZI+a1uqiGTGT6dXj/GxVXo4jLQtBXiq/p5IiTRmJAPbKQkMzLw3Ef/z2gmEF37KVZwAU3S2KEwEhghPIsA9rhkFMbKEUM3trZgOiCYUbFB5G4I3//IiaZRL3lmpfFMpVitZHDl0iI7QCfLQOaqia1RDdUSRRs/oFb05D86L8+58zFqXnGzmAP2B8/kD2iGSsw==</latexit>min =\u0000sz<latexit sha1_base64=\"OXdvs7X7RdZOZARTQeZFogawHdU=\">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvgqewWUY8FLx4r2g9ol5JNs21okl2SrFiW/gQvHhTx6i/y5r8xbfegrQ8GHu/NMDMvTAQ31vO+UWFtfWNzq7hd2tnd2z8oHx61TJxqypo0FrHuhMQwwRVrWm4F6ySaERkK1g7HNzO//ci04bF6sJOEBZIMFY84JdZJ95I89csVr+rNgVeJn5MK5Gj0y1+9QUxTyZSlghjT9b3EBhnRllPBpqVealhC6JgMWddRRSQzQTY/dYrPnDLAUaxdKYvn6u+JjEhjJjJ0nZLYkVn2ZuJ/Xje10XWQcZWklim6WBSlAtsYz/7GA64ZtWLiCKGau1sxHRFNqHXplFwI/vLLq6RVq/qX1dpdrVK/yOMowgmcwjn4cAV1uIUGNIHCEJ7hFd6QQC/oHX0sWgsonzmGP0CfP2cIjdU=</latexit>max<latexit sha1_base64=\"OXdvs7X7RdZOZARTQeZFogawHdU=\">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvgqewWUY8FLx4r2g9ol5JNs21okl2SrFiW/gQvHhTx6i/y5r8xbfegrQ8GHu/NMDMvTAQ31vO+UWFtfWNzq7hd2tnd2z8oHx61TJxqypo0FrHuhMQwwRVrWm4F6ySaERkK1g7HNzO//ci04bF6sJOEBZIMFY84JdZJ95I89csVr+rNgVeJn5MK5Gj0y1+9QUxTyZSlghjT9b3EBhnRllPBpqVealhC6JgMWddRRSQzQTY/dYrPnDLAUaxdKYvn6u+JjEhjJjJ0nZLYkVn2ZuJ/Xje10XWQcZWklim6WBSlAtsYz/7GA64ZtWLiCKGau1sxHRFNqHXplFwI/vLLq6RVq/qX1dpdrVK/yOMowgmcwjn4cAV1uIUGNIHCEJ7hFd6QQC/oHX0sWgsonzmGP0CfP2cIjdU=</latexit>max\n",
            "<latexit sha1_base64=\"OXdvs7X7RdZOZARTQeZFogawHdU=\">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvgqewWUY8FLx4r2g9ol5JNs21okl2SrFiW/gQvHhTx6i/y5r8xbfegrQ8GHu/NMDMvTAQ31vO+UWFtfWNzq7hd2tnd2z8oHx61TJxqypo0FrHuhMQwwRVrWm4F6ySaERkK1g7HNzO//ci04bF6sJOEBZIMFY84JdZJ95I89csVr+rNgVeJn5MK5Gj0y1+9QUxTyZSlghjT9b3EBhnRllPBpqVealhC6JgMWddRRSQzQTY/dYrPnDLAUaxdKYvn6u+JjEhjJjJ0nZLYkVn2ZuJ/Xje10XWQcZWklim6WBSlAtsYz/7GA64ZtWLiCKGau1sxHRFNqHXplFwI/vLLq6RVq/qX1dpdrVK/yOMowgmcwjn4cAV1uIUGNIHCEJ7hFd6QQC/oHX0sWgsonzmGP0CfP2cIjdU=</latexit>maxDUPLICATE 2 (less high)\n",
            "------------\n",
            "Given the new context, refine the original summary.\n",
            "If the context isn't useful, return the original summary.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
            "We have provided an existing summary up to a certain point: The white paper delves into the significance of neural network quantization in reducing computational costs for edge devices with power and compute constraints. It introduces advanced algorithms like Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) to mitigate quantization noise. The paper also delves into the hardware background of quantization, the advantages of fixed-point representations, and standard pipelines for achieving top-notch performance in common deep learning models and tasks. Additionally, it discusses practical considerations and debugging workflows for quantizing new models. The paper further explores symmetric and asymmetric uniform quantization schemes and the impact of quantization parameters on clipping and rounding errors. It delves into the details of choosing the right trade-off between these errors for optimal quantization performance.\n",
            "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
            "------------\n",
            "0127-128<latexit sha1_base64=\"usQ8n/DwTDCgaXu4jV5+gU9AQfU=\">AAACCXicbVC7SgNBFJ2Nrxhfq5Y2g0GIhWE3BE0ZsLGMYB6QhDA7mU2GzM4uM3clcdnWxl+xsVDE1j+w82+cPApNPHDhcM693HuPFwmuwXG+rcza+sbmVnY7t7O7t39gHx41dBgryuo0FKFqeUQzwSWrAwfBWpFiJPAEa3qj66nfvGdK81DewSRi3YAMJPc5JWCkno01LnQCAkPPT8ZpL+kAG0MScwmVNMUXD+c9O+8UnRnwKnEXJI8WqPXsr04/pHHAJFBBtG67TgTdhCjgVLA014k1iwgdkQFrGypJwHQ3mX2S4jOj9LEfKlMS8Ez9PZGQQOtJ4JnO6dF62ZuK/3ntGPxKN+EyioFJOl/kxwJDiKex4D5XjIKYGEKo4uZWTIdEEQomvJwJwV1+eZU0SkX3sli6Leer5UUcWXSCTlEBuegKVdENqqE6ougRPaNX9GY9WS/Wu/Uxb81Yi5lj9AfW5w+QIZo1</latexit>s(xuint8\u0000z)\n",
            "Figure 3: A visual explanation of the different uniform quantization grids for a bit-width of 8. sis the\n",
            "scaling factor, zthe zero-point. The ﬂoating-point grid is in black, the integer quantized grid in blue.\n",
            "2.2.2 Power-of-two quantizer\n",
            "Power-of-two quantization is a special case of symmetric quantization, in which the scale factor is\n",
            "restricted to a power-of-two, s= 2−k. This choice can bring hardware efﬁciencies because scaling\n",
            "withscorresponds to simple bit-shifting. However, the restricted expressiveness of the scale factor\n",
            "can complicate the trade-off between rounding and clipping error.\n",
            "2.2.3 Quantization granularity\n",
            "So far, we have deﬁned a single set of quantization parameters (quantizer) per tensor, one for the\n",
            "weights and one for activations, as seen in equation (3). This is called per-tensor quantization .\n",
            "We can also deﬁne a separate quantizer for individual segments of a tensor (e.g., output channels\n",
            "of a weight tensor), thus increasing the quantization granularity . In neural network quantization,\n",
            "per-tensor quantization is the the most common choice of granularity due to its simpler hardware\n",
            "implementation: all accumulators in equation (3)use the same scale factor, swsx. However, we\n",
            "could use ﬁner granularity to further improve performance. For example, for weight tensors, we can\n",
            "specify a different quantizer per output channel. This is known as per-channel quantization and its\n",
            "implications are discussed in more detailed in section 2.4.2.\n",
            "Other works go beyond per-channel quantization parameters and apply separate quantizers per group\n",
            "of weights or activations (Rouhani et al., 2020; Stock et al., 2019; Nascimento et al., 2019). Increasing\n",
            "the granularity of the groups generally improves accuracy at the cost of some extra overhead. The\n",
            "overhead is associated with accumulators handling sums of values with varying scale factors. Most\n",
            "existing ﬁxed-point accelerators do not currently support such logic and for this reason, we will not\n",
            "consider them in this work. However, as research in this area grows, more hardware support for these\n",
            "methods can be expected in the future.\n",
            "2.3 Quantization simulation\n",
            "To test how well a neural network would run on a quantized device, we often simulate the quantized\n",
            "behavior on the same general purpose hardware we use for training neural networks. This is called\n",
            "57Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample textQuantized inference on device\n",
            "ConvWeightsInputBiasesOutput\n",
            "int32\n",
            "int8Accumulatorint32RequantizationActivationint32int8\n",
            "int8ConvWeightsInputBiases+ActivationQuantizerQuantizerOutput(a) Diagram for quantized on-device inference with\n",
            "ﬁxed-point operations.\n",
            "5Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample textQuantized inference on device\n",
            "ConvWeightsInputBiasesOutput\n",
            "int32\n",
            "int8Accumulatorint32RequantizationActivationint32int8\n",
            "int8ConvWeightsInputBiases+ActivationQuantizerQuantizerOutput(b) Simulated quantization using ﬂoating-point\n",
            "operations.\n",
            "Figure 4: Schematic overview of quantized forward pass for convolutional layer: a) Compute graph\n",
            "of actual on-device quantized inference. b) Simulation of quantized inference for general-purpose\n",
            "ﬂoating-point hardware.\n",
            "quantization simulation . We aim to approximate ﬁxed-point operations using ﬂoating-point hard-\n",
            "ware. Such simulations are signiﬁcantly easier to implement compared to running experiments on\n",
            "actual quantized hardware or using quantized kernels. They allow the user to efﬁciently test various\n",
            "quantization options and it enables GPU acceleration for quantization-aware training as described\n",
            "in section 4. In this section, we ﬁrst explain the fundamentals of this simulation process and then\n",
            "discuss techniques that help to reduce the difference between the simulated and the actual on-device\n",
            "performance.\n",
            "Previously, we saw how matrix-vector multiplication is calculated in dedicated ﬁxed-point hardware.\n",
            "In ﬁgure 4a, we generalize this process for a convolutional layer, but we also include an activation\n",
            "function to make it more realistic. During on-device inference, all the inputs (biases, weight and input\n",
            "activations) to the hardware are in a ﬁxed-point format. However, when we simulate quantization\n",
            "using common deep learning frameworks and general-purpose hardware these quantities are in\n",
            "ﬂoating-point. This is why we introduce quantizer blocks in the compute graph to induce quantization\n",
            "effects.\n",
            "Figure 4b shows how the same convolutional layer is modelled in a deep-learning framework. Quan-\n",
            "tizer blocks are added in between the weights and the convolution to simulate weight quantization,\n",
            "and after the activation function to simulate activation quantization. The bias is often not quantized\n",
            "because it is stored in higher-precision. In section 2.3.2, we discuss in more detail when it is appropri-\n",
            "ate to position the quantizer after the non-linearity. The quantizer block implements the quantization\n",
            "function of equation (7)and each quantizer is deﬁned by a set of quantization parameters (scale factor,\n",
            "zero-point, bit-width). Both the input and output of the quantizer are in ﬂoating-point format but the\n",
            "output lies on the quantization grid.\n",
            "2.3.1 Batch normalization folding\n",
            "Batch normalization (Ioffe & Szegedy, 2015) is a standard component of modern convolutional\n",
            "networks. Batch normalization normalizes the output of a linear layer before scaling and adding an\n",
            "offset (see equation 9). For on-device inference, these operations are folded into the previous or next\n",
            "linear layers in a step called batch normalization folding (Krishnamoorthi, 2018; Jacob et al., 2018).\n",
            "This removes the batch normalization operations entirely from the network, as the calculations are\n",
            "absorbed into an adjacent linear layer. Besides reducing the computational overhead of the additional\n",
            "scaling and offset, this prevents extra data movement and the quantization of the layer’s output. More\n",
            "6formally, during inference, batch normalization is deﬁned as an afﬁne map of the output x:\n",
            "BatchNorm( x) =γ(x−µ√\n",
            "σ2+ϵ)\n",
            "+β (9)\n",
            "whereµandσare the mean and variance computed during training as exponential moving average\n",
            "over batch-statistics, and γandβare learned afﬁne hyper-parameters per-channel. If batch normal-\n",
            "ization is applied right after a linear layer y= BatchNorm( Wx), we can rewrite the terms such\n",
            "that the batch normalization operation is fused with the linear layer itself. Assuming a weight matrix\n",
            "W∈Rn×mwe apply batch normalization to each output ykfork={1,...,n}:\n",
            "yk= BatchNorm( Wk,:x)\n",
            "=γk(\n",
            "Wk,:x−µk√\n",
            "σ2\n",
            "k+ϵ)\n",
            "+βk\n",
            "=γkWk,:√\n",
            "σ2\n",
            "k+ϵx+(\n",
            "βk−γkµk√\n",
            "σ2\n",
            "k+ϵ)\n",
            "=˜Wk,:x+˜bk (10)\n",
            "where:\n",
            "˜Wk,:=γkWk,:√\n",
            "σ2\n",
            "k+ϵ, (11)\n",
            "˜bk=βk−γkµk√\n",
            "σ2\n",
            "k+ϵ. (12)\n",
            "2.3.2 Activation function fusing\n",
            "In our naive quantized accelerator introduced in section 2.1, we saw that the requantization of\n",
            "activations happens after the matrix multiplication or convolutional output values are calculated.\n",
            "However, in practice, we often have a non-linearity directly following the linear operation. It would\n",
            "be wasteful to write the linear layer’s activations to memory, and then load them back into a compute\n",
            "core to apply a non-linearity. For this reason, many hardware solutions come with a hardware unit that\n",
            "applies the non-linearity before the requantization step. If this is the case, we only have to simulate\n",
            "requantization that happens after the non-linearity. For example, ReLU non-linearities are readily\n",
            "modelled by the requantization block, as you can just set the minimum representable value of that\n",
            "activation quantization to 0.\n",
            "Other more complex activation functions, such as sigmoid or Swish (Ramachandran et al., 2017),\n",
            "require more dedicated support. If this support is not available, we need to add a quantization step\n",
            "before and after the non-linearity in the graph. This can have a big impact on the accuracy of\n",
            "quantized model. Although newer activations like Swish functions provide accuracy improvement in\n",
            "ﬂoating-point, these may vanish after quantization or may be less efﬁcient to deploy on ﬁxed-point\n",
            "hardware.\n",
            "2.3.3 Other layers and quantization\n",
            "There are many other types of layers being used in neural networks. How these are modeled depends\n",
            "greatly on the speciﬁc hardware implementation. Sometimes the mismatch between simulated\n",
            "quantization and on-target performance is down to layers not being properly quantized. Here, we\n",
            "provide some guidance on how to simulate quantization for a few commonly used layers:\n",
            "Max pooling Activation quantization is not required because the input and output values are on the\n",
            "same quantization grid.\n",
            "Average pooling The average of integers is not necessarily an integer. For this reason, a quantization\n",
            "step is required after average-pooling. However, we use the same quantizer for the inputs\n",
            "and outputs as the quantization range does not signiﬁcantly change.\n",
            "Element-wise addition Despite its simple nature, this operation is difﬁcult to simulate accurately.\n",
            "During addition, the quantization ranges of both inputs have to match exactly. If these ranges\n",
            "7do not match, extra care is needed to make addition work as intended. There is no single\n",
            "accepted solution for this but adding a requantization step can simulate the added noise\n",
            "coarsely. Another approach is to optimize the network by tying the quantization grids of the\n",
            "inputs. This would prevent the requantization step but may require ﬁne-tuning.\n",
            "------------\n",
            "Given the new context, refine the original summary.\n",
            "If the context isn't useful, return the original summary.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
            "We have provided an existing summary up to a certain point: The white paper explores the importance of neural network quantization in reducing computational costs for edge devices with power and compute constraints. It introduces advanced algorithms like Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) to address quantization noise. The paper delves into the hardware background of quantization, advantages of fixed-point representations, and standard pipelines for achieving optimal performance in deep learning models and tasks. It also discusses practical considerations and debugging workflows for quantizing new models, symmetric and asymmetric uniform quantization schemes, and the impact of quantization parameters on clipping and rounding errors. The paper further explores the trade-off between these errors for optimal quantization performance. Additionally, it discusses power-of-two quantization and quantization granularity, including per-channel quantization. The paper also covers quantization simulation techniques, batch normalization folding, activation function fusing, and simulating quantization for various types of layers in neural networks.\n",
            "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
            "------------\n",
            "Concatenation The two branches that are being concatenated generally do not share the same\n",
            "quantization parameters. This means that their quantization grids may not overlap making a\n",
            "requantization step necessary. As with element-wise addition, it is possible to optimize your\n",
            "network to have shared quantization parameters for the branches being concatenated.\n",
            "2.4 Practical considerations\n",
            "When quantizing neural networks with multiple layers, we are confronted with a large space of\n",
            "quantization choices including the quantization scheme, granularity, and bit-width. In this section, we\n",
            "explore some of the practical considerations that help reduce the search space.\n",
            "Note that in this white paper we only consider homogeneous bit-width. This means that the bit-width\n",
            "chosen for either weights or activations remains constant across all layers. Homogeneous bit-width is\n",
            "more universally supported by hardware but some recent works also explore the implementation of\n",
            "heterogeneous bit-width or mixed-precision (van Baalen et al., 2020; Dong et al., 2019; Uhlich et al.,\n",
            "2020).\n",
            "2.4.1 Symmetric vs. asymmetric quantization\n",
            "For each weight and activation quantization, we have to choose a quantization scheme. On one hand,\n",
            "asymmetric quantization is more expressive because there is an extra offset parameter, but on the\n",
            "other hand there is a possible computational overhead. To see why this is the case, consider what\n",
            "happens when asymmetric weights, ˆW=sw(Wint−zw), are multiplied with asymmetric activations\n",
            "ˆx=sx(xint−zx):\n",
            "ˆWˆx=sw(Wint−zw)sx(xint−zx)\n",
            "=swsxWintxint−swzwsxxint−swsxzxWint+swzwsxzx. (13)\n",
            "The ﬁrst term is what we would have if both operations were in symmetric format. The third and\n",
            "fourth terms depend only on the scale, offset and weight values, which are known in advance. Thus\n",
            "these two terms can be pre-computed and added to the bias term of a layer at virtually no cost. The\n",
            "second term, however, depends on the input data x. This means that for each batch of data we need to\n",
            "compute an additional term during inference. This can lead to signiﬁcant overhead in both latency\n",
            "and power, as it is equivalent to adding an extra channel.\n",
            "For this reason, it is a common approach to use asymmetric activation quantization andsymmetric\n",
            "weight quantization that avoids the additional data-dependent term.\n",
            "2.4.2 Per-tensor and per-channel quantization\n",
            "In section 2.2.3, we discussed different levels of quantization granularity. Per-tensor quantization\n",
            "of weights and activations has been standard for a while because it is supported by all ﬁxed-point\n",
            "accelerators. However, per-channel quantization of the weights can improve accuracy, especially\n",
            "when the distribution of weights varies signiﬁcantly from channel to channel. Looking back at the\n",
            "quantized MAC operation in equation (3), we can see that per-channel weight quantization can\n",
            "be implemented in the accelerator by applying a separate per-channel weight scale factor without\n",
            "requiring rescaling. Per-channel quantization of activations is much harder to implement because\n",
            "we cannot factor the scale factor out of the summation and would, therefore, require rescaling the\n",
            "accumulator for each input channel. Whereas per-channel quantization of weights is increasingly\n",
            "becoming common practice, not all commercial hardware supports it. Therefore, it is important to\n",
            "check if it is possible in your intended target device.\n",
            "3 Post-training quantization\n",
            "Post-training quantization (PTQ) algorithms take a pre-trained FP32 network and convert it directly\n",
            "into a ﬁxed-point network without the need for the original training pipeline. These methods can\n",
            "8Model (FP32 accuracy) ResNet18 (69.68) MobileNetV2 (71.72)\n",
            "Bit-width W8 W6 W4 W8 W6 W4\n",
            "Min-Max 69.57 63.90 0.12 71.16 64.48 0.59\n",
            "MSE 69.45 64.64 18.82 71.15 65.43 13.77\n",
            "Min-Max (Per-channel) 69.60 69.08 44.49 71.21 68.52 18.40\n",
            "MSE (Per-channel) 69.66 69.24 54.67 71.46 68.89 27.17\n",
            "Table 1: Ablation study for different methods of range setting of (symmetric uniform) weight\n",
            "quantizers while keeping the activations in FP32. Average ImageNet validation accuracy (%) over 5\n",
            "runs.\n",
            "be data-free or may require a small calibration set, which is often readily available. Additionally,\n",
            "having almost no hyperparameter tuning makes them usable via a single API call as a black-box\n",
            "method to quantize a pretrained neural network in a computationally efﬁcient manner. This frees the\n",
            "neural network designer from having to be an expert in quantization and thus allows for a much wider\n",
            "application of neural network quantization.\n",
            "A fundamental step in the PTQ process is ﬁnding good quantization ranges for each quantizer. We\n",
            "brieﬂy discussed in section 2.2 how the choice of quantization range affects the quantization error. In\n",
            "this section, we start by discussing various common methods used in practice to ﬁnd good quantization\n",
            "parameters. We then explore common issues observed during PTQ and introduce the most successful\n",
            "techniques to overcome them. Using these techniques we present a standard post-training quantization\n",
            "pipeline, which we ﬁnd to work best in most common scenarios and, ﬁnally, we introduce a set of\n",
            "debugging steps to improve the performance of the quantized model.\n",
            "3.1 Quantization range setting\n",
            "Quantization range setting refers to the method of determining clipping thresholds of the quantization\n",
            "grid,qminandqmax(see equation 7). The key trade-off in range setting is between clipping and\n",
            "rounding error, described in section 2.2, and their impact on the ﬁnal task loss for each quantizer\n",
            "being conﬁgured. Each of the methods described here provides a different trade-off between the two\n",
            "quantities. These methods typically optimize local cost functions instead of the task loss. This is\n",
            "because in PTQ we aim for computationally fast methods without the need for end-to-end training.\n",
            "Weights can usually be quantized without any need for calibration data. However, determining\n",
            "parameters for activation quantization often requires a few batches of calibration data.\n",
            "Min-max To cover the whole dynamic range of the tensor, we can deﬁne the quantization parame-\n",
            "ters as follows\n",
            "qmin= min V, (14)\n",
            "qmax= max V, (15)\n",
            "where Vdenotes the tensor to be quantized. This leads to no clipping error. However, this approach\n",
            "is sensitive to outliers as strong outliers may cause excessive rounding errors.\n",
            "Mean squared error (MSE) One way to alleviate the issue of large outliers is to use MSE-based\n",
            "range setting. In this range setting method we ﬁnd qminandqmaxthat minimize the MSE between the\n",
            "original and the quantized tensor:\n",
            "arg min\n",
            "qmin,qmaxV−ˆV(qmin,qmax)2\n",
            "F, (16)\n",
            "where ˆV(qmin,qmax)denotes the quantized version of Vand∥·∥Fis the Frobenius norm. The\n",
            "optimization problem is commonly solved using grid search, golden section method or analytical\n",
            "approximations with closed-form solution (Banner et al., 2019). Several variants of this range setting\n",
            "method exist in literature but they are all very similar in terms of objective function and optimization.\n",
            "9Model (FP32 accuracy) ResNet18 (69.68) MobileNetV2 (71.72)\n",
            "Bit-width A8 A6 A4 A8 A6 A4\n",
            "Min-Max 69.60 68.19 18.82 70.96 64.58 0.53\n",
            "MSE 69.59 67.84 31.40 71.35 67.55 13.57\n",
            "MSE + Xent 69.60 68.91 59.07 71.36 68.85 30.94\n",
            "BN (α= 6) 69.54 68.73 23.83 71.32 65.20 0.66\n",
            "Table 2: Ablation study for different methods of range setting of (asymmetric uniform) activation\n",
            "quantizers while keeping the weights in FP32. Average ImageNet validation accuracy (%) over 5\n",
            "runs.\n",
            "Cross entropy For certain layers, all values in the tensor being quantized may not be equally\n",
            "important. One such scenario is the quantization of logits in the last layer of classiﬁcation networks,\n",
            "in which it is important to preserve the order of the largest value after quantization. MSE may not\n",
            "be a suitable metric for this, as it weighs all the values in a tensor equally regardless of their order.\n",
            "For a larger number of classes, we usually have a large number of small or negative logits that are\n",
            "unimportant for prediction accuracy and few larger values that matter. In this case, MSE would incur a\n",
            "large quantization error to the few larger important logits while trying to reduce the quantization error\n",
            "of the more populous smaller logits. In this speciﬁc case, it is beneﬁcial to minimize the following\n",
            "cross-entropy loss function\n",
            "arg min\n",
            "qmin,qmaxH(ψ(v),ψ(ˆv(qmin,qmax))) (17)\n",
            "whereH(·,·)denotes the cross-entropy function, ψis the softmax function, and vis the logits vector.\n",
            "BN based range setting Range setting for activation quantizers often requires some calibration\n",
            "data. If a layer has batch-normalized activations, the per-channel mean and standard deviation of\n",
            "the activations are equal to the learned batch normalization shift and scale parameters, respectively.\n",
            "These can then be used to ﬁnd suitable parameters for activation quantizer as follows (Nagel et al.,\n",
            "2019):\n",
            "qmin= min ( β−αγ) (18)\n",
            "qmax= max ( β+αγ) (19)\n",
            "where βandγare vectors of per-channel learned shift and scale parameters, and α>0. Nagel et al.\n",
            "(2019) uses α= 6so that only large outliers are clipped.\n",
            "Comparison In table 1 we compare range setting methods for weight quantization. For high bit-\n",
            "widths, the MSE and min-max approaches are mostly on par. However, at lower bit-widths the MSE\n",
            "approach clearly outperforms the min-max. In table 2, we present a similar comparison for activation\n",
            "quantization. We note that MSE combined with cross-entropy for the last layer, denoted as MSE +\n",
            "Xent, outperforms other methods, especially at lower bit-widths. The table also clearly demonstrates\n",
            "the beneﬁt of using cross-entropy for the last layer instead of the MSE objective.\n",
            "3.2 Cross-Layer Equalization\n",
            "A common issue for quantization error is that elements in the same tensor can have signiﬁcantly\n",
            "different magnitudes. As discussed in the previous section, range setting for the quantization grid\n",
            "tries to ﬁnd a good trade-off between clipping and rounding error. Unfortunately, in some cases, the\n",
            "------------\n",
            "Given the new context, refine the original summary.\n",
            "If the context isn't useful, return the original summary.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
            "We have provided an existing summary up to a certain point: The white paper explores the importance of neural network quantization in reducing computational costs for edge devices with power and compute constraints. It introduces advanced algorithms like Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) to address quantization noise. The paper delves into the hardware background of quantization, advantages of fixed-point representations, and standard pipelines for achieving optimal performance in deep learning models and tasks. It also discusses practical considerations and debugging workflows for quantizing new models, symmetric and asymmetric uniform quantization schemes, and the impact of quantization parameters on clipping and rounding errors. The paper further explores the trade-off between these errors for optimal quantization performance, including per-channel quantization and quantization granularity. Additionally, it covers quantization simulation techniques, batch normalization folding, activation function fusing, and simulating quantization for various types of layers in neural networks. The paper also discusses Post-Training Quantization (PTQ) algorithms, their benefits, and the process of finding good quantization ranges for each quantizer. It explores common methods for setting quantization ranges, such as Min-Max, Mean Squared Error (MSE), Cross-Entropy, and BN based range setting, and compares their effectiveness for weight and activation quantization. The paper concludes with a discussion on Cross-Layer Equalization to address quantization errors caused by differences in magnitudes within the same tensor.\n",
            "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
            "------------\n",
            "difference in magnitude between them is so large that even for moderate quantization (e.g., INT8),\n",
            "we cannot ﬁnd a suitable trade-off. Nagel et al. (2019) showed that this is especially prevalent in\n",
            "depth-wise separable layers since only a few weights are responsible for each output feature and\n",
            "this might result in higher variability of the weights. Further, they noted that batch normalization\n",
            "folding adds to this effect and can result in a strong imbalance between weights connected to various\n",
            "output channels (see ﬁgure 5). While the latter is less of an issue for a more ﬁne-grained quantization\n",
            "granularity (e.g., per-channel quantization), this remains a big issue for the more widely used per-\n",
            "tensor quantization. Several papers (Krishnamoorthi, 2018; Nagel et al., 2019; Sheng et al., 2018b)\n",
            "noted that efﬁcient models with depth-wise separable convolutions, such as MobileNetV1 (Howard\n",
            "101234567891011121314151617181920212223242526272829303132\n",
            "Output channel index50\n",
            "25\n",
            "0255075100RangeWeight range per output channelFigure 5: Per (output) channel weight ranges of the ﬁrst depthwise-separable layer in MobileNetV2\n",
            "after BN folding. The boxplots show the min and max value, the 2nd and 3rd quartile and the median\n",
            "are plotted for each channel.\n",
            "12Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample text=<latexit sha1_base64=\"meYEYs0luY9YGmCX9DxDNv5g0Cs=\">AAAB+XicbVDLSsNAFL3xWesr6tLNYBHqpiRF1GXRjcsK9gFtLJPppB06mYSZSaGE/IkbF4q49U/c+TdO2iy09cDA4Zx7uWeOH3OmtON8W2vrG5tb26Wd8u7e/sGhfXTcVlEiCW2RiEey62NFORO0pZnmtBtLikOf044/ucv9zpRKxSLxqGcx9UI8EixgBGsjDWy7H2I99oO0kz2lVfciG9gVp+bMgVaJW5AKFGgO7K/+MCJJSIUmHCvVc51YeymWmhFOs3I/UTTGZIJHtGeowCFVXjpPnqFzowxREEnzhEZz9fdGikOlZqFvJvOcatnLxf+8XqKDGy9lIk40FWRxKEg40hHKa0BDJinRfGYIJpKZrIiMscREm7LKpgR3+curpF2vuVe1+sNlpXFb1FGCUziDKrhwDQ24hya0gMAUnuEV3qzUerHerY/F6JpV7JzAH1ifPwwyk0c=</latexit>W(1)<latexit sha1_base64=\"nKzQnz9bX9AwUJlw9oNMON3dZiY=\">AAAB+XicbVDLSsNAFL3xWesr6tLNYBHqpiRF1GXRjcsK9gFtLJPppB06mYSZSaGE/IkbF4q49U/c+TdO2iy09cDA4Zx7uWeOH3OmtON8W2vrG5tb26Wd8u7e/sGhfXTcVlEiCW2RiEey62NFORO0pZnmtBtLikOf044/ucv9zpRKxSLxqGcx9UI8EixgBGsjDWy7H2I99oO0kz2l1fpFNrArTs2ZA60StyAVKNAc2F/9YUSSkApNOFaq5zqx9lIsNSOcZuV+omiMyQSPaM9QgUOqvHSePEPnRhmiIJLmCY3m6u+NFIdKzULfTOY51bKXi/95vUQHN17KRJxoKsjiUJBwpCOU14CGTFKi+cwQTCQzWREZY4mJNmWVTQnu8pdXSbtec69q9YfLSuO2qKMEp3AGVXDhGhpwD01oAYEpPMMrvFmp9WK9Wx+L0TWr2DmBP7A+fwANuJNI</latexit>W(2)\n",
            "<latexit sha1_base64=\"21yzaa2R3mRHu9WKmzG0lV/IwqY=\">AAAB8nicbVBNS8NAEN3Ur1q/qh69LBbBU02KqMeiF48V7AekoWw2m3bpZjfsToQS+jO8eFDEq7/Gm//GbZuDtj4YeLw3w8y8MBXcgOt+O6W19Y3NrfJ2ZWd3b/+genjUMSrTlLWpEkr3QmKY4JK1gYNgvVQzkoSCdcPx3czvPjFtuJKPMElZkJCh5DGnBKzk92mkAHsXZsAH1Zpbd+fAq8QrSA0VaA2qX/1I0SxhEqggxviem0KQEw2cCjat9DPDUkLHZMh8SyVJmAny+clTfGaVCMdK25KA5+rviZwkxkyS0HYmBEZm2ZuJ/3l+BvFNkHOZZsAkXSyKM4FB4dn/OOKaURATSwjV3N6K6YhoQsGmVLEheMsvr5JOo+5d1RsPl7XmbRFHGZ2gU3SOPHSNmugetVAbUaTQM3pFbw44L86787FoLTnFzDH6A+fzB2f8kLA=</latexit>·1/si<latexit sha1_base64=\"iC1CO8RBStlRYCXn0LpsTNBqVaI=\">AAAB8XicbVBNS8NAEN3Ur1q/qh69LBbBU0mKqMeiF48V7Ae2oWw2m3bpZjfsToQS+i+8eFDEq//Gm//GTZuDtj4YeLw3w8y8IBHcgOt+O6W19Y3NrfJ2ZWd3b/+genjUMSrVlLWpEkr3AmKY4JK1gYNgvUQzEgeCdYPJbe53n5g2XMkHmCbMj8lI8ohTAlZ6HNBQATZDXhlWa27dnQOvEq8gNVSgNax+DUJF05hJoIIY0/fcBPyMaOBUsFllkBqWEDohI9a3VJKYGT+bXzzDZ1YJcaS0LQl4rv6eyEhszDQObGdMYGyWvVz8z+unEF37GZdJCkzSxaIoFRgUzt/HIdeMgphaQqjm9lZMx0QTCjakPARv+eVV0mnUvct64/6i1rwp4iijE3SKzpGHrlAT3aEWaiOKJHpGr+jNMc6L8+58LFpLTjFzjP7A+fwBvmOQUA==</latexit>·si\n",
            "<latexit sha1_base64=\"ATgUpwfGWoZqdP72T2bjWzFePXU=\">AAAB8XicbVDLSgMxFL3js9ZX1aWbYBFclZki6rLoxmUF+8C2lEx6pw3NZIYkI5ahf+HGhSJu/Rt3/o2ZdhbaeiBwOOdecu7xY8G1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZQohg0WiUi1fapRcIkNw43AdqyQhr7Alj++yfzWIyrNI3lvJjH2QjqUPOCMGis9dENqRn6QPk37pbJbcWcgy8TLSRly1Pulr+4gYkmI0jBBte54bmx6KVWGM4HTYjfRGFM2pkPsWCppiLqXzhJPyalVBiSIlH3SkJn6eyOlodaT0LeTWUK96GXif14nMcFVL+UyTgxKNv8oSAQxEcnOJwOukBkxsYQyxW1WwkZUUWZsSUVbgrd48jJpViveRaV6d16uXed1FOAYTuAMPLiEGtxCHRrAQMIzvMKbo50X5935mI+uOPnOEfyB8/kD/4ORIw==</latexit>x<latexit sha1_base64=\"4BSd+cda8Ysxy44cgNaLWSX+h7I=\">AAAB+XicbVDLSsNAFL3xWesr6tLNYBHqpiRF1GXRjcsK9gFtLJPppB06mYSZSSGE/okbF4q49U/c+TdO2iy09cDA4Zx7uWeOH3OmtON8W2vrG5tb26Wd8u7e/sGhfXTcVlEiCW2RiEey62NFORO0pZnmtBtLikOf044/ucv9zpRKxSLxqNOYeiEeCRYwgrWRBrbdD7Ee+0GWzp6yqnsxG9gVp+bMgVaJW5AKFGgO7K/+MCJJSIUmHCvVc51YexmWmhFOZ+V+omiMyQSPaM9QgUOqvGyefIbOjTJEQSTNExrN1d8bGQ6VSkPfTOY51bKXi/95vUQHN17GRJxoKsjiUJBwpCOU14CGTFKieWoIJpKZrIiMscREm7LKpgR3+curpF2vuVe1+sNlpXFb1FGCUziDKrhwDQ24hya0gMAUnuEV3qzMerHerY/F6JpV7JzAH1ifP0Cok2k=</latexit>y(1)<latexit sha1_base64=\"7PJEp3dNA0GEj7zWjIJF3sjCXEg=\">AAAB+XicbVDLSsNAFL3xWesr6tLNYBHqpiRF1GXRjcsK9gFtLJPppB06mYSZSSGE/okbF4q49U/c+TdO2iy09cDA4Zx7uWeOH3OmtON8W2vrG5tb26Wd8u7e/sGhfXTcVlEiCW2RiEey62NFORO0pZnmtBtLikOf044/ucv9zpRKxSLxqNOYeiEeCRYwgrWRBrbdD7Ee+0GWzp6yav1iNrArTs2ZA60StyAVKNAc2F/9YUSSkApNOFaq5zqx9jIsNSOczsr9RNEYkwke0Z6hAodUedk8+QydG2WIgkiaJzSaq783MhwqlYa+mcxzqmUvF//zeokObryMiTjRVJDFoSDhSEcorwENmaRE89QQTCQzWREZY4mJNmWVTQnu8pdXSbtec69q9YfLSuO2qKMEp3AGVXDhGhpwD01oAYEpPMMrvFmZ9WK9Wx+L0TWr2DmBP7A+fwBCLpNq</latexit>y(2)\n",
            "<latexit sha1_base64=\"tfEVQCQsfmYqAUfLb1yILPHpKNc=\">AAAB73icbVBNSwMxEJ2tX7V+VT16CRahXspuEfVY9OKxgv2AdinZbLYNzSZrkhXK0j/hxYMiXv073vw3pu0etPXBwOO9GWbmBQln2rjut1NYW9/Y3Cpul3Z29/YPyodHbS1TRWiLSC5VN8CaciZoyzDDaTdRFMcBp51gfDvzO09UaSbFg5kk1I/xULCIEWys1I2qfRJKcz4oV9yaOwdaJV5OKpCjOSh/9UNJ0pgKQzjWuue5ifEzrAwjnE5L/VTTBJMxHtKepQLHVPvZ/N4pOrNKiCKpbAmD5urviQzHWk/iwHbG2Iz0sjcT//N6qYmu/YyJJDVUkMWiKOXISDR7HoVMUWL4xBJMFLO3IjLCChNjIyrZELzll1dJu17zLmv1+4tK4yaPowgncApV8OAKGnAHTWgBAQ7P8ApvzqPz4rw7H4vWgpPPHMMfOJ8/ZViPjg==</latexit>f(·)<latexit sha1_base64=\"tfEVQCQsfmYqAUfLb1yILPHpKNc=\">AAAB73icbVBNSwMxEJ2tX7V+VT16CRahXspuEfVY9OKxgv2AdinZbLYNzSZrkhXK0j/hxYMiXv073vw3pu0etPXBwOO9GWbmBQln2rjut1NYW9/Y3Cpul3Z29/YPyodHbS1TRWiLSC5VN8CaciZoyzDDaTdRFMcBp51gfDvzO09UaSbFg5kk1I/xULCIEWys1I2qfRJKcz4oV9yaOwdaJV5OKpCjOSh/9UNJ0pgKQzjWuue5ifEzrAwjnE5L/VTTBJMxHtKepQLHVPvZ/N4pOrNKiCKpbAmD5urviQzHWk/iwHbG2Iz0sjcT//N6qYmu/YyJJDVUkMWiKOXISDR7HoVMUWL4xBJMFLO3IjLCChNjIyrZELzll1dJu17zLmv1+4tK4yaPowgncApV8OAKGnAHTWgBAQ7P8ApvzqPz4rw7H4vWgpPPHMMfOJ8/ZViPjg==</latexit>f(·)\n",
            "Figure 6: Illustration of the rescaling for a single channel. Scaling a channel in the ﬁrst layer by a\n",
            "factorsileads a reparametrization of the equivalent channel in the second layer by 1/si.\n",
            "et al., 2017) and MobileNetV2 (Sandler et al., 2018), show a signiﬁcant drop for PTQ or even result\n",
            "in random performance.\n",
            "A solution to overcome such imbalances without the need to use per-channel quantization is introduced\n",
            "by Nagel et al. (2019). A similar approach was introduced in concurrent work by Meller et al. (2019).\n",
            "In both papers, the authors observe that for many common activation functions (e.g., ReLU, PreLU),\n",
            "a positive scaling equivariance holds:\n",
            "f(sx) =sf(x) (20)\n",
            "for any non-negative real number s. This equivariance holds for any homogeneous function of degree\n",
            "one and can be extended to also hold for any piece-wise linear function by scaling its parameterization\n",
            "(e.g. ReLU6). We can exploit this positive scaling equivariance in consecutive layers in neural\n",
            "networks. Given two layers, h=f(W(1)x+b(1))andy=f(W(2)h+b(2)), through scaling\n",
            "equivariance we have that:\n",
            "y=f(W(2)f(W(1)x+b(1)) +b(2))\n",
            "=f(W(2)Sˆf(S−1W(1)x+S−1b(1)) +b(2))\n",
            "=f(˜W(2)ˆf(˜W(1)x+˜b(1)) +b(2)) (21)\n",
            "where S= diag( s)is a diagonal matrix with value Siidenoting the scaling factor sifor neuron\n",
            "i. This allows us to reparameterize our model with ˜W(2)=W(2)S,˜W(1)=S−1W(1)and\n",
            "˜b(1)=S−1b(1). In case of CNNs the scaling will be per-channel and broadcast accordingly over the\n",
            "spatial dimensions. We illustrate this rescaling procedure in ﬁgure 6.\n",
            "To make the model more robust to quantization, we can ﬁnd a scaling factor sisuch that the\n",
            "quantization noise in the rescaled layers is minimal. The cross-layer equalization (CLE) procedure\n",
            "11Model FP32 INT8\n",
            "Original model 71.72 0.12\n",
            "+ CLE 71.70 69.91\n",
            "+ absorbing bias 71.57 70.92\n",
            "Per-channel quantization 71.72 70.65\n",
            "Table 3: Impact of cross-layer equalization (CLE) for MobileNetV2 . ImageNet validation accuracy\n",
            "(%), evaluated at full precision and 8-bit quantization.\n",
            "(Nagel et al., 2019) achieves this by equalizing dynamic ranges across consecutive layers. They prove\n",
            "that an optimal weight equalization is achieved by setting Ssuch that:\n",
            "si=1\n",
            "r(2)\n",
            "i√\n",
            "r(1)\n",
            "ir(2)\n",
            "i (22)\n",
            "where r(j)\n",
            "iis the dynamic range of channel iof weight tensor j. The algorithm of Meller et al. (2019)\n",
            "introduces a similar scaling factor that also takes the intermediate activation tensor into account.\n",
            "However, they do not have a proof of optimality for this approach.\n",
            "Absorbing high biases Nagel et al. (2019) further notice that in some cases, especially after CLE,\n",
            "high biases can lead to differences in the dynamic ranges of the activations. Therefore, they propose a\n",
            "procedure to, if possible, absorb high biases into the next layer. To absorb cfrom layer one (followed\n",
            "by a ReLU activation function f) into layer two, we can do the following reparameterization:\n",
            "y=W(2)h+b(2)\n",
            "=W(2)(f(W(1)x+b(1)) +c−c) +b(2)\n",
            "=W(2)(f(W(1)x+˜b(1)) +c) +b(2)\n",
            "=W(2)˜h+˜b(2)(23)\n",
            "where˜b(2)=W(2)c+b(2),˜h=h−c, and˜b(1)=b(1)−c. In step two, we use the fact that\n",
            "for a layer with ReLU function f, there is a non-negative vector csuch thatr(Wx+b−c) =\n",
            "r(Wx+b)−c. The trivial solution c=0holds for all x. However, depending on the distribution\n",
            "ofxand the values of Wandb, there can be some values ci>0for which this equality holds for\n",
            "(almost) all xin the empirical distribution. This value is equal to\n",
            "ci= max(\n",
            "0,min\n",
            "x(\n",
            "W(1)\n",
            "ix+b(1)\n",
            "i))\n",
            ". (24)\n",
            "where minxis evaluated on a small calibration dataset. To remove dependence on data, the authors\n",
            "propose to estimate the right hand side of (24) by the shift and scale parameters of the batch\n",
            "normalization layer which results3inci= max(0,βi−3γi).\n",
            "Experiments In table 3, we demonstrate the effect of CLE and bias absorption for quantiz-\n",
            "ingMobileNetV2 to 8-bit. As skip connections break the equivariance between layers, we apply\n",
            "------------\n",
            "Given the new context, refine the original summary.\n",
            "If the context isn't useful, return the original summary.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
            "We have provided an existing summary up to a certain point: The white paper delves into the importance of neural network quantization in reducing computational costs for edge devices with power and compute constraints. It introduces advanced algorithms like Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) to address quantization noise. The paper also discusses hardware background, advantages of fixed-point representations, standard pipelines for optimal performance in deep learning models, and practical considerations for quantizing new models. Furthermore, it explores symmetric and asymmetric uniform quantization schemes, quantization granularity, and simulation techniques. The paper concludes with a discussion on Cross-Layer Equalization to address quantization errors caused by differences in magnitudes within the same tensor, introducing a solution to overcome imbalances without per-channel quantization through positive scaling equivariance and cross-layer equalization. Additionally, it covers the impact of bias absorption on model robustness to quantization, demonstrating the effectiveness of these techniques on MobileNetV2 for 8-bit quantization.\n",
            "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
            "------------\n",
            "cross-layer equalization only to the layers within each residual block. Similar to Krishnamoorthi\n",
            "(2018), we observe that model performance is close to random when quantizing MobileNetV2 to\n",
            "INT8. Applying CLE brings us back within 2% of FP32 performance, close to the performance of\n",
            "per-channel quantization. We note that absorbing high biases results in a small drop in FP32 perfor-\n",
            "mance, as it is an approximation, but it boosts quantized performance by 1% due to more precise\n",
            "activation quantization. Together, CLE and bias absorption followed by per-tensor quantization yield\n",
            "better results than per-channel quantization.\n",
            "3.3 Bias correction\n",
            "Another common issue is that quantization error is often biased. This means that the expected output\n",
            "of the original and quantized layer or network is shifted ( E[Wx]̸=E[\n",
            "ˆWx]\n",
            "). This kind of error is\n",
            "3Assuming xis normally distributed, the equality will hold for approximately 99.865% of the inputs.\n",
            "12more pronounced in depth-wise separable layers with only a few elements per output channel (usually\n",
            "9 for a 3×3kernel). The main contributor to this error is often the clipping error, as a few strongly\n",
            "clipped outliers will likely lead to a shift in the expected distribution.\n",
            "Several papers (Nagel et al., 2019; Meller et al., 2019; Finkelstein et al., 2019) noted this issue and\n",
            "introduce methods to correct for the expected shift in distribution. For a quantized layer ˆWwith\n",
            "quantization error ∆W=ˆW−W, the expected output distribution is\n",
            "E[ˆy] =E[\n",
            "ˆWx]\n",
            "=E[(W+ ∆W)x]\n",
            "=E[Wx] +E[∆Wx]. (25)\n",
            "Thus the biased error is given by E[∆Wx]. Since ∆Wis constant, we have that E[∆Wx] =\n",
            "∆WE[x]. In case ∆WE[x]is nonzero, the output distribution is shifted. To counteract this shift\n",
            "we can substract it from the output:\n",
            "E[ycorr] =E[\n",
            "ˆWx]\n",
            "−∆WE[x] =E[y]. (26)\n",
            "Note, this correction term is a vector with the same shape as the bias and can thus be absorbed into\n",
            "the bias without any additional overhead at inference time. There are several ways of calculating the\n",
            "bias correction term, the two most common of which are empirical bias correction andanalytic bias\n",
            "correction .\n",
            "Empirical bias correction If we have access to a calibration dataset the bias correction term can\n",
            "simply be calculated by comparing the activations of the quantized and full precision model. In\n",
            "practice, this can be done layer-wise by computing\n",
            "∆WE[x] =E[\n",
            "ˆWx]\n",
            "−E[Wx]. (27)\n",
            "Analytic bias correction Nagel et al. (2019) introduce a method to analytically calculate the biased\n",
            "error, without the need for data. For common networks with batch normalization and ReLU functions,\n",
            "they use the BN statistics of the preceding layer in order to compute the expected input distribution\n",
            "E[x]. The BN parameters γandβcorrespond to the mean and standard deviation of the BN layers\n",
            "output. Assuming input values are normally distributied, the effect of ReLU on the distribution can\n",
            "be modeled using the clipped normal distribution. They show that\n",
            "E[x] =E[ReLU (xpre)]\n",
            "=γN(−β\n",
            "γ)\n",
            "+β[\n",
            "1−Φ(−β\n",
            "γ)]\n",
            "(28)\n",
            "where xpreis the pre-activation output, which is assumed to be normally distributed with the per-\n",
            "channel means βand per-channel standard deviations γ,Φ(·)is the standard normal CDF, and the\n",
            "notationN(x)is used to denote the standard normal PDF. Note, all vector operations are element-\n",
            "wise (per-channel) operations. After calculating the input distribution E[x], the correction term can\n",
            "be simply derived by multiplying it with the weight quantization error ∆W.\n",
            "Experiments In table 4, we demonstrate the effect of bias correction for quantizing MobileNetV2\n",
            "to 8-bit. Applying analytical bias correction improves quantized model performance from random to\n",
            "over 50%, indicating that the biased error introduced by quantization signiﬁcantly harms model per-\n",
            "formance. When combining bias correction with CLE, we see that both techniques are complementary.\n",
            "Together, they achieve near FP32 performance without using any data.\n",
            "3.4 AdaRound\n",
            "Neural network weights are usually quantized by projecting each FP32 value to the nearest quantiza-\n",
            "tion grid point, as indicated by ⌊·⌉in equation (4)for a uniform quantization grid. We refer to this\n",
            "quantization strategy as rounding-to-nearest. The rounding-to-nearest strategy is motivated by the fact\n",
            "that, for a ﬁxed quantization grid, it yields the lowest MSE between the ﬂoating-point and quantized\n",
            "weights. However, Nagel et al. (2020) showed that rounding-to-nearest is not optimal in terms of the\n",
            "13Model FP32 INT8\n",
            "Original Model 71.72 0.12\n",
            "+ bias correction 71.72 52.02\n",
            "CLE + bias absorption 71.57 70.92\n",
            "+ bias correction 71.57 71.19\n",
            "Table 4: Impact of bias correction for MobileNetV2 . ImageNet validation accuracy (%) evaluated at\n",
            "full precision and 8-bit quantization.\n",
            "0.5 1.0 1.5 2.0 2.5 3.0\n",
            "1\n",
            "2wTH(w)w\n",
            "35404550556065Accuracy (%)\n",
            "Stochastic rounding\n",
            "Nearest rounding\n",
            "Figure 7: Correlation between the cost in equation (30) vs. ImageNet validation accuracy (%) of 100\n",
            "stochastic rounding vectors ˆwfor4-bit quantization of only the ﬁrst layer of ResNet18.\n",
            "task loss when quantizing weights in the post-training regime. To illustrate this the authors quantized\n",
            "the weights of the ﬁrst layer of ResNet18 to 4 bits using 100 different stochastic rounding samples\n",
            "(Gupta et al., 2015) and evaluated the performance of the network for each rounding choice. The\n",
            "best rounding choice among these outperformed rounding-to-nearest by more than 10%. Figure 7\n",
            "illustrates this by plotting the performance of these rounding choices on the y-axis. In this section,\n",
            "we describe AdaRound (Nagel et al., 2020), a systematic approach to ﬁnding good weight rounding\n",
            "choices for PTQ. AdaRound is a theoretically well-founded and computationally efﬁcient method\n",
            "that shows signiﬁcant performance improvement in practice.\n",
            "As the main goal is to minimize the impact of quantization on the ﬁnal task loss, we start by\n",
            "formulating the optimization problem in terms of this loss\n",
            "arg min\n",
            "∆wE[L(x,y,w+ ∆w)−L(x,y,w)] (29)\n",
            "where ∆wdenotes the perturbation due to quantization and can take two possible values for each\n",
            "weight, one by rounding the weight up and the other by rounding the weight down. We want to\n",
            "solve this binary optimization problem efﬁciently. As a ﬁrst step, we approximate the cost function\n",
            "using a second-order Taylor series expansion. This alleviates the need for performance evaluation for\n",
            "each new rounding choice during the optimization. We further assume that the model has converged,\n",
            "implying that the contribution of the gradient term in the approximation can be ignored, and that\n",
            "the Hessian is block-diagonal, which ignores cross-layer correlations. This leads to the following\n",
            "Hessian based quadratic unconstrained binary optimization (QUBO) problem\n",
            "arg min\n",
            "∆w(ℓ)E[\n",
            "∆w(ℓ)TH(w(ℓ))∆w(ℓ)]\n",
            "(30)\n",
            "The clear correlation in ﬁgure 7 between the validation accuracy and objective of equation (30)\n",
            "indicates that the latter serves as a good proxy for the task loss (equation 29), even for 4-bit weight\n",
            "quantization. Despite the performance gains (see table 5), equation (30) cannot be widely applied for\n",
            "weight rounding for main two reasons:\n",
            "•The memory and computational complexity of calculating the Hessian is impractical for\n",
            "general use-cases.\n",
            "14Rounding First layer All layers\n",
            "Nearest 52.29 23.99\n",
            "H(w)task loss (equation 30) 68.62 N/A\n",
            "Cont. relaxation MSE (equation 32) 69.58 66.56\n",
            "AdaRound (equation 35) 69.58 68.60\n",
            "Table 5: Impact of various approximations and assumptions made in section 3.4 on the ImageNet\n",
            "validation accuracy (%) for ResNet18 averaged over 5 runs. N/A implies that the corresponding\n",
            "experiment was computationally infeasible.\n",
            "• The QUBO problem of equation (30) is NP-Hard.\n",
            "To tackle the ﬁrst problem, the authors introduced additional suitable assumptions that allow simpli-\n",
            "fying the objective of equation (30) to the following local optimization problem that minimizes the\n",
            "MSE of the output activations for a layer.\n",
            "arg min\n",
            "∆W(ℓ)\n",
            "k,:E[(\n",
            "∆W(ℓ)\n",
            "k,:x(ℓ−1))2]\n",
            "(31)\n",
            "Equation (31) requires neither the computation of the Hessian nor any other backward or forward\n",
            "propagation information from the subsequent layers. Note that the approximations and the analysis\n",
            "that have been used to link the QUBO problem of equation (30) with the local optimization problem\n",
            "of equation (31) is independent of the rounding problem. Hence this analysis also beneﬁts the design\n",
            "of algorithms for other problems, including model compression and NAS (Moons et al., 2020).\n",
            "The optimization of (31) is still an NP-hard optimization problem. To ﬁnd a good approximate\n",
            "solution with reasonable computational complexity, the authors relax the optimization problem to the\n",
            "following continuous optimization problem\n",
            "arg min\n",
            "VWx−˜Wx2\n",
            "F+λfreg(V), (32)\n",
            "where∥·∥2\n",
            "Fdenotes the Frobenius norm and ˜Ware the soft-quantized weights deﬁned as\n",
            "˜W=s·clamp(⌊W\n",
            "s⌋\n",
            "+h(V) ;n,p)\n",
            ". (33)\n",
            "We usenandpto denote integer grid limits, n=qmin/sandp=qmax/s.Vi,jis the continuous\n",
            "variable that we optimize over and hcan be any monotonic function with values between 0and1,\n",
            "i.e.,h(Vi,j)∈[0,1]. In Nagel et al. (2020), the authors use a rectiﬁed sigmoid as h. The objective of\n",
            "(32) also introduces a regularizer term that encourages the continuous optimization variables h(Vi,j)\n",
            "to converge to either 0or1, so that they are valid solutions to the discrete optimization in (31). The\n",
            "regularizer used in Nagel et al. (2020) is\n",
            "freg(V) =∑\n",
            "i,j1−|2h(Vi,j)−1|β, (34)\n",
            "whereβis annealed during the course of optimization to initially allow free movement of h(Vi,j)\n",
            "and later to force them to converge to 0or1. To avoid error accumulation across layers of the neural\n",
            "network and to account for the non-linearity, the authors propose the following ﬁnal optimization\n",
            "problem\n",
            "arg min\n",
            "Vfa(Wx)−fa(\n",
            "˜Wˆ x)2\n",
            "F+λfreg(V), (35)\n",
            "where ˆ xis the layer’s input with all preceding layers quantized and fais the activation function. The\n",
            "objective of (35) can be effectively and efﬁciently optimized using stochastic gradient descent. This\n",
            "approach of optimizing weight rounding is known as AdaRound.\n",
            "------------\n",
            "Given the new context, refine the original summary.\n",
            "If the context isn't useful, return the original summary.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
            "We have provided an existing summary up to a certain point: The white paper explores the significance of neural network quantization in reducing computational costs for edge devices with power and compute constraints. It introduces advanced algorithms like Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) to address quantization noise and discusses hardware background, advantages of fixed-point representations, standard pipelines for optimal performance in deep learning models, and practical considerations for quantizing new models. The paper also delves into symmetric and asymmetric uniform quantization schemes, quantization granularity, and simulation techniques. Furthermore, it explores Cross-Layer Equalization (CLE) to address quantization errors caused by differences in magnitudes within the same tensor and introduces a solution to overcome imbalances without per-channel quantization. Additionally, it covers the impact of bias absorption on model robustness to quantization, demonstrating the effectiveness of these techniques on MobileNetV2 for 8-bit quantization. The paper then discusses bias correction methods to address biased quantization errors and introduces AdaRound, a systematic approach to finding optimal weight rounding choices for Post-Training Quantization (PTQ) to minimize the impact on task loss. Through experiments and analysis, the paper demonstrates the effectiveness of bias correction, Cross-Layer Equalization (CLE), and AdaRound in improving model performance during quantization.\n",
            "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
            "------------\n",
            "To summarize, the way we round weights during the quantization operation has a signiﬁcant impact\n",
            "on the performance of the network. AdaRound provides a theoretically sound, computationally fast\n",
            "weight rounding method. It requires only a small amount of unlabeled data samples, no hyperparame-\n",
            "ter tuning or end-to-end ﬁnetuning, and can be applied to fully connected and convolutional layers of\n",
            "any neural network.\n",
            "1513Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample textPTQ Pipeline\n",
            "Pre-trained FP Model\n",
            "Add Quantizers\n",
            "CLE\n",
            "Weight Range SettingYesNo\n",
            "Use Data\n",
            "Symmetric Weights\n",
            "Asymmetric Activations\n",
            "MSE Range Setting\n",
            "MSE Range Setting\n",
            "Bias Correction\n",
            "AdaRound\n",
            "Activation Range SettingFigure 8: Standard PTQ pipeline. Blue boxes represent required steps and the turquoise boxes\n",
            "recommended choices.\n",
            "3.5 Standard PTQ pipeline\n",
            "In this section, we present a best-practice pipeline for PTQ based on relevant literature and extensive\n",
            "experimentation. We illustrate the recommended pipeline in ﬁgure 8. This pipeline achieves competi-\n",
            "tive PTQ results for many computer vision as well as natural language processing models and tasks.\n",
            "Depending on the model, some steps might not be required, or other choices could lead to equal or\n",
            "better performance.\n",
            "Cross-layer equalization First we apply cross-layer equalization (CLE), which is a pre-processing\n",
            "step for the full precision model to make it more quantization friendly. CLE is particularly\n",
            "important for models with depth-wise separable layers and for per-tensor quantization, but it\n",
            "often also shows improvements for other layers and quantization choices.\n",
            "Add quantizers Next we choose our quantizers and add quantization operations in our network as\n",
            "described in section 2.3. The choice of quantizer might depend on the speciﬁc target HW;\n",
            "for common AI accelerators we recommend using symmetric quantizers for the weights\n",
            "and asymmetric quantizers for the activations. If supported by the HW/SW stack then it is\n",
            "favorable to use per-channel quantization for weights.\n",
            "Weight range setting To set the quantization parameters of all weight tensors we recommend using\n",
            "the layer-wise MSE based criteria. In the speciﬁc case of per-channel quantization, using\n",
            "the min-max method can be favorable in some cases.\n",
            "AdaRound In case we have a small calibration dataset4available we next apply AdaRound in\n",
            "order to optimize the rounding of the weights. This step is crucial to enable low-bit weight\n",
            "quantization (e.g. 4 bits) in the PTQ.\n",
            "Bias correction In case we do not have such a calibration dataset and the network uses batch\n",
            "normalization, we can use analytical bias correction instead.\n",
            "Activation range setting As the ﬁnal step, we determine the quantization ranges of all data-\n",
            "dependent tensors in the network (i.e., activations). We use the MSE based criteria for\n",
            "most of the layers, which requires a small calibration set to ﬁnd the minimum MSE loss.\n",
            "Alternatively, we can use the BN based range setting to have a fully data-free pipeline.\n",
            "3.6 Experiments\n",
            "We now evaluate the performance of the aforementioned PTQ pipeline on common computer vision\n",
            "and natural language understanding applications. Our results are summarized in table 6. For the\n",
            "task of semantic segmentation, we evaluate DeepLabV3 (with a MobileNetV2 backbone) (Chen\n",
            "et al., 2017) on Pascal VOC and for object detection, EfﬁcientDet (Tan et al., 2020) on COCO 2017.\n",
            "The rest of the computer vision models are evaluated on the ImageNet classiﬁcation benchmark.\n",
            "For natural language understanding, we evaluate BERT-base on the GLUE benchmark (Wang et al.,\n",
            "2018).\n",
            "In all cases, we observe that 8-bit quantization of weights and activation (W8A8) leads to only\n",
            "marginal loss of accuracy compared to ﬂoating-point (within 0.7%) for all models. For W8A8\n",
            "4Usually, between 500 and 1000 unlabeled images are sufﬁcient as a calibration set.\n",
            "16Per-tensor Per-channel\n",
            "Models FP32 W8A8 W4A8 W8A8 W4A8\n",
            "ResNet18 69.68 69.60 68.62 69.56 68.91\n",
            "ResNet50 76.07 75.87 75.15 75.88 75.43\n",
            "MobileNetV2 71.72 70.99 69.21 71.16 69.79\n",
            "InceptionV3 77.40 77.68 76.48 77.71 76.82\n",
            "EfﬁcientNet lite 75.42 75.25 71.24 75.39 74.01\n",
            "DeeplabV3 72.94 72.44 70.80 72.27 71.67\n",
            "EfﬁcientDet-D1 40.08 38.29 0.31 38.67 35.08\n",
            "BERT-base†83.06 82.43 81.76 82.77 82.02\n",
            "Table 6: Performance (average over 5 runs) of our standard PTQ pipeline for various models and tasks.\n",
            "DeeplabV3 (MobileNetV2 backbone) is evaluated on Pascal VOC (mean intersection over union),\n",
            "EfﬁcientDet-D1 on COCO 2017 (mean average precision), BERT-base on the GLUE benchmark\n",
            "and other models on ImageNet (accuracy). We evaluate all models on the respective validation sets.\n",
            "Higher is better in all cases.†A few quantized activations are kept in higher precision (16 bits).\n",
            "quantization we also see no signiﬁcant gains from using per-channel quantization. However, the\n",
            "picture changes when weights are quantized to 4 bits (W4A8). For ResNet18/50 and InceptionV3 the\n",
            "accuracy drop is still within 1% of ﬂoating-point for both per-tensor and per-channel quantization.\n",
            "However, for more efﬁcient networks, such as MobileNetV2 and EfﬁcientNet lite, the drop increases\n",
            "to 2.5% and 4.2% respectively for per-tensor quantization. This is likely due to the quantization of\n",
            "the depth-wise separable convolutions. Here, per-channel quantization can show a signiﬁcant beneﬁt,\n",
            "for example, in EfﬁcientNet lite per-channel quantization increases the accuracy by 2.8% compared\n",
            "to per-tensor quantization, bringing it within 1.4% of full-precision accuracy. We see similar effects\n",
            "for EfﬁcientDet-D1 and DeeplabV3 which both uses depth-wise separable convolutions in their\n",
            "backbone.\n",
            "For BERT-base, we observe that a few activation tensors have extreme differences in their dynamic\n",
            "ranges. To make PTQ still work, we identiﬁed these layers using our debugging procedure outlined\n",
            "in section 3.7 and kept them in 16 bit. Otherwise BERT-base follows similar trends as most other\n",
            "models and our PTQ pipeline allows 4 bit weight quantization within 1.5% drop in GLUE score.\n",
            "3.7 Debugging\n",
            "We showed that the standard PTQ pipeline can achieve competitive results for a wide range of models\n",
            "and networks. However, if after following the steps of our pipeline, the model’s performance is still\n",
            "not satisfactory, we recommend a set of diagnostics steps to identify the bottlenecks and improve\n",
            "the performance. While this is not strictly an algorithm, these debugging steps can provide insights\n",
            "on why a quantized model underperforms and help to tackle the underlying issues. These steps are\n",
            "shown as a ﬂow chart in ﬁgure 9 and are described in more detail below:\n",
            "FP32 sanity check An important initial debugging step is to ensure that the ﬂoating-point and quan-\n",
            "tized model behave similarly in the forward pass, especially when using custom quantization\n",
            "pipelines. Set the quantized model bit-width to 32 bits for both weights and activation, or\n",
            "by-pass the quantization operation, if possible, and check that the accuracy matches that of\n",
            "the FP32 model.\n",
            "Weights or activations quantization The next debugging step is to identify how activation or\n",
            "weight quantization impact the performance independently. Does performance recover\n",
            "if all weights are quantized to a higher bit-width while activations are kept in a lower bit-\n",
            "width, or conversely if all activations use a high bit-width and activations a low bit-width?\n",
            "This step can show the relative contribution of activations and weight quantization to the\n",
            "overall performance drop and point us towards the appropriate solution.\n",
            "Fixing weight quantization If the previous step shows that weight quantization does cause signiﬁ-\n",
            "cant accuracy drop, then there are a few solutions to try:\n",
            "•Apply CLE if not already implemented, especially for models with depth-wise separable\n",
            "convolutions.\n",
            "179Quantized ModelSet !!=!\"=32Fix ModelYESNOerror ≈0for &∈{Activations,Weights}Quantize all tensors in &error <+YESFix problemRange SettingAdjust CLEAdaRoundPer channelCLEBias correctionActivationsWeightserror <+YESNOfor layer l=0,…,0Set !!=!\"=32error <+Visualize tensor 1($)Fix problem in 1($)error <+NO\n",
            "NOYES\n",
            "YESQATNOQuantize tensor 1($)\n",
            "Final Quantized ModelENDEND\n",
            "Quantize Model\n",
            "error <+YESNOFigure 9: PTQ debugging ﬂow chart. Error is the difference between ﬂoating-point and quantized\n",
            "model accuracy.\n",
            "18•Try per-channel quantization. This will address the issue of uneven per-channel weight\n",
            "distribution.\n",
            "• Apply bias correction or AdaRound if calibration data is available.\n",
            "Fixing activation quantization To reduce the quantization error from activation quantization, we\n",
            "can also try using different range setting methods or adjust CLE to take activation quantiza-\n",
            "tion ranges into account, as vanilla CLE can lead to uneven activation distribution.\n",
            "Per-layer analysis If the global solutions have not restored accuracy to acceptable levels, we consider\n",
            "each quantizer individually. We set each quantizer sequentially, to the target bit-width while\n",
            "keeping the rest of the network to 32 bits (see inner for loop in ﬁgure 9).\n",
            "Visualizing layers If the quantization of a individual tensor leads to signiﬁcant accuracy drop, we\n",
            "recommended visualizing the tensor distribution at different granularities, e.g. per-channel\n",
            "as in ﬁgure 5, and dimensions, e.g., per-token or per-embedding for activations in BERT.\n",
            "Fixing individual quantizers The visualization step can reveal the source of the tensor’s sensitivity\n",
            "to quantization. Some common solutions involve custom range setting for this quantizer or\n",
            "allowing a higher bit-width for problematic quantizer, e.g., BERT-base from table 6. If the\n",
            "problem is ﬁxed and the accuracy recovers, we continue to the next quantizer. If not, we\n",
            "may have to resort to other methods, such as quantization-aware training (QAT), which is\n",
            "discussed in section 4.\n",
            "After completing the above steps, the last step is to quantize the complete model to the desired\n",
            "------------\n",
            "Given the new context, refine the original summary.\n",
            "If the context isn't useful, return the original summary.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
            "We have provided an existing summary up to a certain point: The white paper delves into the significance of neural network quantization for edge devices with power and compute constraints, introducing advanced algorithms like Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) to address quantization noise. It explores hardware background, advantages of fixed-point representations, standard pipelines for optimal performance in deep learning models, and practical considerations for quantizing new models. The paper delves into symmetric and asymmetric uniform quantization schemes, quantization granularity, and simulation techniques. Additionally, it discusses Cross-Layer Equalization (CLE) to address quantization errors and introduces solutions like bias correction and AdaRound for improving model robustness to quantization. Through experiments and analysis, the paper demonstrates the effectiveness of bias correction, CLE, and AdaRound in improving model performance during quantization. The paper also presents a standard PTQ pipeline and evaluates its performance on various computer vision and natural language understanding applications, showing that 8-bit quantization of weights and activation leads to only marginal loss of accuracy compared to floating-point for all models. The paper concludes by discussing debugging steps to identify and improve the performance of quantized models.\n",
            "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
            "------------\n",
            "bit-width. If the accuracy is acceptable, we have our ﬁnal quantized model ready to use. Otherwise,\n",
            "we can consider higher bit-widths and smaller granularities or revert to more powerful quantization\n",
            "methods, such as quantization-aware training.\n",
            "4 Quantization-aware training\n",
            "The post-training quantization techniques described in the previous section are the ﬁrst go-to tool in\n",
            "our quantization toolkit. They are very effective and fast to implement because they do not require\n",
            "retraining of the network with labeled data. However, they have limitations, especially when aiming\n",
            "for low-bit quantization of activations, such as 4-bit and below. Post-training techniques may not\n",
            "be enough to mitigate the large quantization error incurred by low-bit quantization. In these cases,\n",
            "we resort to quantization-aware training (QAT). QAT models the quantization noise source (see\n",
            "section 2.3) during training. This allows the model to ﬁnd more optimal solutions than post-training\n",
            "quantization. However, the higher accuracy comes with the usual costs of neural network training,\n",
            "i.e., longer training times, need for labeled data and hyper-parameter search.\n",
            "In this section, we explore how back-propagation works in networks with simulated quantization\n",
            "and provide a standard pipeline for training models with QAT effectively. We will also discuss the\n",
            "implications of batch normalization folding and per-channel quantization in QAT and provide results\n",
            "for a wide range of tasks and models.\n",
            "4.1 Simulating quantization for backward path\n",
            "In section 2.3, we saw how quantization can be simulated using ﬂoating-point in deep learning\n",
            "frameworks. However, if we look at the computational graph of ﬁgure 4, to train such a network\n",
            "we need to back-propagate through the simulated quantizer block. This poses an issue because the\n",
            "gradient of the round-to-nearest operation in equation (4)is either zero or undeﬁned everywhere,\n",
            "which makes gradient-based training impossible. A way around this would be to approximate the\n",
            "gradient using the straight-through estimator (STE, Bengio et al. 2013), which approximates the\n",
            "gradient of the rounding operator as 1:\n",
            "∂⌊y⌉\n",
            "∂y= 1 (36)\n",
            "Using this approximation we can now calculate the gradient of the quantization operation from\n",
            "equation (7). For clarity we assume symmetric quantization, namely z= 0, but the same result\n",
            "applies to asymmetric quantization since the zero-point is a constant. We use nandpto deﬁne the\n",
            "integer grid limits, such that n=qmin/sandp=qmax/s. The gradient of equation (7) w.r.t its input,\n",
            "19xi, is given by:\n",
            "∂ˆxi\n",
            "∂xi=∂q(xi)\n",
            "∂xi\n",
            "=s·∂\n",
            "∂xiclamp(⌊xi\n",
            "s⌉\n",
            ";n,p)\n",
            "+ 0\n",
            "=\n",
            "\n",
            "s·∂⌊xi/s⌉\n",
            "∂(xi/s)∂(xi/s)\n",
            "∂xiifqmin≤xi≤qmax,\n",
            "s·∂n\n",
            "∂xiifxi<q min,\n",
            "s·∂p\n",
            "∂xiifxi>q max,\n",
            "={1ifqmin≤xi≤qmax,\n",
            "0otherwise.(37)\n",
            "Using this gradient deﬁnition we can now back-propagate through the quantization blocks. Figure 10\n",
            "shows a simple computational graph for the forward and backward pass used in quantization-aware\n",
            "training. The forward pass is identical to that of ﬁgure 4, but in the backward pass we effectively\n",
            "skip the quantizer block due to the STE assumption. In earlier QAT work the quantization ranges\n",
            "8Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample textQAT Graph\n",
            "ConvWeightsInputBiases+ActivationQuantizerQuantizer\n",
            "#$$%&%BackwardForward\n",
            "Figure 10: Forward and backward computation graph for quantization aware training with STE\n",
            "assumption.\n",
            "for weights and activations were updated at each iteration most commonly using the min-max range\n",
            "(Krishnamoorthi, 2018). In later work (Esser et al., 2020; Jain et al., 2019; Bhalgat et al., 2020), the\n",
            "STE is used to calculate the gradient w.r.t. the quantization parameters, zands. Using the chain rule\n",
            "and the STE, we ﬁrst calculate the gradient w.r.t. the scale-factor:\n",
            "∂ˆxi\n",
            "∂s=∂\n",
            "∂s[\n",
            "s·clamp(⌊xi\n",
            "s⌉\n",
            ";n,p)]\n",
            "=\n",
            "\n",
            "−xi/s+⌊xi/s⌉ifqmin≤xi≤qmax,\n",
            "n ifxi<q min,\n",
            "p ifxi>q max.(38)\n",
            "Originally, we restricted the zero-point to be an integer. To make zero-point learnable we convert into\n",
            "a real number and apply the rounding operator. The modiﬁed quantization function is deﬁned as:\n",
            "ˆx=q(x;s,z) =s·[\n",
            "clamp(⌊x\n",
            "s⌉\n",
            "+⌊z⌉;n,p)\n",
            "−⌊z⌉]\n",
            "(39)\n",
            "The gradient w.r.t. tozis calculated by applying the STE once again to the rounding operator:\n",
            "∂ˆxi\n",
            "∂z={0qmin≤xi≤qmax,\n",
            "−sotherwise.(40)\n",
            "20Model (FP32 accuracy) ResNet18 (69.68) MobileNetV2 (71.72)\n",
            "Bit-width W4A8 W4A4 W4A8 W4A4\n",
            "Static folding BN 69.76 68.32 70.17 66.43\n",
            "Double forward (Krishnamoorthi, 2018) 69.42 68.20 66.87 63.54\n",
            "Static folding (per-channel) 69.58 68.15 70.52 66.32\n",
            "Keep original BN (per-channel) 70.01 68.83 70.48 66.89\n",
            "Table 7: Ablation study with various ways to include BN into QAT. The learning rate is individually\n",
            "optimized for each conﬁguration. Average ImageNet validation accuracy (%) over 3 runs.\n",
            "4.2 Batch normalization folding and QAT\n",
            "In section 2.3.1, we introduced batch normalization folding that absorbs the scaling and addition\n",
            "into a linear layer to allow for more efﬁcient inference. During quantization-aware training, we want\n",
            "to simulate inference behavior closely, which is why we have to account for BN-folding during\n",
            "training. Note that in some QAT literature, the BN-folding effect is ignored. While this is ﬁne when\n",
            "we employ per-channel quantization (more below in this section), keeping BN unfolded for per-tensor\n",
            "quantization will result in one of the two following cases:\n",
            "1.The BN layer applies per-channel rescaling during inference. In this case we might as well\n",
            "use per-channel quantization in the ﬁrst place.\n",
            "2.We fold BN during deployment into the weight tensor and incur potentially signiﬁcant\n",
            "accuracy drop as we trained the network to adapt to a different quantization noise.\n",
            "A simple but effective approach to modeling BN-folding in QAT is to statically fold the BN scale and\n",
            "offset into the linear layer’s weights and bias, as we saw in equations (11) and(12). This corresponds\n",
            "to re-parametrization of the weights and effectively removes the batch normalization operation\n",
            "from the network entirely. When starting from a converged pre-trained model, static folding is very\n",
            "effective, as we can see from the result of table 7.\n",
            "An alternative approach by Jacob et al. (2018) both updates the running statistics during QAT and\n",
            "applies BN-folding using a correction. This approach is more cumbersome and computationally\n",
            "costly because it involves a double forward pass : one for the batch-statistics and one for the quantized\n",
            "linear operation. However, based on our experiments (see table 7), static-folding performs on par or\n",
            "better despite its simplicity.\n",
            "Per-channel quantization In section 2.4.2, we mentioned that per-channel quantization of the\n",
            "weights can improve accuracy when it is supported by hardware. The static folding re-parametrization\n",
            "is also valid for per-channel quantization. However, per-channel quantization provides additional\n",
            "ﬂexibility as it allows us to absorb the batch normalization scaling operation into the per-channel\n",
            "scale-factor. Let us see how this is possible by revisiting the BN folding equation from section 2.3.1,\n",
            "but this time introduce per-channel quantization of the weights, such that ˆWk,:=q(Wk,:;sw,k) =\n",
            "sw,kWint\n",
            "k,:. By applying batch normalization to the output of a linear layer similar to equation (10),\n",
            "we get:\n",
            "ˆyk= BatchNorm(\n",
            "ˆWk,:x)\n",
            "=γkˆWk,:√\n",
            "σ2\n",
            "k+ϵx+(\n",
            "βk−γkµk√\n",
            "σ2\n",
            "k+ϵ)\n",
            "=γksw,k√\n",
            "σ2\n",
            "k+ϵWint\n",
            "k,:x+˜bk\n",
            "=˜sw,k(\n",
            "Wint\n",
            "k,:x)\n",
            "+˜bk (41)\n",
            "We can see that it is now possible to absorb the batch normalization scaling parameters into the\n",
            "per-channel scale-factor. For QAT, this means that we can keep the BN layer intact during training\n",
            "and merge the BN scaling factor into the per-channel quantization parameters afterward. In practice,\n",
            "21Model (FP32 accuracy) ResNet18 (69.68) MobileNetV2 (71.72)\n",
            "PTQ QAT PTQ QAT\n",
            "W4A8 w/ min-max weight init 0.12 69.61 0.56 69.96\n",
            "W4A8 w/ MSE weight init 18.58 69.76 12.99 70.13\n",
            "W4A4 w/ min-max act init 7.51 68.23 0.22 66.55\n",
            "W4A4 w/ MSE act init 9.62 68.41 0.71 66.29\n",
            "Table 8: Ablation study for various ways to initialize the quantization grid. The learning rate is\n",
            "individually optimized for each conﬁguration. ImageNet validation accuracy (%) averaged over 3\n",
            "runs.\n",
            "this modeling approach is on par or better for per-channel quantization compared to static folding as\n",
            "we can see from the last two rows of table 7.\n",
            "4.3 Initialization for QAT\n",
            "In this section, we will explore the effect of initialization for QAT. It is common practice in literature\n",
            "to start from a pre-trained FP32 model (Esser et al., 2020; Krishnamoorthi, 2018; Jacob et al., 2018;\n",
            "Jain et al., 2019). While it is clear that starting from an FP32 model is beneﬁcial, the effect of the\n",
            "quantization initialization on the ﬁnal QAT result is less studied. Here we explore the effect of using\n",
            "several of our PTQ techniques as an initial step before doing QAT.\n",
            "Effect of range estimation To assess the effect of the initial range setting (see section 3.1) for\n",
            "weights and activations, we perform two sets of experiments, which are summarized in table 8. In the\n",
            "ﬁrst experiment, we quantize the weights to 4-bits and keep the activations in 8-bits. We compare the\n",
            "min-max initialization with the MSE based initialization for the weights quantization range. While\n",
            "the MSE initialized model has a signiﬁcantly higher starting accuracy, the gap closes after training\n",
            "for 20 epochs.\n",
            "To explore the same effect for activation quantization, we perform a similar experiment, where we now\n",
            "quantize the activation to 4-bits and compare min-max initialization with MSE based initialization.\n",
            "The observations from weight range initialization hold here as well. In ﬁgure 11 we show the full\n",
            "training curve of this experiment. In the ﬁrst few epochs, there is a signiﬁcant advantage for using\n",
            "MSE initialization, which almost vanishes in the later stage of training. In conclusion, a better\n",
            "initialization can lead to better QAT results, but the gain is usually small and vanishes the longer the\n",
            "training lasts.\n",
            "0 2 4 6 8 10 12 14 16 18 20\n",
            "------------\n",
            "Given the new context, refine the original summary.\n",
            "If the context isn't useful, return the original summary.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
            "We have provided an existing summary up to a certain point: The white paper delves into the significance of neural network quantization for edge devices with power and compute constraints, introducing advanced algorithms like Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) to address quantization noise. It explores hardware background, advantages of fixed-point representations, standard pipelines for optimal performance in deep learning models, and practical considerations for quantizing new models. The paper delves into symmetric and asymmetric uniform quantization schemes, quantization granularity, and simulation techniques. Additionally, it discusses Cross-Layer Equalization (CLE) to address quantization errors and introduces solutions like bias correction and AdaRound for improving model robustness to quantization. Through experiments and analysis, the paper demonstrates the effectiveness of bias correction, CLE, and AdaRound in improving model performance during quantization. The paper also presents a standard PTQ pipeline and evaluates its performance on various computer vision and natural language understanding applications, showing that 8-bit quantization of weights and activation leads to only marginal loss of accuracy compared to floating-point for all models. Furthermore, the paper explores quantization-aware training (QAT) as a more advanced technique for low-bit quantization, providing insights into back-propagation with simulated quantization and discussing batch normalization folding and per-channel quantization in QAT. The paper concludes by examining the impact of initialization on QAT results, showing that better initialization can lead to improved performance but the gain diminishes over time.\n",
            "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
            "------------\n",
            "Epochs6263646566676869Accuracy (%)\n",
            "W4A4 w/ min-max acts\n",
            "W4A4 w/ MSE acts\n",
            "Figure 11: Inﬂuence of the initial activation range setting on the QAT training behavior of ResNet18 .\n",
            "Average ImageNet validation accuracy (%) after each training epoch over 3 runs (and standard\n",
            "deviation shaded).\n",
            "22Model (FP32 accuracy) ResNet18 (69.68) MobileNetV2 (71.72)\n",
            "PTQ QAT PTQ QAT\n",
            "W4A8 baseline 18.58 69.74 0.10 0.10\n",
            "W4A8 w/ CLE 16.29 69.76 12.99 70.13\n",
            "W4A8 w/ CLE + BC 38.58 69.72 46.90 70.07\n",
            "Table 9: Ablation study with various PTQ initialization. The learning rate is individually optimized\n",
            "for each conﬁguration. ImageNet validation accuracy (%) averaged over 3 runs.\n",
            "Effect of CLE In table 9 we compare the effect of other PTQ improvements such as CLE and\n",
            "bias correction. While for ResNet18 we do not see a signiﬁcant difference in the ﬁnal QAT perfor-\n",
            "mance, for MobileNetV2 we observe that it cannot be trained without CLE. This is likely due to the\n",
            "catastrophic performance drop caused by per-tensor quantization, which we discussed in section 3.2.\n",
            "In conclusion, for models that have severe issues with plain PTQ we may need advanced PTQ\n",
            "techniques such as CLE to initialize QAT. In most other cases, an improved PTQ initialization leads\n",
            "only to a minor improvement in the ﬁnal QAT performance.\n",
            "4.4 Standard QAT pipeline\n",
            "In this section, we present a best-practice pipeline for QAT based on relevant literature and extensive\n",
            "experimentation. We illustrate the recommended pipeline in ﬁgure 12. This pipeline yields good QAT\n",
            "results over a variety of computer vision and natural language processing models and tasks, and can\n",
            "be seen as the go-to tool for achieving low-bit quantization performance. As discussed in previous\n",
            "sections, we always start from a pre-trained model and follow some PTQ steps in order to have faster\n",
            "convergence and higher accuracy.\n",
            "Cross-layer equalization Similar to PTQ, we ﬁrst apply CLE to the full precision model. As\n",
            "we saw in table 9, this step is necessary for models that suffer from imbalanced weight\n",
            "distributions, such as MobileNet architectures. For other networks or in the case of per-\n",
            "channel quantization this step can be optional.\n",
            "Add quantizers Next, we choose our quantizers and add quantization operations in our network as\n",
            "described in section 2.3. The choice for quantizer might depend on the speciﬁc target HW,\n",
            "for common AI accelerators we recommend using symmetric quantizers for the weights\n",
            "and asymmetric quantizers for the activations. If supported by the HW/SW stack, then it is\n",
            "favorable to use per-channel quantization for weights. At this stage we will also take care\n",
            "that our simulation of batch normalization is correct, as discussed in section 4.2.\n",
            "Range estimation Before training we have to initialize all quantization parameters. A better ini-\n",
            "tialization will help faster training and might improve the ﬁnal accuracy, though often\n",
            "the improvement is small (see table 8). In general, we recommend to set all quantization\n",
            "parameters using the layer-wise MSE based criteria. In the speciﬁc case of per-channel\n",
            "quantization, using the min-max setting can sometimes be favorable.\n",
            "Learnable Quantization Parameters We recommend making the quantizer paramaters learnable,\n",
            "as discussed in section 4.1. Learning the quantization parameters directly, rather than\n",
            "updating them at every epoch, leads to higher performance especially when dealing with\n",
            "low-bit quantization. However, using learnable quantizers requires special care when setting\n",
            "up the optimizer for the task. When using SGD-type optimizers, the learning rate for the\n",
            "quantization parameters needs to be reduced compared to the rest of the network parameters.\n",
            "The learning rate adjustment can be avoided if we use optimizers with adaptive learning\n",
            "rates such as Adam or RMSProp.\n",
            "4.5 Experiments\n",
            "Using our QAT pipeline, we quantize and evaluate the same models we used for PTQ in section\n",
            "3.6. Our results are presented in table 10 for different bit-widths and quantization granularities.\n",
            "DeepLabV3 is trained for 80 epochs on Pascal VOC; EfﬁcientDet for 20 epochs on COCO 2017;\n",
            "all other vision models are trained for 20 epochs on ImageNet. BERT-base is trained on each of the\n",
            "2314Confidential and Proprietary —Qualcomm Technologies, Inc. and/or its affiliated companies.Source sample textQAT Pipeline\n",
            "Pre-trained FP model\n",
            "Add Quantizers\n",
            "Range Estimation\n",
            "CLE\n",
            "Learnable Quantization Params\n",
            "Train\n",
            "Symmetric Weights\n",
            "Asymmetric Activations\n",
            "MSE Range Setting\n",
            "Figure 12: Standard quantization-aware training pipeline. The blue boxes represent the steps and the\n",
            "turquoise boxes recommended choices.\n",
            "Per-tensor Per-channel\n",
            "Models FP32 W8A8 W4A8 W4A4 W8A8 W4A8 W4A4\n",
            "ResNet18 69.68 70.38 69.76 68.32 70.43 70.01 68.83\n",
            "ResNet50 76.07 76.21 75.89 75.10 76.58 76.52 75.53\n",
            "InceptionV3 77.40 78.33 77.84 77.49 78.45 78.12 77.74\n",
            "MobileNetV2 71.72 71.76 70.17 66.43 71.82 70.48 66.89\n",
            "EfﬁcientNet lite 75.42 75.17 71.55 70.22 74.75 73.92 71.55\n",
            "DeeplabV3 72.94 73.99 70.90 66.78 72.87 73.01 68.90\n",
            "EfﬁcientDet-D1 40.08 38.94 35.34 24.70 38.97 36.75 28.68\n",
            "BERT-base 83.06 83.26 82.64 78.83 82.44 82.39 77.63\n",
            "Table 10: Performance (average over 3 runs) of our standard QAT pipeline for various models\n",
            "and tasks. DeeplabV3 ( MobileNetV2 backbone) is evaluated on Pascal VOC (mean intersection\n",
            "over union), EfﬁcientDet-D1 on COCO 2017 (mean average precision), BERT-base on the GLUE\n",
            "benchmark and all other models on ImageNet (accuracy). We evaluate all models on the respective\n",
            "validation sets. Higher is better in all cases.\n",
            "corresponding GLUE tasks for 3 to 12 epochs depending on the task and the quantization granularity.\n",
            "We use the Adam optimizer for all models. We present the results with the best learning rate per\n",
            "quantization conﬁguration and perform no further hyper-parameter tuning.\n",
            "We observe that for networks without depth-wise separable convolutions (ﬁrst 3 rows of table 10),\n",
            "W8A8 and W4A8 quantization perform on par with and even outperform the ﬂoating-point model in\n",
            "certain cases. This could be due to the regularizing effect of training with quantization noise or due to\n",
            "the additional ﬁne-tuning during QAT. For the more aggressive W4A4 case, we notice a small drop\n",
            "but still within 1% of the ﬂoating-point accuracy.\n",
            "Quantizing networks with depth-wise separable layers ( MobileNetV2 , EfﬁcientNet lite, DeeplabV3,\n",
            "EfﬁcientDet-D1) is more challenging; a trend we also observed from the PTQ results in section 3.6\n",
            "and discussed in the literature (Chin et al., 2020; Sheng et al., 2018a). Whereas 8-bit quantization\n",
            "incurs close to no accuracy drop, quantizing weights to 4 bits leads to a larger drop, e.g. approximately\n",
            "4% drop for EfﬁcientNet lite with per-tensor quantization. Per-channel quantization can improve\n",
            "performance signiﬁcantly bringing DeepLabV3 to ﬂoating-point accuracy and reducing the gap of\n",
            "MobileNetV2 and EfﬁcientNet lite to less than 1.5%. Quantizing both weights and activations to\n",
            "4-bits remains a challenging for such networks, even with per-channel quantization it can lead to a\n",
            "drop of up to 5%. EfﬁcientDet-D1 remains more difﬁcult to quantize than the other networks in this\n",
            "group.\n",
            "For BERT-base we observe that QAT with range learning can efﬁciently deal with the high dynamic\n",
            "ranges allowing to keep all activations in 8 bits (unlike for PTQ). W4A8 stays within 1% of the\n",
            "original GLUE score, indicating that low bit weight quantization is not a problem for transformer\n",
            "models. We only notice a signiﬁcant drop in performance when combining this with low bit activation\n",
            "quantization (W4A4).\n",
            "245 Summary and Conclusions\n",
            "Deep learning has become an integral part of many machine learning applications and can now\n",
            "be found in countless electronic devices and services, from smartphones and home appliances to\n",
            "drones, robots and self-driving cars. As the popularity and reach of deep learning in our everyday life\n",
            "increases, so does the need for fast and power-efﬁcient neural network inference. Neural network\n",
            "quantization is one of the most effective ways of reducing the energy and latency requirements of\n",
            "neural networks during inference.\n",
            "Quantization allows us to move from ﬂoating-point representations to a ﬁxed-point format and, in\n",
            "combination with dedicated hardware utilizing efﬁcient ﬁxed-point operations, has the potential\n",
            "to achieve signiﬁcant power gains and accelerate inference. However, to exploit these savings, we\n",
            "require robust quantization methods that can maintain high accuracy, while reducing the bit-width\n",
            "of weights and activations. To this end, we consider two main classes of quantization algorithms:\n",
            "Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT).\n",
            "Post-training quantization techniques take a pre-trained FP32 networks and convert it into a ﬁxed-\n",
            "point network without the need for the original training pipeline. This makes them a lightweight,\n",
            "push-button approach to quantization with low engineering effort and computational cost. We describe\n",
            "a series of recent advances in PTQ and introduce a PTQ pipeline that leads to near ﬂoating-point\n",
            "accuracy results for a wide range of models and machine learning tasks. In particular, using the\n",
            "proposed pipeline we can achieve 8-bit quantization of weights and activations within only 1% of\n",
            "the ﬂoating-point accuracy for all networks. We further show that many networks can be quantized\n",
            "even to 4-bit weights with only a small additional drop in performance. In addition, we introduce a\n",
            "debugging workﬂow to effectively identify and ﬁx problems that might occur when quantizing new\n",
            "networks.\n",
            "Quantization-aware training models the quantization noise during training through simulated quanti-\n",
            "zation operations. This training procedure allows for better solutions to be found compared to PTQ\n",
            "while enabling more effective and aggressive activation quantization. Similar to PTQ, we introduce a\n",
            "standard training pipeline utilizing the latest algorithms in the ﬁeld. We also pay special attention to\n",
            "------------\n",
            "Given the new context, refine the original summary.\n",
            "If the context isn't useful, return the original summary.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
            "We have provided an existing summary up to a certain point: The white paper explores the significance of neural network quantization for edge devices with power and compute constraints, introducing advanced algorithms like Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) to address quantization noise. It delves into hardware background, advantages of fixed-point representations, standard pipelines for optimal performance in deep learning models, and practical considerations for quantizing new models. The paper discusses symmetric and asymmetric uniform quantization schemes, quantization granularity, and simulation techniques. It also explores Cross-Layer Equalization (CLE) to address quantization errors and introduces solutions like bias correction and AdaRound for improving model robustness to quantization. Through experiments and analysis, the paper demonstrates the effectiveness of bias correction, CLE, and AdaRound in improving model performance during quantization. Additionally, it presents a standard PTQ pipeline and evaluates its performance on various computer vision and natural language understanding applications, showing that 8-bit quantization of weights and activation leads to only marginal loss of accuracy compared to floating-point for all models. The paper further explores quantization-aware training (QAT) as a more advanced technique for low-bit quantization, providing insights into back-propagation with simulated quantization and discussing batch normalization folding and per-channel quantization in QAT. The paper concludes by examining the impact of initialization on QAT results, showing that better initialization can lead to improved performance but the gain diminishes over time. The paper also presents a best-practice pipeline for QAT, highlighting the importance of cross-layer equalization, adding quantizers, range estimation, learnable quantization parameters, and training. The experiments show the performance of the standard QAT pipeline on various models and tasks, demonstrating the effectiveness of quantization in reducing energy and latency requirements while maintaining high accuracy.\n",
            "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
            "------------\n",
            "batch normalization folding during QAT and show that simple static folding outperforms other more\n",
            "computationally expensive approaches. We demonstrate that with our QAT pipeline we can achieve\n",
            "4-bit quantization of weights, and for some models even 4-bit activations, with only a small drop of\n",
            "accuracy compared to ﬂoating-point.\n",
            "The choice between PTQ and QAT depends on the accuracy and power requirements of the application.\n",
            "Both approaches are an essential part of any model efﬁciency toolkit and we hope that our proposed\n",
            "pipelines will help engineers deploy high-performing quantized models with less time and effort.\n",
            "References\n",
            "Banner, R., Nahshan, Y ., and Soudry, D. Post training 4-bit quantization of convolutional networks\n",
            "for rapid-deployment. Neural Information Processing Systems (NeuRIPS) , 2019. 9\n",
            "Bengio, Y ., Léonard, N., and Courville, A. Estimating or propagating gradients through stochastic\n",
            "neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013. 19\n",
            "Bhalgat, Y ., Lee, J., Nagel, M., Blankevoort, T., and Kwak, N. Lsq+: Improving low-bit quantization\n",
            "through learnable offsets and better initialization. In Proceedings of the IEEE/CVF Conference on\n",
            "Computer Vision and Pattern Recognition (CVPR) Workshops , June 2020. 20\n",
            "Chen, L.-C., Papandreou, G., Schroff, F., and Adam, H. Rethinking atrous convolution for semantic\n",
            "image segmentation, 2017. 16\n",
            "Chin, T.-W., Chuang, P. I.-J., Chandra, V ., and Marculescu, D. One weight bitwidth to rule them\n",
            "all. In Bartoli, A. and Fusiello, A. (eds.), Computer Vision – ECCV 2020 Workshops , pp. 85–103,\n",
            "Cham, 2020. Springer International Publishing. ISBN 978-3-030-68238-5. 24\n",
            "Dong, Z., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. HAWQ: hessian aware quantization\n",
            "of neural networks with mixed-precision. International Conference on Computer Vision (ICCV) ,\n",
            "2019. 8\n",
            "25Esser, S. K., McKinstry, J. L., Bablani, D., Appuswamy, R., and Modha, D. S. Learned step size\n",
            "quantization. International Conference on Learning Representations (ICLR) , 2020. 20, 22\n",
            "Finkelstein, A., Almog, U., and Grobman, M. Fighting quantization bias with bias. arXiv preprint\n",
            "arxiv:1906.03193 , 2019. 13\n",
            "Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. Deep learning with limited numerical\n",
            "precision. International Conference on Machine Learning, ICML , 2015. 14\n",
            "Horowitz, M. 1.1 computing’s energy problem (and what we can do about it). In 2014 IEEE\n",
            "International Solid-State Circuits Conference Digest of Technical Papers (ISSCC) , pp. 10–14, 2014.\n",
            "doi: 10.1109/ISSCC.2014.6757323. 2, 3\n",
            "Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and\n",
            "Adam, H. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications.\n",
            "arXiv preprint arXiv:1704.04861 , 2017. 10\n",
            "Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal\n",
            "covariate shift. In Bach, F. and Blei, D. (eds.), Proceedings of the 32nd International Conference on\n",
            "Machine Learning , volume 37 of Proceedings of Machine Learning Research , pp. 448–456, Lille,\n",
            "France, 07–09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/ioffe15.html .\n",
            "6\n",
            "Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and Kalenichenko,\n",
            "D. Quantization and training of neural networks for efﬁcient integer-arithmetic-only inference.\n",
            "Conference on Computer Vision and Pattern Recognition (CVPR) , 2018. 3, 6, 21, 22\n",
            "Jain, S. R., Gural, A., Wu, M., and Dick, C. Trained uniform quantization for accurate and efﬁcient\n",
            "neural network inference on ﬁxed-point hardware. CoRR , abs/1903.08066, 2019. URL http:\n",
            "//arxiv.org/abs/1903.08066 . 20, 22\n",
            "Krishnamoorthi, R. Quantizing deep convolutional networks for efﬁcient inference: A whitepaper.\n",
            "arXiv preprint arXiv:1806.08342 , 2018. 6, 10, 12, 20, 21, 22\n",
            "Meller, E., Finkelstein, A., Almog, U., and Grobman, M. Same, same but different: Recovering neural\n",
            "network quantization error through weight factorization. In Proceedings of the 36th International\n",
            "Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , pp.\n",
            "4486–4495, 2019. URL http://proceedings.mlr.press/v97/meller19a.html . 11, 12, 13\n",
            "Moons, B., Noorzad, P., Skliar, A., Mariani, G., Mehta, D., Lott, C., and Blankevoort, T. Distilling\n",
            "optimal neural networks: Rapid search in diverse spaces. arXiv preprint arXiv:2012.08859 , 2020.\n",
            "15\n",
            "Nagel, M., van Baalen, M., Blankevoort, T., and Welling, M. Data-free quantization through weight\n",
            "equalization and bias correction. International Conference on Computer Vision (ICCV) , 2019. 10,\n",
            "11, 12, 13\n",
            "Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. Up or down? Adaptive\n",
            "rounding for post-training quantization. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th\n",
            "International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning\n",
            "Research , pp. 7197–7206. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/\n",
            "v119/nagel20a.html . 13, 14, 15\n",
            "Nascimento, M. G. d., Fawcett, R., and Prisacariu, V . A. Dsconv: Efﬁcient convolution operator.\n",
            "InProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , October\n",
            "2019. 5\n",
            "Ramachandran, P., Zoph, B., and Le, Q. V . Searching for activation functions. CoRR , abs/1710.05941,\n",
            "2017. URL http://arxiv.org/abs/1710.05941 . 7\n",
            "Rouhani, B., Lo, D., Zhao, R., Liu, M., Fowers, J., Ovtcharov, K., Vinogradsky, A., Massengill, S.,\n",
            "Yang, L., Bittner, R., Forin, A., Zhu, H., Na, T., Patel, P., Che, S., Koppaka, L. C., Song, X., Som,\n",
            "S., Das, K., Tiwary, S., Reinhardt, S., Lanka, S., Chung, E., and Burger, D. Pushing the limits of\n",
            "narrow precision inferencing at cloud scale with microsoft ﬂoating point. In Neural Information\n",
            "Processing Systems (NeurIPS 2020) . ACM, November 2020. 5\n",
            "26Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. Mobilenetv2: Inverted residuals\n",
            "and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern\n",
            "recognition , pp. 4510–4520, 2018. 11\n",
            "Sheng, T., Feng, C., Zhuo, S., Zhang, X., Shen, L., and Aleksic, M. A quantization-friendly separable\n",
            "convolution for mobilenets. 2018 1st Workshop on Energy Efﬁcient Machine Learning and\n",
            "Cognitive Computing for Embedded Applications (EMC2) , Mar 2018a. doi: 10.1109/emc2.2018.\n",
            "00011. URL http://dx.doi.org/10.1109/EMC2.2018.00011 . 24\n",
            "Sheng, T., Feng, C., Zhuo, S., Zhang, X., Shen, L., and Aleksic, M. A quantization-friendly separable\n",
            "convolution for mobilenets. In 1st Workshop on Energy Efﬁcient Machine Learning and Cognitive\n",
            "Computing for Embedded Applications (EMC2) , 2018b. URL https://ieeexplore.ieee.org/\n",
            "abstract/document/8524017 . 10\n",
            "Stock, P., Joulin, A., Gribonval, R., Graham, B., and Jégou, H. And the bit goes down: Revisiting the\n",
            "quantization of neural networks. CoRR , abs/1907.05686, 2019. URL http://arxiv.org/abs/\n",
            "1907.05686 . 5\n",
            "Tan, M., Pang, R., and Le, Q. V . Efﬁcientdet: Scalable and efﬁcient object detection, 2020. 16\n",
            "Uhlich, S., Mauch, L., Cardinaux, F., Yoshiyama, K., Garcia, J. A., Tiedemann, S., Kemp, T., and\n",
            "Nakamura, A. Mixed precision dnns: All you need is a good parametrization. In International\n",
            "Conference on Learning Representations , 2020. URL https://openreview.net/forum?id=\n",
            "Hyx0slrFvH . 8\n",
            "van Baalen, M., Louizos, C., Nagel, M., Amjad, R. A., Wang, Y ., Blankevoort, T., and Welling, M.\n",
            "Bayesian bits: Unifying quantization and pruning. In Larochelle, H., Ranzato, M., Hadsell, R.,\n",
            "Balcan, M. F., and Lin, H. (eds.), Advances in Neural Information Processing Systems , volume 33,\n",
            "pp. 5741–5752. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/\n",
            "paper/2020/file/3f13cf4ddf6fc50c0d39a1d5aeb57dd8-Paper.pdf . 8\n",
            "Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. GLUE: A multi-task benchmark\n",
            "and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP\n",
            "Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pp. 353–355,\n",
            "Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/\n",
            "W18-5446. URL https://www.aclweb.org/anthology/W18-5446 . 16\n",
            "27\n",
            "------------\n",
            "Given the new context, refine the original summary.\n",
            "If the context isn't useful, return the original summary.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOPrybcCnKgy",
        "outputId": "41c2aaa5-741c-4ef8-9b39-ae389e4dc08e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The white paper delves into the significance of neural network quantization for edge devices with power and compute constraints, introducing advanced algorithms like Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) to address quantization noise. It explores hardware background, advantages of fixed-point representations, standard pipelines for optimal performance in deep learning models, and practical considerations for quantizing new models. The paper discusses symmetric and asymmetric uniform quantization schemes, quantization granularity, and simulation techniques. It also explores Cross-Layer Equalization (CLE) to address quantization errors and introduces solutions like bias correction and AdaRound for improving model robustness to quantization. Through experiments and analysis, the paper demonstrates the effectiveness of bias correction, CLE, and AdaRound in improving model performance during quantization. Additionally, it presents a standard PTQ pipeline and evaluates its performance on various computer vision and natural language understanding applications, showing that 8-bit quantization of weights and activation leads to only marginal loss of accuracy compared to floating-point for all models. The paper further explores quantization-aware training (QAT) as a more advanced technique for low-bit quantization, providing insights into back-propagation with simulated quantization and discussing batch normalization folding and per-channel quantization in QAT. The paper concludes by examining the impact of initialization on QAT results, showing that better initialization can lead to improved performance but the gain diminishes over time. The paper also presents a best-practice pipeline for QAT, highlighting the importance of cross-layer equalization, adding quantizers, range estimation, learnable quantization parameters, and training. It emphasizes that both PTQ and QAT are essential for model efficiency and offers pipelines to help engineers deploy high-performing quantized models with less time and effort.\n"
          ]
        }
      ]
    }
  ]
}